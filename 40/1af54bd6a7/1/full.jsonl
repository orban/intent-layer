{"type":"file-history-snapshot","messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","snapshot":{"messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","trackedFileBackups":{},"timestamp":"2026-02-17T20:03:32.850Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:clear","command":"entire hooks claude-code session-start"},"parentToolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","toolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","timestamp":"2026-02-17T20:03:32.563Z","uuid":"32eb0310-bc8a-49de-9d3d-dd7357017788"}
{"parentUuid":"32eb0310-bc8a-49de-9d3d-dd7357017788","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:clear","command":"${CLAUDE_PLUGIN_ROOT}/hooks-handlers/session-start.sh"},"parentToolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","toolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","timestamp":"2026-02-17T20:03:32.563Z","uuid":"9e87e1ea-e20d-4529-90c4-9d0f23f4b8d8"}
{"parentUuid":"9e87e1ea-e20d-4529-90c4-9d0f23f4b8d8","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:clear","command":"${CLAUDE_PLUGIN_ROOT}/hooks/session-start.sh"},"parentToolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","toolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","timestamp":"2026-02-17T20:03:32.563Z","uuid":"12ed3b70-18da-4eba-9d99-ec89676ca6be"}
{"parentUuid":"12ed3b70-18da-4eba-9d99-ec89676ca6be","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:clear","command":"${CLAUDE_PLUGIN_ROOT}/scripts/inject-learnings.sh"},"parentToolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","toolUseID":"528f6254-dabe-4e31-b89d-daa54e842e66","timestamp":"2026-02-17T20:03:32.563Z","uuid":"7f8b3657-28a1-4b4a-99a1-e890b8a54936"}
{"parentUuid":"7f8b3657-28a1-4b4a-99a1-e890b8a54936","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":"Implement the following plan:\n\n# Plan: Add statistical analysis to eval harness\n\n## Context\n\nOur eval harness runs Claude bug-fix trials under 3 conditions (none, flat_llm, intent_layer) but reports raw pass/fail counts with no statistical analysis. The paper we're replicating (arxiv 2602.11988v1) ran each task once with no confidence intervals — their 2-4% differences are indistinguishable from noise.\n\nWe explored integrating with Cerberus (external TypeScript stats tool) but 3 independent reviewers agreed: port the ~80 lines of math directly instead of building a cross-language adapter. Cerberus taught us *what* to compute; we implement it natively.\n\n## What we're adding\n\nWilson Score confidence intervals on pass rates, rendered inline in the existing reporter output. Run `eval-harness run --repetitions 10` and CIs appear automatically.\n\n## Files to create\n\n### 1. `eval-harness/lib/stats.py` (~80 lines)\n\nPort from `~/dev/cerberus/src/stats.ts` lines 7-176:\n\n- `_inverse_normal_cdf(p: float) -> float` — Abramowitz & Stegun 26.2.23 rational approximation. Direct mechanical port from stats.ts lines 7-67. Avoids scipy dependency.\n- `wilson_score_interval(successes: int, n: int, confidence: float = 0.90) -> tuple[float, float, float]` — returns `(lower, upper, center)`. Port from stats.ts lines 152-176. Wilson is better than Wald for small samples and extreme proportions (0% or 100%).\n- `ci_overlap(ci_a: tuple[float, float], ci_b: tuple[float, float]) -> bool` — returns True if two confidence intervals overlap. Simple comparison: `a_lower <= b_upper and b_lower <= a_upper`.\n\nThat's it. No SPRT (deferred — fixed repetitions + CIs already give statistical rigor). No BH correction (we compare per-task, not globally).\n\n## Files to modify\n\n### 2. `eval-harness/lib/reporter.py`\n\nTwo changes:\n\n**a) Add CIs to per-condition output when `--repetitions > 1`**\n\nThe reporter already computes `success_rate`, `successes`, `total_valid_runs` for multi-run results (in `_serialize_condition()`). Add:\n\n```python\nfrom lib.stats import wilson_score_interval\n\nci_lower, ci_upper, ci_center = wilson_score_interval(successes, total_valid, confidence=0.90)\n# Add to the result dict:\n\"ci_90\": {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n```\n\n**b) Add CI comparison to markdown output**\n\nIn the markdown table generation, change the condition columns from:\n```\n| none: 5/8 (62.5%) |\n```\nto:\n```\n| none: 5/8 62% [42,80] |\n```\n\nAnd add a comparison column showing whether intent_layer's CI overlaps with none's:\n```\n| IL vs none: +12% (overlap) |  ← CIs overlap, not significant\n| IL vs none: +30% (sig.)    |  ← CIs don't overlap, significant\n```\n\n### 3. `eval-harness/lib/reporter.py` — summary section\n\nAdd to the summary block:\n- Per-condition: pass rate with CI (e.g., \"intent_layer: 62.5% [42.1%, 79.5%]\")\n- Significance flag: which per-task comparisons show non-overlapping CIs\n- CI width as variance proxy: average CI width per condition (wider = more variable)\n\n## What we're NOT doing\n\n- **No Cerberus integration** — reviewers unanimously rejected the adapter pattern\n- **No SPRT** — adaptive early stopping is a cost optimization, not a correctness requirement. Defer until cost is a problem. Fixed `--repetitions N` + Wilson CIs is statistically sound.\n- **No BH correction** — we're comparing per-task (8 comparisons), not testing all 24 studies globally. With only 8 comparisons and 90% CIs, the false discovery risk is manageable. Can add later.\n- **No new CLI commands** — CIs appear automatically in existing output when repetitions > 1\n- **No new dependencies** — the inverse normal CDF approximation avoids needing scipy\n\n## Key files to read during implementation\n\n- `~/dev/cerberus/src/stats.ts` lines 7-176 — source of truth for Wilson CI + inverse CDF\n- `eval-harness/lib/reporter.py` — `_serialize_condition()`, `_compile_markdown()`, `_compute_summary()`\n- `eval-harness/lib/reporter.py` — existing multi-run format: `success_rate`, `successes`, `total_valid_runs`, `median`, `runs`\n\n## Verification\n\n1. **Unit test `stats.py`**: Test `wilson_score_interval` against known values:\n   - `wilson_score_interval(0, 10, 0.90)` → lower near 0, upper > 0\n   - `wilson_score_interval(10, 10, 0.90)` → lower < 1, upper near 1\n   - `wilson_score_interval(5, 10, 0.90)` → center = 0.50, bounds symmetric-ish\n   - Cross-check a few values against Cerberus's TypeScript implementation\n\n2. **Integration test**: Run `eval-harness run --tasks tasks/fastmcp.yaml --repetitions 2 --condition none -p 1` on a single task, verify CIs appear in both JSON and markdown output\n\n3. **Verify CI semantics**: For our existing 8-task single-run results, `wilson_score_interval(5, 8, 0.90)` should give ~[36%, 84%] — confirming the paper's 2-4% differences are well within noise\n\n\nIf you need specific details from before exiting plan mode (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/ryo/.REDACTED.jsonl\n\nIf this plan can be broken down into multiple independent tasks, consider using the TeamCreate tool to create a team and parallelize the work."},"uuid":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","timestamp":"2026-02-17T20:03:32.324Z","planContent":"# Plan: Add statistical analysis to eval harness\n\n## Context\n\nOur eval harness runs Claude bug-fix trials under 3 conditions (none, flat_llm, intent_layer) but reports raw pass/fail counts with no statistical analysis. The paper we're replicating (arxiv 2602.11988v1) ran each task once with no confidence intervals — their 2-4% differences are indistinguishable from noise.\n\nWe explored integrating with Cerberus (external TypeScript stats tool) but 3 independent reviewers agreed: port the ~80 lines of math directly instead of building a cross-language adapter. Cerberus taught us *what* to compute; we implement it natively.\n\n## What we're adding\n\nWilson Score confidence intervals on pass rates, rendered inline in the existing reporter output. Run `eval-harness run --repetitions 10` and CIs appear automatically.\n\n## Files to create\n\n### 1. `eval-harness/lib/stats.py` (~80 lines)\n\nPort from `~/dev/cerberus/src/stats.ts` lines 7-176:\n\n- `_inverse_normal_cdf(p: float) -> float` — Abramowitz & Stegun 26.2.23 rational approximation. Direct mechanical port from stats.ts lines 7-67. Avoids scipy dependency.\n- `wilson_score_interval(successes: int, n: int, confidence: float = 0.90) -> tuple[float, float, float]` — returns `(lower, upper, center)`. Port from stats.ts lines 152-176. Wilson is better than Wald for small samples and extreme proportions (0% or 100%).\n- `ci_overlap(ci_a: tuple[float, float], ci_b: tuple[float, float]) -> bool` — returns True if two confidence intervals overlap. Simple comparison: `a_lower <= b_upper and b_lower <= a_upper`.\n\nThat's it. No SPRT (deferred — fixed repetitions + CIs already give statistical rigor). No BH correction (we compare per-task, not globally).\n\n## Files to modify\n\n### 2. `eval-harness/lib/reporter.py`\n\nTwo changes:\n\n**a) Add CIs to per-condition output when `--repetitions > 1`**\n\nThe reporter already computes `success_rate`, `successes`, `total_valid_runs` for multi-run results (in `_serialize_condition()`). Add:\n\n```python\nfrom lib.stats import wilson_score_interval\n\nci_lower, ci_upper, ci_center = wilson_score_interval(successes, total_valid, confidence=0.90)\n# Add to the result dict:\n\"ci_90\": {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n```\n\n**b) Add CI comparison to markdown output**\n\nIn the markdown table generation, change the condition columns from:\n```\n| none: 5/8 (62.5%) |\n```\nto:\n```\n| none: 5/8 62% [42,80] |\n```\n\nAnd add a comparison column showing whether intent_layer's CI overlaps with none's:\n```\n| IL vs none: +12% (overlap) |  ← CIs overlap, not significant\n| IL vs none: +30% (sig.)    |  ← CIs don't overlap, significant\n```\n\n### 3. `eval-harness/lib/reporter.py` — summary section\n\nAdd to the summary block:\n- Per-condition: pass rate with CI (e.g., \"intent_layer: 62.5% [42.1%, 79.5%]\")\n- Significance flag: which per-task comparisons show non-overlapping CIs\n- CI width as variance proxy: average CI width per condition (wider = more variable)\n\n## What we're NOT doing\n\n- **No Cerberus integration** — reviewers unanimously rejected the adapter pattern\n- **No SPRT** — adaptive early stopping is a cost optimization, not a correctness requirement. Defer until cost is a problem. Fixed `--repetitions N` + Wilson CIs is statistically sound.\n- **No BH correction** — we're comparing per-task (8 comparisons), not testing all 24 studies globally. With only 8 comparisons and 90% CIs, the false discovery risk is manageable. Can add later.\n- **No new CLI commands** — CIs appear automatically in existing output when repetitions > 1\n- **No new dependencies** — the inverse normal CDF approximation avoids needing scipy\n\n## Key files to read during implementation\n\n- `~/dev/cerberus/src/stats.ts` lines 7-176 — source of truth for Wilson CI + inverse CDF\n- `eval-harness/lib/reporter.py` — `_serialize_condition()`, `_compile_markdown()`, `_compute_summary()`\n- `eval-harness/lib/reporter.py` — existing multi-run format: `success_rate`, `successes`, `total_valid_runs`, `median`, `runs`\n\n## Verification\n\n1. **Unit test `stats.py`**: Test `wilson_score_interval` against known values:\n   - `wilson_score_interval(0, 10, 0.90)` → lower near 0, upper > 0\n   - `wilson_score_interval(10, 10, 0.90)` → lower < 1, upper near 1\n   - `wilson_score_interval(5, 10, 0.90)` → center = 0.50, bounds symmetric-ish\n   - Cross-check a few values against Cerberus's TypeScript implementation\n\n2. **Integration test**: Run `eval-harness run --tasks tasks/fastmcp.yaml --repetitions 2 --condition none -p 1` on a single task, verify CIs appear in both JSON and markdown output\n\n3. **Verify CI semantics**: For our existing 8-task single-run results, `wilson_score_interval(5, 8, 0.90)` should give ~[36%, 84%] — confirming the paper's 2-4% differences are well within noise\n"}
{"parentUuid":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BqTWQGPVEPwKpKnAoiyjgi","type":"message","role":"assistant","content":[{"type":"text","text":"\n\n`★ Insight ─────────────────────────────────────`\nThis plan adds statistical rigor to an eval harness that currently reports raw counts. Wilson Score intervals are ideal here because they handle edge cases (0% and 100% pass rates) gracefully — unlike Wald intervals which collapse to zero width at boundaries. The inverse normal CDF approximation (Abramowitz & Stegun 26.2.23) avoids pulling in scipy for a single function.\n`─────────────────────────────────────────────────`\n\nLet me start by reading the source files I need to port from and modify."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":10438,"cache_read_input_tokens":49171,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10438},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErKXyvxD3LZU9zyyMM","type":"assistant","uuid":"6a871058-1f1e-4e3f-87e5-862885b1d97a","timestamp":"2026-02-17T20:03:39.255Z"}
{"parentUuid":"6a871058-1f1e-4e3f-87e5-862885b1d97a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BqTWQGPVEPwKpKnAoiyjgi","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012K5GAau1NW8Ez5oihvbpyq","name":"Read","input":{"file_path":"/Users/ryo/dev/cerberus/src/stats.ts","limit":180},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":10438,"cache_read_input_tokens":49171,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10438},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErKXyvxD3LZU9zyyMM","type":"assistant","uuid":"57ddd9eb-cd13-4db0-be62-2ff5f00b9476","timestamp":"2026-02-17T20:03:39.810Z"}
{"parentUuid":"57ddd9eb-cd13-4db0-be62-2ff5f00b9476","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","parentToolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","uuid":"364a425e-326f-40ea-86fc-9d147c6dd095","timestamp":"2026-02-17T20:03:39.813Z"}
{"parentUuid":"364a425e-326f-40ea-86fc-9d147c6dd095","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","toolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","timestamp":"2026-02-17T20:03:39.907Z","uuid":"09286dba-a154-469f-9fcd-2986aa0cece4"}
{"parentUuid":"09286dba-a154-469f-9fcd-2986aa0cece4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","toolUseID":"toolu_012K5GAau1NW8Ez5oihvbpyq","timestamp":"2026-02-17T20:03:39.907Z","uuid":"8bc33aea-a8c1-499b-b743-ad1b45825061"}
{"parentUuid":"57ddd9eb-cd13-4db0-be62-2ff5f00b9476","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012K5GAau1NW8Ez5oihvbpyq","type":"tool_result","content":"     1→import type { SPRTConfig, SPRTState, ConfidenceInterval } from \"./types.js\";\n     2→\n     3→// ── Z-score lookup ───────────────────────────────────────────\n     4→// Rational approximation of the inverse normal CDF (Abramowitz & Stegun 26.2.23).\n     5→// Accurate to ~4.5e-4. Good enough for CI computation; avoids a dependency.\n     6→\n     7→function inverseNormalCDF(p: number): number {\n     8→  if (p <= 0 || p >= 1) {\n     9→    throw new RangeError(`inverseNormalCDF: p must be in (0,1), got ${p}`);\n    10→  }\n    11→\n    12→  // Coefficients for the rational approximation\n    13→  const a1 = -3.969683028665376e1;\n    14→  const a2 = 2.209460984245205e2;\n    15→  const a3 = -2.759285104469687e2;\n    16→  const a4 = 1.383577518672690e2;\n    17→  const a5 = -3.066479806614716e1;\n    18→  const a6 = 2.506628277459239e0;\n    19→\n    20→  const b1 = -5.447609879822406e1;\n    21→  const b2 = 1.615858368580409e2;\n    22→  const b3 = -1.556989798598866e2;\n    23→  const b4 = 6.680131188771972e1;\n    24→  const b5 = -1.328068155288572e1;\n    25→\n    26→  const c1 = -7.784894002430293e-3;\n    27→  const c2 = -3.223964580411365e-1;\n    28→  const c3 = -2.400758277161838e0;\n    29→  const c4 = -2.549732539343734e0;\n    30→  const c5 = 4.374664141464968e0;\n    31→  const c6 = 2.938163982698783e0;\n    32→\n    33→  const d1 = 7.784695709041462e-3;\n    34→  const d2 = 3.224671290700398e-1;\n    35→  const d3 = 2.445134137142996e0;\n    36→  const d4 = 3.754408661907416e0;\n    37→\n    38→  const pLow = 0.02425;\n    39→  const pHigh = 1 - pLow;\n    40→\n    41→  let q: number;\n    42→  let r: number;\n    43→\n    44→  if (p < pLow) {\n    45→    // Rational approximation for lower region\n    46→    q = Math.sqrt(-2 * Math.log(p));\n    47→    return (\n    48→      (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6) /\n    49→      ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n    50→    );\n    51→  } else if (p <= pHigh) {\n    52→    // Rational approximation for central region\n    53→    q = p - 0.5;\n    54→    r = q * q;\n    55→    return (\n    56→      ((((((a1 * r + a2) * r + a3) * r + a4) * r + a5) * r + a6) * q) /\n    57→      (((((b1 * r + b2) * r + b3) * r + b4) * r + b5) * r + 1)\n    58→    );\n    59→  } else {\n    60→    // Rational approximation for upper region\n    61→    q = Math.sqrt(-2 * Math.log(1 - p));\n    62→    return (\n    63→      -(((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6) /\n    64→      ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n    65→    );\n    66→  }\n    67→}\n    68→\n    69→// ── SPRT ─────────────────────────────────────────────────────\n    70→\n    71→export function sprtConfigFromContract(\n    72→  threshold: number,\n    73→  confidence: number,\n    74→): SPRTConfig {\n    75→  const p0 = threshold;\n    76→  const p1 = Math.max(0.01, threshold - 0.10);\n    77→  const alpha = 1 - confidence;\n    78→  const beta = 0.20;\n    79→\n    80→  return { p0, p1, alpha, beta };\n    81→}\n    82→\n    83→export function createSPRT(config: SPRTConfig): SPRTState {\n    84→  const A = (1 - config.beta) / config.alpha; // upper boundary\n    85→  const B = config.beta / (1 - config.alpha); // lower boundary\n    86→\n    87→  return {\n    88→    logLR: 0,\n    89→    observations: 0,\n    90→    successes: 0,\n    91→    decision: \"continue\",\n    92→    upperBoundary: Math.log(A),\n    93→    lowerBoundary: Math.log(B),\n    94→  };\n    95→}\n    96→\n    97→export function updateSPRT(\n    98→  state: SPRTState,\n    99→  config: SPRTConfig,\n   100→  success: boolean,\n   101→): SPRTState {\n   102→  if (state.decision !== \"continue\") {\n   103→    return state; // already decided\n   104→  }\n   105→\n   106→  // Guard against degenerate probabilities\n   107→  const p0 = Math.max(1e-10, Math.min(1 - 1e-10, config.p0));\n   108→  const p1 = Math.max(1e-10, Math.min(1 - 1e-10, config.p1));\n   109→\n   110→  // Log-likelihood ratio update for Bernoulli SPRT\n   111→  const logUpdate = success\n   112→    ? Math.log(p0 / p1)\n   113→    : Math.log((1 - p0) / (1 - p1));\n   114→\n   115→  const newLogLR = state.logLR + logUpdate;\n   116→  const newObservations = state.observations + 1;\n   117→  const newSuccesses = state.successes + (success ? 1 : 0);\n   118→\n   119→  // NaN guard\n   120→  if (!Number.isFinite(newLogLR)) {\n   121→    return {\n   122→      ...state,\n   123→      observations: newObservations,\n   124→      successes: newSuccesses,\n   125→      decision: \"continue\", // can't decide on NaN, keep going\n   126→    };\n   127→  }\n   128→\n   129→  let decision: SPRTState[\"decision\"] = \"continue\";\n   130→\n   131→  // Upper boundary: strong evidence FOR the null hypothesis (agent meets threshold)\n   132→  if (newLogLR >= state.upperBoundary) {\n   133→    decision = \"accept\";\n   134→  }\n   135→  // Lower boundary: strong evidence AGAINST the null hypothesis (agent fails)\n   136→  else if (newLogLR <= state.lowerBoundary) {\n   137→    decision = \"reject\";\n   138→  }\n   139→\n   140→  return {\n   141→    logLR: newLogLR,\n   142→    observations: newObservations,\n   143→    successes: newSuccesses,\n   144→    decision,\n   145→    upperBoundary: state.upperBoundary,\n   146→    lowerBoundary: state.lowerBoundary,\n   147→  };\n   148→}\n   149→\n   150→// ── Wilson Score Confidence Interval ─────────────────────────\n   151→\n   152→export function wilsonScoreInterval(\n   153→  successes: number,\n   154→  n: number,\n   155→  confidence: number,\n   156→): ConfidenceInterval {\n   157→  if (n === 0) {\n   158→    return { lower: 0, upper: 1, center: 0, n: 0 };\n   159→  }\n   160→\n   161→  const z = inverseNormalCDF(1 - (1 - confidence) / 2);\n   162→  const z2 = z * z;\n   163→  const pHat = successes / n;\n   164→\n   165→  const denominator = 1 + z2 / n;\n   166→  const center = (pHat + z2 / (2 * n)) / denominator;\n   167→  const spread =\n   168→    (z * Math.sqrt(pHat * (1 - pHat) / n + z2 / (4 * n * n))) / denominator;\n   169→\n   170→  return {\n   171→    lower: Math.max(0, center - spread),\n   172→    upper: Math.min(1, center + spread),\n   173→    center,\n   174→    n,\n   175→  };\n   176→}\n   177→\n   178→// ── Multiple testing corrections ─────────────────────────────\n   179→\n   180→export function bonferroniCorrection("}]},"uuid":"b8554a1d-297c-423c-94f5-29b3f410a9aa","timestamp":"2026-02-17T20:03:39.906Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/ryo/dev/cerberus/src/stats.ts","content":"import type { SPRTConfig, SPRTState, ConfidenceInterval } from \"./types.js\";\n\n// ── Z-score lookup ───────────────────────────────────────────\n// Rational approximation of the inverse normal CDF (Abramowitz & Stegun 26.2.23).\n// Accurate to ~4.5e-4. Good enough for CI computation; avoids a dependency.\n\nfunction inverseNormalCDF(p: number): number {\n  if (p <= 0 || p >= 1) {\n    throw new RangeError(`inverseNormalCDF: p must be in (0,1), got ${p}`);\n  }\n\n  // Coefficients for the rational approximation\n  const a1 = -3.969683028665376e1;\n  const a2 = 2.209460984245205e2;\n  const a3 = -2.759285104469687e2;\n  const a4 = 1.383577518672690e2;\n  const a5 = -3.066479806614716e1;\n  const a6 = 2.506628277459239e0;\n\n  const b1 = -5.447609879822406e1;\n  const b2 = 1.615858368580409e2;\n  const b3 = -1.556989798598866e2;\n  const b4 = 6.680131188771972e1;\n  const b5 = -1.328068155288572e1;\n\n  const c1 = -7.784894002430293e-3;\n  const c2 = -3.223964580411365e-1;\n  const c3 = -2.400758277161838e0;\n  const c4 = -2.549732539343734e0;\n  const c5 = 4.374664141464968e0;\n  const c6 = 2.938163982698783e0;\n\n  const d1 = 7.784695709041462e-3;\n  const d2 = 3.224671290700398e-1;\n  const d3 = 2.445134137142996e0;\n  const d4 = 3.754408661907416e0;\n\n  const pLow = 0.02425;\n  const pHigh = 1 - pLow;\n\n  let q: number;\n  let r: number;\n\n  if (p < pLow) {\n    // Rational approximation for lower region\n    q = Math.sqrt(-2 * Math.log(p));\n    return (\n      (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6) /\n      ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n    );\n  } else if (p <= pHigh) {\n    // Rational approximation for central region\n    q = p - 0.5;\n    r = q * q;\n    return (\n      ((((((a1 * r + a2) * r + a3) * r + a4) * r + a5) * r + a6) * q) /\n      (((((b1 * r + b2) * r + b3) * r + b4) * r + b5) * r + 1)\n    );\n  } else {\n    // Rational approximation for upper region\n    q = Math.sqrt(-2 * Math.log(1 - p));\n    return (\n      -(((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6) /\n      ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n    );\n  }\n}\n\n// ── SPRT ─────────────────────────────────────────────────────\n\nexport function sprtConfigFromContract(\n  threshold: number,\n  confidence: number,\n): SPRTConfig {\n  const p0 = threshold;\n  const p1 = Math.max(0.01, threshold - 0.10);\n  const alpha = 1 - confidence;\n  const beta = 0.20;\n\n  return { p0, p1, alpha, beta };\n}\n\nexport function createSPRT(config: SPRTConfig): SPRTState {\n  const A = (1 - config.beta) / config.alpha; // upper boundary\n  const B = config.beta / (1 - config.alpha); // lower boundary\n\n  return {\n    logLR: 0,\n    observations: 0,\n    successes: 0,\n    decision: \"continue\",\n    upperBoundary: Math.log(A),\n    lowerBoundary: Math.log(B),\n  };\n}\n\nexport function updateSPRT(\n  state: SPRTState,\n  config: SPRTConfig,\n  success: boolean,\n): SPRTState {\n  if (state.decision !== \"continue\") {\n    return state; // already decided\n  }\n\n  // Guard against degenerate probabilities\n  const p0 = Math.max(1e-10, Math.min(1 - 1e-10, config.p0));\n  const p1 = Math.max(1e-10, Math.min(1 - 1e-10, config.p1));\n\n  // Log-likelihood ratio update for Bernoulli SPRT\n  const logUpdate = success\n    ? Math.log(p0 / p1)\n    : Math.log((1 - p0) / (1 - p1));\n\n  const newLogLR = state.logLR + logUpdate;\n  const newObservations = state.observations + 1;\n  const newSuccesses = state.successes + (success ? 1 : 0);\n\n  // NaN guard\n  if (!Number.isFinite(newLogLR)) {\n    return {\n      ...state,\n      observations: newObservations,\n      successes: newSuccesses,\n      decision: \"continue\", // can't decide on NaN, keep going\n    };\n  }\n\n  let decision: SPRTState[\"decision\"] = \"continue\";\n\n  // Upper boundary: strong evidence FOR the null hypothesis (agent meets threshold)\n  if (newLogLR >= state.upperBoundary) {\n    decision = \"accept\";\n  }\n  // Lower boundary: strong evidence AGAINST the null hypothesis (agent fails)\n  else if (newLogLR <= state.lowerBoundary) {\n    decision = \"reject\";\n  }\n\n  return {\n    logLR: newLogLR,\n    observations: newObservations,\n    successes: newSuccesses,\n    decision,\n    upperBoundary: state.upperBoundary,\n    lowerBoundary: state.lowerBoundary,\n  };\n}\n\n// ── Wilson Score Confidence Interval ─────────────────────────\n\nexport function wilsonScoreInterval(\n  successes: number,\n  n: number,\n  confidence: number,\n): ConfidenceInterval {\n  if (n === 0) {\n    return { lower: 0, upper: 1, center: 0, n: 0 };\n  }\n\n  const z = inverseNormalCDF(1 - (1 - confidence) / 2);\n  const z2 = z * z;\n  const pHat = successes / n;\n\n  const denominator = 1 + z2 / n;\n  const center = (pHat + z2 / (2 * n)) / denominator;\n  const spread =\n    (z * Math.sqrt(pHat * (1 - pHat) / n + z2 / (4 * n * n))) / denominator;\n\n  return {\n    lower: Math.max(0, center - spread),\n    upper: Math.min(1, center + spread),\n    center,\n    n,\n  };\n}\n\n// ── Multiple testing corrections ─────────────────────────────\n\nexport function bonferroniCorrection(","numLines":180,"startLine":1,"totalLines":226}},"sourceToolAssistantUUID":"57ddd9eb-cd13-4db0-be62-2ff5f00b9476"}
{"parentUuid":"b8554a1d-297c-423c-94f5-29b3f410a9aa","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BqTWQGPVEPwKpKnAoiyjgi","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WbHunojVn5jk2fMWbKKJME","name":"Read","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":10438,"cache_read_input_tokens":49171,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10438},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErKXyvxD3LZU9zyyMM","type":"assistant","uuid":"1805bd3f-8fa4-4fa7-804d-de5b2d72443c","timestamp":"2026-02-17T20:03:40.438Z"}
{"parentUuid":"1805bd3f-8fa4-4fa7-804d-de5b2d72443c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","parentToolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","uuid":"d28d71aa-6337-4b46-83a0-4c5f42287a5b","timestamp":"2026-02-17T20:03:40.440Z"}
{"parentUuid":"d28d71aa-6337-4b46-83a0-4c5f42287a5b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","toolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","timestamp":"2026-02-17T20:03:40.505Z","uuid":"e6e296dd-c361-4006-b08e-a13fd9acffa8"}
{"parentUuid":"e6e296dd-c361-4006-b08e-a13fd9acffa8","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","toolUseID":"toolu_01WbHunojVn5jk2fMWbKKJME","timestamp":"2026-02-17T20:03:40.505Z","uuid":"feb03b3c-b073-4a4d-a093-b157158c85ca"}
{"parentUuid":"1805bd3f-8fa4-4fa7-804d-de5b2d72443c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WbHunojVn5jk2fMWbKKJME","type":"tool_result","content":"     1→# lib/reporter.py\n     2→from __future__ import annotations\n     3→import json\n     4→import statistics\n     5→from dataclasses import dataclass, asdict\n     6→from datetime import datetime\n     7→from pathlib import Path\n     8→from typing import Any\n     9→\n    10→from lib.task_runner import TaskResult, Condition\n    11→\n    12→\n    13→@dataclass\n    14→class EvalResults:\n    15→    eval_id: str\n    16→    timestamp: str\n    17→    results: list[dict[str, Any]]\n    18→    summary: dict[str, Any]\n    19→\n    20→\n    21→class Reporter:\n    22→    def __init__(self, output_dir: str):\n    23→        self.output_dir = Path(output_dir)\n    24→        self.output_dir.mkdir(parents=True, exist_ok=True)\n    25→\n    26→    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n    27→        \"\"\"Compile task results into structured eval results.\n    28→\n    29→        When repetitions > 1, groups multiple runs of the same task+condition\n    30→        and reports medians for efficiency metrics alongside success rates.\n    31→        Individual runs are preserved in a \"runs\" array.\n    32→        \"\"\"\n    33→        # Group by (task_id, condition) — allows multiple runs per pair\n    34→        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n    35→        for r in results:\n    36→            if r.task_id not in grouped:\n    37→                grouped[r.task_id] = {}\n    38→            cond = r.condition.value\n    39→            if cond not in grouped[r.task_id]:\n    40→                grouped[r.task_id][cond] = []\n    41→            grouped[r.task_id][cond].append(r)\n    42→\n    43→        compiled = []\n    44→        for task_id, conditions in grouped.items():\n    45→            none_runs = conditions.get(\"none\", [])\n    46→            flat_runs = conditions.get(\"flat_llm\", [])\n    47→            il_runs = conditions.get(\"intent_layer\", [])\n    48→\n    49→            task_result = {\n    50→                \"task_id\": task_id,\n    51→                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n    52→                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n    53→                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n    54→                \"deltas\": {\n    55→                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n    56→                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n    57→                }\n    58→            }\n    59→            compiled.append(task_result)\n    60→\n    61→        summary = self._compute_summary(results)\n    62→\n    63→        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    64→        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n    65→\n    66→        return EvalResults(\n    67→            eval_id=eval_id,\n    68→            timestamp=timestamp,\n    69→            results=compiled,\n    70→            summary=summary\n    71→        )\n    72→\n    73→    def _serialize_single_result(self, r: TaskResult) -> dict:\n    74→        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n    75→        result = {\n    76→            \"success\": r.success,\n    77→            \"test_output\": r.test_output[:1000],  # Truncate\n    78→        }\n    79→\n    80→        if r.skill_generation:\n    81→            result[\"fix_only\"] = {\n    82→                \"wall_clock_seconds\": r.wall_clock_seconds,\n    83→                \"input_tokens\": r.input_tokens,\n    84→                \"output_tokens\": r.output_tokens,\n    85→                \"tool_calls\": r.tool_calls,\n    86→                \"lines_changed\": r.lines_changed,\n    87→                \"files_touched\": r.files_touched,\n    88→            }\n    89→            result[\"skill_generation\"] = {\n    90→                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n    91→                \"input_tokens\": r.skill_generation.input_tokens,\n    92→                \"output_tokens\": r.skill_generation.output_tokens,\n    93→                \"cache_hit\": r.skill_generation.cache_hit,\n    94→            }\n    95→            result[\"total\"] = {\n    96→                \"wall_clock_seconds\": (\n    97→                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n    98→                ),\n    99→                \"input_tokens\": (\n   100→                    r.input_tokens + r.skill_generation.input_tokens\n   101→                ),\n   102→                \"output_tokens\": (\n   103→                    r.output_tokens + r.skill_generation.output_tokens\n   104→                ),\n   105→            }\n   106→        else:\n   107→            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n   108→            result[\"input_tokens\"] = r.input_tokens\n   109→            result[\"output_tokens\"] = r.output_tokens\n   110→            result[\"tool_calls\"] = r.tool_calls\n   111→            result[\"lines_changed\"] = r.lines_changed\n   112→            result[\"files_touched\"] = r.files_touched\n   113→\n   114→        if r.error:\n   115→            result[\"error\"] = r.error\n   116→\n   117→        if r.exit_code is not None:\n   118→            result[\"exit_code\"] = r.exit_code\n   119→        if r.is_timeout:\n   120→            result[\"is_timeout\"] = r.is_timeout\n   121→\n   122→        return result\n   123→\n   124→    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n   125→        \"\"\"Serialize one or more runs for a single condition.\n   126→\n   127→        Single run: backward-compatible flat structure.\n   128→        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n   129→        success rate and efficiency metrics.\n   130→        \"\"\"\n   131→        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n   132→\n   133→        if len(runs) == 1:\n   134→            # Single run: backward-compatible\n   135→            return self._serialize_single_result(runs[0])\n   136→\n   137→        # Multiple runs: include individual results + aggregated medians\n   138→        result: dict[str, Any] = {}\n   139→\n   140→        # Success rate (primary metric — separated from efficiency)\n   141→        successes = sum(1 for r in valid_runs if r.success)\n   142→        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n   143→        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n   144→        result[\"successes\"] = successes\n   145→        result[\"total_valid_runs\"] = len(valid_runs)\n   146→\n   147→        # Efficiency medians (secondary metrics — from valid runs only)\n   148→        if valid_runs:\n   149→            result[\"median\"] = {\n   150→                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n   151→                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n   152→                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n   153→                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n   154→                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n   155→            }\n   156→\n   157→        # Individual runs for drill-down\n   158→        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n   159→\n   160→        return result\n   161→\n   162→    @staticmethod\n   163→    def _get_fix_metrics(cond_data: dict) -> dict:\n   164→        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n   165→\n   166→        Works with both single-run format and multi-run median format.\n   167→        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n   168→        tool_calls, lines_changed.\n   169→        \"\"\"\n   170→        # Multi-run: use median\n   171→        if \"median\" in cond_data:\n   172→            m = cond_data[\"median\"]\n   173→            return {\n   174→                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n   175→                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n   176→                \"tool_calls\": m[\"tool_calls\"],\n   177→                \"lines_changed\": m[\"lines_changed\"],\n   178→            }\n   179→        # Single run with skill_generation\n   180→        if \"fix_only\" in cond_data:\n   181→            fix = cond_data[\"fix_only\"]\n   182→            return {\n   183→                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n   184→                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n   185→                \"tool_calls\": fix[\"tool_calls\"],\n   186→                \"lines_changed\": fix[\"lines_changed\"],\n   187→            }\n   188→        # Single run without skill_generation\n   189→        return {\n   190→            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n   191→            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n   192→            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n   193→            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n   194→        }\n   195→\n   196→    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n   197→        \"\"\"Compute delta between baseline and treatment using median metrics.\n   198→\n   199→        For single runs, behaves identically to the old _compute_single_delta.\n   200→        For multiple runs, uses medians for efficiency and success rate delta.\n   201→        \"\"\"\n   202→        if not baseline_runs or not treatment_runs:\n   203→            return {}\n   204→\n   205→        # Filter out infra errors for stats\n   206→        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n   207→        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n   208→\n   209→        if not b_valid or not t_valid:\n   210→            return {}\n   211→\n   212→        # Success rate delta\n   213→        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n   214→        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n   215→        rate_delta = t_rate - b_rate\n   216→\n   217→        # Median efficiency metrics\n   218→        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n   219→        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n   220→        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n   221→        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n   222→        b_tools = statistics.median(r.tool_calls for r in b_valid)\n   223→        t_tools = statistics.median(r.tool_calls for r in t_valid)\n   224→        b_lines = statistics.median(r.lines_changed for r in b_valid)\n   225→        t_lines = statistics.median(r.lines_changed for r in t_valid)\n   226→\n   227→        def pct(baseline_val, treatment_val):\n   228→            if baseline_val:\n   229→                return (treatment_val - baseline_val) / baseline_val * 100\n   230→            return 0\n   231→\n   232→        return {\n   233→            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n   234→                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n   235→            ),\n   236→            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n   237→            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n   238→            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n   239→            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n   240→        }\n   241→\n   242→    @staticmethod\n   243→    def _is_infra_error(r: TaskResult) -> bool:\n   244→        \"\"\"Check if result is a non-experimental error (excluded from stats).\n   245→\n   246→        Infrastructure errors, pre-validation failures, and skill generation\n   247→        failures all indicate harness problems, not experimental outcomes.\n   248→        \"\"\"\n   249→        if r.error is None:\n   250→            return False\n   251→        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n   252→\n   253→    def _compute_summary(self, results: list[TaskResult]) -> dict:\n   254→        \"\"\"Compute overall summary statistics.\n   255→\n   256→        Infrastructure errors (clone/docker/workspace failures) are excluded\n   257→        from success rate calculations to avoid corrupting experimental data.\n   258→        \"\"\"\n   259→        def success_rate(task_results: list[TaskResult]) -> float:\n   260→            valid = [r for r in task_results if not self._is_infra_error(r)]\n   261→            if not valid:\n   262→                return 0\n   263→            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n   264→\n   265→        none_results = [r for r in results if r.condition == Condition.NONE]\n   266→        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n   267→        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n   268→\n   269→        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n   270→\n   271→        return {\n   272→            \"total_tasks\": len(set(r.task_id for r in results)),\n   273→            \"infrastructure_errors\": infra_errors,\n   274→            \"none_success_rate\": success_rate(none_results),\n   275→            \"flat_llm_success_rate\": success_rate(flat_results),\n   276→            \"intent_layer_success_rate\": success_rate(il_results),\n   277→        }\n   278→\n   279→    def write_json(self, results: EvalResults) -> str:\n   280→        \"\"\"Write results to JSON file.\"\"\"\n   281→        path = self.output_dir / f\"{results.eval_id}.json\"\n   282→        with open(path, \"w\") as f:\n   283→            json.dump(asdict(results), f, indent=2)\n   284→        return str(path)\n   285→\n   286→    def write_markdown(self, results: EvalResults) -> str:\n   287→        \"\"\"Write results to Markdown file.\n   288→\n   289→        Handles both single-run (backward-compatible) and multi-run formats.\n   290→        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n   291→        \"\"\"\n   292→        path = self.output_dir / f\"{results.eval_id}.md\"\n   293→\n   294→        lines = [\n   295→            f\"# Eval Results: {results.eval_id}\",\n   296→            \"\",\n   297→            f\"**Timestamp:** {results.timestamp}\",\n   298→            \"\",\n   299→            \"## Summary\",\n   300→            \"\",\n   301→            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n   302→            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n   303→            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n   304→            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n   305→            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n   306→            \"\",\n   307→            \"## Results\",\n   308→            \"\",\n   309→            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n   310→            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n   311→        ]\n   312→\n   313→        for r in results.results:\n   314→            task_id = r[\"task_id\"]\n   315→            deltas = r.get(\"deltas\", {})\n   316→\n   317→            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n   318→                cond_data = r.get(cond_key)\n   319→                if cond_data is None:\n   320→                    continue\n   321→\n   322→                # Success: multi-run shows rate, single-run shows PASS/FAIL\n   323→                if \"success_rate\" in cond_data:\n   324→                    rate = cond_data[\"success_rate\"]\n   325→                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n   326→                else:\n   327→                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n   328→\n   329→                # Extract efficiency metrics via unified helper\n   330→                metrics = self._get_fix_metrics(cond_data)\n   331→                time_s = metrics[\"wall_clock_seconds\"]\n   332→                tokens = metrics[\"tokens\"]\n   333→                tool_calls = metrics[\"tool_calls\"]\n   334→                lines_changed = metrics[\"lines_changed\"]\n   335→\n   336→                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n   337→\n   338→                # Deltas: none is baseline, shows \"—\"\n   339→                if cond_key == \"none\":\n   340→                    d_time = \"\\u2014\"\n   341→                    d_tokens = \"\\u2014\"\n   342→                else:\n   343→                    delta = deltas.get(cond_key, {})\n   344→                    d_time = delta.get(\"time_percent\", \"N/A\")\n   345→                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n   346→\n   347→                lines.append(\n   348→                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n   349→                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n   350→                    f\"{d_time} | {d_tokens} |\"\n   351→                )\n   352→\n   353→            # Blank row between tasks\n   354→            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n   355→\n   356→        # Remove trailing blank row\n   357→        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n   358→            lines.pop()\n   359→\n   360→        with open(path, \"w\") as f:\n   361→            f.write(\"\\n\".join(lines))\n   362→\n   363→        return str(path)\n   364→"}]},"uuid":"b736142f-4fd0-46a7-9158-8e698f242a96","timestamp":"2026-02-17T20:03:40.503Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","content":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","numLines":364,"startLine":1,"totalLines":364}},"sourceToolAssistantUUID":"1805bd3f-8fa4-4fa7-804d-de5b2d72443c"}
{"parentUuid":"b736142f-4fd0-46a7-9158-8e698f242a96","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BqTWQGPVEPwKpKnAoiyjgi","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018jV8U8HrA3zPhpj4Fio21B","name":"Glob","input":{"pattern":"eval-harness/lib/*.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":10438,"cache_read_input_tokens":49171,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10438},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErKXyvxD3LZU9zyyMM","type":"assistant","uuid":"fb287c82-ccdb-4d87-9ed4-f99ea59472f4","timestamp":"2026-02-17T20:03:40.777Z"}
{"parentUuid":"fb287c82-ccdb-4d87-9ed4-f99ea59472f4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Glob","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","parentToolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","uuid":"bf5bcdc5-5791-413d-92c4-aad25eefd4d4","timestamp":"2026-02-17T20:03:40.783Z"}
{"parentUuid":"bf5bcdc5-5791-413d-92c4-aad25eefd4d4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Glob","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","toolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","timestamp":"2026-02-17T20:03:41.186Z","uuid":"beaa67e1-bf23-4c1e-beab-b134d53fd31a"}
{"parentUuid":"beaa67e1-bf23-4c1e-beab-b134d53fd31a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Glob","command":"callback"},"parentToolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","toolUseID":"toolu_018jV8U8HrA3zPhpj4Fio21B","timestamp":"2026-02-17T20:03:41.186Z","uuid":"abdee253-79da-4723-927d-e5f504b2e5fe"}
{"parentUuid":"fb287c82-ccdb-4d87-9ed4-f99ea59472f4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018jV8U8HrA3zPhpj4Fio21B","type":"tool_result","content":"/Users/ryo/dev/intent-layer/eval-harness/lib/__init__.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/docker_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/git_scanner.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/agent_config.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/models.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/prompt_builder.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/claude_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/git_ops.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/task_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/index_cache.py\n/Users/ryo/dev/intent-layer/eval-harness/lib/cli.py"}]},"uuid":"8b186d53-9b5d-458c-a775-2c19d67bb97a","timestamp":"2026-02-17T20:03:41.184Z","toolUseResult":{"filenames":["/Users/ryo/dev/intent-layer/eval-harness/lib/__init__.py","/Users/ryo/dev/intent-layer/eval-harness/lib/docker_runner.py","/Users/ryo/dev/intent-layer/eval-harness/lib/git_scanner.py","/Users/ryo/dev/intent-layer/eval-harness/lib/agent_config.py","/Users/ryo/dev/intent-layer/eval-harness/lib/models.py","/Users/ryo/dev/intent-layer/eval-harness/lib/prompt_builder.py","/Users/ryo/dev/intent-layer/eval-harness/lib/claude_runner.py","/Users/ryo/dev/intent-layer/eval-harness/lib/git_ops.py","/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","/Users/ryo/dev/intent-layer/eval-harness/lib/task_runner.py","/Users/ryo/dev/intent-layer/eval-harness/lib/index_cache.py","/Users/ryo/dev/intent-layer/eval-harness/lib/cli.py"],"durationMs":349,"numFiles":12,"truncated":false},"sourceToolAssistantUUID":"fb287c82-ccdb-4d87-9ed4-f99ea59472f4"}
{"parentUuid":"8b186d53-9b5d-458c-a775-2c19d67bb97a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01Syho93simdrXv4Ti4CRMQo","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Good, I have all the source material. Let me also check the existing test structure for the eval harness.","signature":"EpECCkYICxgCKkBCR8Gx8dQ2TMxZen/47H6srMaoYl8ufpyJmijTv+2Q4JdNr7T6Xs8p6AtJO10TAFmzu7ErzcYf3q/uSJXXVDDfEgza9yHKH3mDWE9oZPoaDG9MP/+cnrR2EFzX0iIw0D3rNwnJJdEZr8I0jKURzu+SI5zTM7LiziRsr2cPPCmMIjI8I9g1nj8SMuYtBqk9KnnFk5FSqpkWWGcJiud7C6IrtmqTKhYwe90TYI9f4ZqZCA8/pO2fIxkxMzJXJGzALPnwb6ON3OR080GfJbIYF8iBQAay2MTEZVPElg8OzL3P+akhHLjcGD8tmlr5P3eQWtIuwrvTUyaAURw03+MjbzMzSyYkS7Nuu4gnGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11843,"cache_read_input_tokens":59609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":11843},"output_tokens":8,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErwFK2fE5z5mnMoioJ","type":"assistant","uuid":"482fcdad-be88-4e22-a860-1c39d1940461","timestamp":"2026-02-17T20:03:44.461Z"}
{"parentUuid":"482fcdad-be88-4e22-a860-1c39d1940461","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01Syho93simdrXv4Ti4CRMQo","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check if there are existing tests for the eval harness, and look at the task_runner for the `TaskResult` and `Condition` types."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11843,"cache_read_input_tokens":59609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":11843},"output_tokens":8,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErwFK2fE5z5mnMoioJ","type":"assistant","uuid":"68e52f8e-15fc-4825-8a15-a247963c6a90","timestamp":"2026-02-17T20:03:45.665Z"}
{"parentUuid":"68e52f8e-15fc-4825-8a15-a247963c6a90","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01Syho93simdrXv4Ti4CRMQo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01G4P4fwmUGP2AESwzyvLYtN","name":"Glob","input":{"pattern":"eval-harness/tests/**/*.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11843,"cache_read_input_tokens":59609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":11843},"output_tokens":8,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErwFK2fE5z5mnMoioJ","type":"assistant","uuid":"fb774261-17ab-48bc-b8fd-88e29ce00627","timestamp":"2026-02-17T20:03:46.113Z"}
{"parentUuid":"fb774261-17ab-48bc-b8fd-88e29ce00627","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Glob","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","parentToolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","uuid":"ed192569-c5af-486e-a38d-24ac7ea849b0","timestamp":"2026-02-17T20:03:46.115Z"}
{"parentUuid":"ed192569-c5af-486e-a38d-24ac7ea849b0","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Glob","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","toolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","timestamp":"2026-02-17T20:03:46.479Z","uuid":"1754b586-cadf-4ee9-aa57-719b3b393956"}
{"parentUuid":"1754b586-cadf-4ee9-aa57-719b3b393956","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Glob","command":"callback"},"parentToolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","toolUseID":"toolu_01G4P4fwmUGP2AESwzyvLYtN","timestamp":"2026-02-17T20:03:46.479Z","uuid":"c6936ef3-6d85-424f-b0f9-fa82a8089573"}
{"parentUuid":"fb774261-17ab-48bc-b8fd-88e29ce00627","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01G4P4fwmUGP2AESwzyvLYtN","type":"tool_result","content":"/Users/ryo/dev/intent-layer/eval-harness/tests/__init__.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_docker_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_scanner.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_scanner_integration.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_integration.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_models.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_prompt_builder.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_agent_config.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_index_cache.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_cli.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_claude_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_ops.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_task_runner.py\n/Users/ryo/dev/intent-layer/eval-harness/tests/test_task_runner_cache.py"}]},"uuid":"1ba36e41-7afe-44e2-a7ed-4f1c059e5e65","timestamp":"2026-02-17T20:03:46.478Z","toolUseResult":{"filenames":["/Users/ryo/dev/intent-layer/eval-harness/tests/__init__.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_docker_runner.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_scanner.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_scanner_integration.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_integration.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_models.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_prompt_builder.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_agent_config.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_index_cache.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_cli.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_claude_runner.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_git_ops.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_task_runner.py","/Users/ryo/dev/intent-layer/eval-harness/tests/test_task_runner_cache.py"],"durationMs":313,"numFiles":15,"truncated":false},"sourceToolAssistantUUID":"fb774261-17ab-48bc-b8fd-88e29ce00627"}
{"parentUuid":"1ba36e41-7afe-44e2-a7ed-4f1c059e5e65","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01Syho93simdrXv4Ti4CRMQo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","name":"Grep","input":{"pattern":"class TaskResult|class Condition","path":"/Users/ryo/dev/intent-layer/eval-harness/lib/task_runner.py","output_mode":"content","-A":15},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11843,"cache_read_input_tokens":59609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":11843},"output_tokens":8,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEErwFK2fE5z5mnMoioJ","type":"assistant","uuid":"bb6e96d6-a8a8-4423-8cc0-6b59984c2c77","timestamp":"2026-02-17T20:03:47.385Z"}
{"parentUuid":"bb6e96d6-a8a8-4423-8cc0-6b59984c2c77","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Grep","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","parentToolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","uuid":"0df75209-8257-4549-abc3-bf82f72a0c19","timestamp":"2026-02-17T20:03:47.388Z"}
{"parentUuid":"0df75209-8257-4549-abc3-bf82f72a0c19","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Grep","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","toolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","timestamp":"2026-02-17T20:03:47.688Z","uuid":"59d335b7-8623-484b-a648-59aa7fbdaf5f"}
{"parentUuid":"59d335b7-8623-484b-a648-59aa7fbdaf5f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Grep","command":"callback"},"parentToolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","toolUseID":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","timestamp":"2026-02-17T20:03:47.688Z","uuid":"1f750dea-8502-4ab9-8744-0c0552b1dce6"}
{"parentUuid":"bb6e96d6-a8a8-4423-8cc0-6b59984c2c77","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019Uq4wANMC5tbr2x4Yr8Lmw","type":"tool_result","content":"37:class Condition(Enum):\n38-    NONE = \"none\"\n39-    FLAT_LLM = \"flat_llm\"\n40-    INTENT_LAYER = \"intent_layer\"\n41-\n42-\n43-@dataclass\n44-class SkillGenerationMetrics:\n45-    wall_clock_seconds: float\n46-    input_tokens: int\n47-    output_tokens: int\n48-    cache_hit: bool = False\n49-    files_created: list[str] = field(default_factory=list)\n50-\n51-\n52-@dataclass\n53:class TaskResult:\n54-    task_id: str\n55-    condition: Condition\n56-    success: bool\n57-    test_output: str\n58-    wall_clock_seconds: float\n59-    input_tokens: int\n60-    output_tokens: int\n61-    tool_calls: int\n62-    lines_changed: int\n63-    files_touched: list[str]\n64-    skill_generation: SkillGenerationMetrics | None = None\n65-    agents_files_read: list[str] | None = None\n66-    error: str | None = None\n67-    exit_code: int | None = None\n68-    is_timeout: bool = False"}]},"uuid":"9150d0f0-5eef-4fa0-87ee-c3cf3ece211a","timestamp":"2026-02-17T20:03:47.687Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"37:class Condition(Enum):\n38-    NONE = \"none\"\n39-    FLAT_LLM = \"flat_llm\"\n40-    INTENT_LAYER = \"intent_layer\"\n41-\n42-\n43-@dataclass\n44-class SkillGenerationMetrics:\n45-    wall_clock_seconds: float\n46-    input_tokens: int\n47-    output_tokens: int\n48-    cache_hit: bool = False\n49-    files_created: list[str] = field(default_factory=list)\n50-\n51-\n52-@dataclass\n53:class TaskResult:\n54-    task_id: str\n55-    condition: Condition\n56-    success: bool\n57-    test_output: str\n58-    wall_clock_seconds: float\n59-    input_tokens: int\n60-    output_tokens: int\n61-    tool_calls: int\n62-    lines_changed: int\n63-    files_touched: list[str]\n64-    skill_generation: SkillGenerationMetrics | None = None\n65-    agents_files_read: list[str] | None = None\n66-    error: str | None = None\n67-    exit_code: int | None = None\n68-    is_timeout: bool = False","numLines":32},"sourceToolAssistantUUID":"bb6e96d6-a8a8-4423-8cc0-6b59984c2c77"}
{"parentUuid":"9150d0f0-5eef-4fa0-87ee-c3cf3ece211a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01QNA9cBT2ZdMi6KbojqggQL","type":"message","role":"assistant","content":[{"type":"text","text":"Let me also read the existing reporter tests to understand the test patterns."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1027,"cache_read_input_tokens":71452,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1027},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsQxSTJof6MQAgbQFD","type":"assistant","uuid":"f2550006-8b6b-4a68-8282-5bf726b77cfa","timestamp":"2026-02-17T20:03:50.801Z"}
{"parentUuid":"f2550006-8b6b-4a68-8282-5bf726b77cfa","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01QNA9cBT2ZdMi6KbojqggQL","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VhteZJZuZT1HwJL86CyuLR","name":"Read","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1027,"cache_read_input_tokens":71452,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1027},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsQxSTJof6MQAgbQFD","type":"assistant","uuid":"d8e7d004-3256-441a-aa23-e4d3d5340fed","timestamp":"2026-02-17T20:03:51.261Z"}
{"parentUuid":"d8e7d004-3256-441a-aa23-e4d3d5340fed","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","parentToolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","uuid":"894270e3-527c-4704-8c2e-d6f50d115c3c","timestamp":"2026-02-17T20:03:51.263Z"}
{"parentUuid":"894270e3-527c-4704-8c2e-d6f50d115c3c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","toolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","timestamp":"2026-02-17T20:03:51.325Z","uuid":"ac6a94b9-1cb8-4de4-9f6b-4acbd62682cd"}
{"parentUuid":"ac6a94b9-1cb8-4de4-9f6b-4acbd62682cd","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","toolUseID":"toolu_01VhteZJZuZT1HwJL86CyuLR","timestamp":"2026-02-17T20:03:51.325Z","uuid":"02a7319a-3484-41bc-a4b3-4b76eac76f5b"}
{"parentUuid":"d8e7d004-3256-441a-aa23-e4d3d5340fed","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VhteZJZuZT1HwJL86CyuLR","type":"tool_result","content":"     1→# tests/test_reporter.py\n     2→import pytest\n     3→import json\n     4→from pathlib import Path\n     5→from lib.reporter import Reporter, EvalResults\n     6→from lib.task_runner import TaskResult, Condition, SkillGenerationMetrics\n     7→\n     8→\n     9→@pytest.fixture\n    10→def three_condition_results():\n    11→    return [\n    12→        TaskResult(\n    13→            task_id=\"fix-123\",\n    14→            condition=Condition.NONE,\n    15→            success=False,\n    16→            test_output=\"FAIL\",\n    17→            wall_clock_seconds=100.0,\n    18→            input_tokens=5000,\n    19→            output_tokens=2000,\n    20→            tool_calls=20,\n    21→            lines_changed=50,\n    22→            files_touched=[\"a.py\", \"b.py\"]\n    23→        ),\n    24→        TaskResult(\n    25→            task_id=\"fix-123\",\n    26→            condition=Condition.FLAT_LLM,\n    27→            success=True,\n    28→            test_output=\"PASS\",\n    29→            wall_clock_seconds=80.0,\n    30→            input_tokens=4000,\n    31→            output_tokens=1500,\n    32→            tool_calls=15,\n    33→            lines_changed=30,\n    34→            files_touched=[\"a.py\"],\n    35→            skill_generation=SkillGenerationMetrics(\n    36→                wall_clock_seconds=20.0,\n    37→                input_tokens=1000,\n    38→                output_tokens=300\n    39→            )\n    40→        ),\n    41→        TaskResult(\n    42→            task_id=\"fix-123\",\n    43→            condition=Condition.INTENT_LAYER,\n    44→            success=True,\n    45→            test_output=\"PASS\",\n    46→            wall_clock_seconds=60.0,\n    47→            input_tokens=3000,\n    48→            output_tokens=1000,\n    49→            tool_calls=10,\n    50→            lines_changed=25,\n    51→            files_touched=[\"a.py\"],\n    52→            skill_generation=SkillGenerationMetrics(\n    53→                wall_clock_seconds=30.0,\n    54→                input_tokens=2000,\n    55→                output_tokens=500\n    56→            )\n    57→        ),\n    58→    ]\n    59→\n    60→\n    61→def test_three_condition_compilation(three_condition_results):\n    62→    \"\"\"All three conditions present — none, flat_llm, intent_layer.\"\"\"\n    63→    reporter = Reporter(output_dir=\"/tmp\")\n    64→    eval_results = reporter.compile_results(three_condition_results)\n    65→\n    66→    assert len(eval_results.results) == 1\n    67→    task = eval_results.results[0]\n    68→\n    69→    assert task[\"task_id\"] == \"fix-123\"\n    70→    assert task[\"none\"] is not None\n    71→    assert task[\"flat_llm\"] is not None\n    72→    assert task[\"intent_layer\"] is not None\n    73→\n    74→    # none should be flat structure (no skill_generation)\n    75→    assert task[\"none\"][\"success\"] is False\n    76→    assert task[\"none\"][\"wall_clock_seconds\"] == 100.0\n    77→    assert \"fix_only\" not in task[\"none\"]\n    78→\n    79→    # flat_llm should have three-level structure\n    80→    assert task[\"flat_llm\"][\"success\"] is True\n    81→    assert \"fix_only\" in task[\"flat_llm\"]\n    82→    assert task[\"flat_llm\"][\"fix_only\"][\"wall_clock_seconds\"] == 80.0\n    83→\n    84→    # intent_layer should have three-level structure\n    85→    assert task[\"intent_layer\"][\"success\"] is True\n    86→    assert \"fix_only\" in task[\"intent_layer\"]\n    87→    assert task[\"intent_layer\"][\"fix_only\"][\"wall_clock_seconds\"] == 60.0\n    88→\n    89→\n    90→def test_deltas_relative_to_none(three_condition_results):\n    91→    \"\"\"Both flat_llm and intent_layer get deltas against the none baseline.\"\"\"\n    92→    reporter = Reporter(output_dir=\"/tmp\")\n    93→    eval_results = reporter.compile_results(three_condition_results)\n    94→\n    95→    task = eval_results.results[0]\n    96→    deltas = task[\"deltas\"]\n    97→\n    98→    # flat_llm delta: time (80 - 100) / 100 = -20%\n    99→    assert deltas[\"flat_llm\"][\"time_percent\"] == \"-20.0%\"\n   100→    # flat_llm delta: tokens (5500 - 7000) / 7000 = -21.4%\n   101→    assert deltas[\"flat_llm\"][\"tokens_percent\"] == \"-21.4%\"\n   102→    # flat_llm delta: success +1 (True - False), single run format\n   103→    assert deltas[\"flat_llm\"][\"success_rate_delta\"] == \"+1\"\n   104→\n   105→    # intent_layer delta: time (60 - 100) / 100 = -40%\n   106→    assert deltas[\"intent_layer\"][\"time_percent\"] == \"-40.0%\"\n   107→    # intent_layer delta: tokens (4000 - 7000) / 7000 = -42.9%\n   108→    assert deltas[\"intent_layer\"][\"tokens_percent\"] == \"-42.9%\"\n   109→    assert deltas[\"intent_layer\"][\"success_rate_delta\"] == \"+1\"\n   110→\n   111→\n   112→def test_zero_baseline_delta():\n   113→    \"\"\"If none has 0 time/tokens, don't divide by zero.\"\"\"\n   114→    results = [\n   115→        TaskResult(\n   116→            task_id=\"fix-zero\",\n   117→            condition=Condition.NONE,\n   118→            success=False,\n   119→            test_output=\"FAIL\",\n   120→            wall_clock_seconds=0.0,\n   121→            input_tokens=0,\n   122→            output_tokens=0,\n   123→            tool_calls=0,\n   124→            lines_changed=0,\n   125→            files_touched=[]\n   126→        ),\n   127→        TaskResult(\n   128→            task_id=\"fix-zero\",\n   129→            condition=Condition.FLAT_LLM,\n   130→            success=True,\n   131→            test_output=\"PASS\",\n   132→            wall_clock_seconds=50.0,\n   133→            input_tokens=3000,\n   134→            output_tokens=1000,\n   135→            tool_calls=10,\n   136→            lines_changed=20,\n   137→            files_touched=[\"a.py\"],\n   138→            skill_generation=SkillGenerationMetrics(\n   139→                wall_clock_seconds=10.0,\n   140→                input_tokens=500,\n   141→                output_tokens=100\n   142→            )\n   143→        ),\n   144→    ]\n   145→\n   146→    reporter = Reporter(output_dir=\"/tmp\")\n   147→    eval_results = reporter.compile_results(results)\n   148→\n   149→    task = eval_results.results[0]\n   150→    delta = task[\"deltas\"][\"flat_llm\"]\n   151→\n   152→    # Should not raise ZeroDivisionError; uses 0 fallback\n   153→    assert delta[\"time_percent\"] == \"+0.0%\"\n   154→    assert delta[\"tokens_percent\"] == \"+0.0%\"\n   155→    assert delta[\"tool_calls_percent\"] == \"+0.0%\"\n   156→    assert delta[\"lines_changed_percent\"] == \"+0.0%\"\n   157→\n   158→\n   159→def test_missing_condition():\n   160→    \"\"\"Only 2 of 3 conditions run — missing one is None.\"\"\"\n   161→    results = [\n   162→        TaskResult(\n   163→            task_id=\"fix-partial\",\n   164→            condition=Condition.NONE,\n   165→            success=True,\n   166→            test_output=\"PASS\",\n   167→            wall_clock_seconds=90.0,\n   168→            input_tokens=4000,\n   169→            output_tokens=1500,\n   170→            tool_calls=18,\n   171→            lines_changed=40,\n   172→            files_touched=[\"x.py\"]\n   173→        ),\n   174→        TaskResult(\n   175→            task_id=\"fix-partial\",\n   176→            condition=Condition.INTENT_LAYER,\n   177→            success=True,\n   178→            test_output=\"PASS\",\n   179→            wall_clock_seconds=50.0,\n   180→            input_tokens=2000,\n   181→            output_tokens=800,\n   182→            tool_calls=8,\n   183→            lines_changed=20,\n   184→            files_touched=[\"x.py\"],\n   185→            skill_generation=SkillGenerationMetrics(\n   186→                wall_clock_seconds=25.0,\n   187→                input_tokens=1500,\n   188→                output_tokens=400\n   189→            )\n   190→        ),\n   191→    ]\n   192→\n   193→    reporter = Reporter(output_dir=\"/tmp\")\n   194→    eval_results = reporter.compile_results(results)\n   195→\n   196→    task = eval_results.results[0]\n   197→    assert task[\"none\"] is not None\n   198→    assert task[\"flat_llm\"] is None\n   199→    assert task[\"intent_layer\"] is not None\n   200→\n   201→    # flat_llm delta should be empty (missing condition)\n   202→    assert task[\"deltas\"][\"flat_llm\"] == {}\n   203→    # intent_layer delta should exist\n   204→    assert task[\"deltas\"][\"intent_layer\"][\"time_percent\"] == \"-44.4%\"\n   205→\n   206→\n   207→def test_summary_three_success_rates(three_condition_results):\n   208→    \"\"\"Summary has all three success rates.\"\"\"\n   209→    reporter = Reporter(output_dir=\"/tmp\")\n   210→    eval_results = reporter.compile_results(three_condition_results)\n   211→\n   212→    summary = eval_results.summary\n   213→    assert summary[\"total_tasks\"] == 1\n   214→    assert summary[\"infrastructure_errors\"] == 0\n   215→    assert summary[\"none_success_rate\"] == 0.0\n   216→    assert summary[\"flat_llm_success_rate\"] == 1.0\n   217→    assert summary[\"intent_layer_success_rate\"] == 1.0\n   218→\n   219→\n   220→def test_markdown_multi_row_layout(tmp_path, three_condition_results):\n   221→    \"\"\"Markdown output has multi-row layout with correct structure.\"\"\"\n   222→    reporter = Reporter(output_dir=str(tmp_path))\n   223→    eval_results = reporter.compile_results(three_condition_results)\n   224→    md_path = reporter.write_markdown(eval_results)\n   225→\n   226→    with open(md_path) as f:\n   227→        content = f.read()\n   228→\n   229→    # Header has new columns\n   230→    assert \"| Task | Condition | Success |\" in content\n   231→    assert \"| Time (s) |\" in content\n   232→\n   233→    # Summary has all three rates\n   234→    assert \"None success rate\" in content\n   235→    assert \"Flat LLM success rate\" in content\n   236→    assert \"Intent Layer success rate\" in content\n   237→\n   238→    lines = content.split(\"\\n\")\n   239→\n   240→    # Find data rows for fix-123\n   241→    data_rows = [l for l in lines if l.startswith(\"| fix-123 |\")]\n   242→    assert len(data_rows) == 3, f\"Expected 3 rows for fix-123, got {len(data_rows)}\"\n   243→\n   244→    # First row: none condition, baseline deltas show em-dash\n   245→    none_row = data_rows[0]\n   246→    assert \"| none |\" in none_row\n   247→    assert \"| FAIL |\" in none_row\n   248→    assert \"\\u2014\" in none_row  # em-dash for baseline\n   249→\n   250→    # Second row: flat_llm\n   251→    flat_row = data_rows[1]\n   252→    assert \"| flat_llm |\" in flat_row\n   253→    assert \"| PASS |\" in flat_row\n   254→    assert \"-20.0%\" in flat_row  # time delta\n   255→\n   256→    # Third row: intent_layer\n   257→    il_row = data_rows[2]\n   258→    assert \"| intent_layer |\" in il_row\n   259→    assert \"| PASS |\" in il_row\n   260→    assert \"-40.0%\" in il_row  # time delta\n   261→\n   262→\n   263→def test_json_output(tmp_path, three_condition_results):\n   264→    \"\"\"JSON output writes correctly with new structure.\"\"\"\n   265→    reporter = Reporter(output_dir=str(tmp_path))\n   266→    eval_results = reporter.compile_results(three_condition_results)\n   267→    json_path = reporter.write_json(eval_results)\n   268→\n   269→    assert Path(json_path).exists()\n   270→    with open(json_path) as f:\n   271→        data = json.load(f)\n   272→\n   273→    assert \"results\" in data\n   274→    assert \"summary\" in data\n   275→    assert len(data[\"results\"]) == 1\n   276→\n   277→    task = data[\"results\"][0]\n   278→    assert \"none\" in task\n   279→    assert \"flat_llm\" in task\n   280→    assert \"intent_layer\" in task\n   281→    assert \"deltas\" in task\n   282→\n   283→\n   284→def test_infrastructure_errors_excluded_from_success_rate():\n   285→    \"\"\"Infrastructure errors should not count toward success rates.\"\"\"\n   286→    results = [\n   287→        TaskResult(\n   288→            task_id=\"fix-good\",\n   289→            condition=Condition.NONE,\n   290→            success=True,\n   291→            test_output=\"PASS\",\n   292→            wall_clock_seconds=50.0,\n   293→            input_tokens=2000,\n   294→            output_tokens=1000,\n   295→            tool_calls=10,\n   296→            lines_changed=20,\n   297→            files_touched=[\"a.py\"]\n   298→        ),\n   299→        TaskResult(\n   300→            task_id=\"fix-infra-fail\",\n   301→            condition=Condition.NONE,\n   302→            success=False,\n   303→            test_output=\"\",\n   304→            wall_clock_seconds=0,\n   305→            input_tokens=0,\n   306→            output_tokens=0,\n   307→            tool_calls=0,\n   308→            lines_changed=0,\n   309→            files_touched=[],\n   310→            error=\"[infrastructure] clone failed: network timeout\"\n   311→        ),\n   312→    ]\n   313→\n   314→    reporter = Reporter(output_dir=\"/tmp\")\n   315→    eval_results = reporter.compile_results(results)\n   316→\n   317→    summary = eval_results.summary\n   318→    assert summary[\"infrastructure_errors\"] == 1\n   319→    # Only fix-good counts — 1 success out of 1 valid = 1.0\n   320→    assert summary[\"none_success_rate\"] == 1.0\n   321→\n   322→\n   323→def test_pre_validation_and_skill_gen_errors_excluded():\n   324→    \"\"\"Pre-validation and skill-generation errors are also excluded from stats.\"\"\"\n   325→    results = [\n   326→        TaskResult(\n   327→            task_id=\"fix-ok\",\n   328→            condition=Condition.NONE,\n   329→            success=True,\n   330→            test_output=\"PASS\",\n   331→            wall_clock_seconds=50.0,\n   332→            input_tokens=2000,\n   333→            output_tokens=1000,\n   334→            tool_calls=10,\n   335→            lines_changed=20,\n   336→            files_touched=[\"a.py\"]\n   337→        ),\n   338→        TaskResult(\n   339→            task_id=\"fix-preval\",\n   340→            condition=Condition.NONE,\n   341→            success=False,\n   342→            test_output=\"\",\n   343→            wall_clock_seconds=0,\n   344→            input_tokens=0,\n   345→            output_tokens=0,\n   346→            tool_calls=0,\n   347→            lines_changed=0,\n   348→            files_touched=[],\n   349→            error=\"[pre-validation] test doesn't fail at pre_fix_commit\"\n   350→        ),\n   351→        TaskResult(\n   352→            task_id=\"fix-skillgen\",\n   353→            condition=Condition.INTENT_LAYER,\n   354→            success=False,\n   355→            test_output=\"\",\n   356→            wall_clock_seconds=0,\n   357→            input_tokens=0,\n   358→            output_tokens=0,\n   359→            tool_calls=0,\n   360→            lines_changed=0,\n   361→            files_touched=[],\n   362→            error=\"[skill-generation] no files created\"\n   363→        ),\n   364→    ]\n   365→\n   366→    reporter = Reporter(output_dir=\"/tmp\")\n   367→    eval_results = reporter.compile_results(results)\n   368→\n   369→    summary = eval_results.summary\n   370→    assert summary[\"infrastructure_errors\"] == 2\n   371→    assert summary[\"none_success_rate\"] == 1.0\n   372→    assert summary[\"intent_layer_success_rate\"] == 0\n   373→\n   374→\n   375→def test_multi_run_serialize_condition():\n   376→    \"\"\"Multiple runs produce success_rate, median, and runs array.\"\"\"\n   377→    reporter = Reporter(output_dir=\"/tmp\")\n   378→\n   379→    runs = [\n   380→        TaskResult(\n   381→            task_id=\"fix-multi\",\n   382→            condition=Condition.NONE,\n   383→            success=True,\n   384→            test_output=\"PASS\",\n   385→            wall_clock_seconds=100.0,\n   386→            input_tokens=5000,\n   387→            output_tokens=2000,\n   388→            tool_calls=20,\n   389→            lines_changed=50,\n   390→            files_touched=[\"a.py\"]\n   391→        ),\n   392→        TaskResult(\n   393→            task_id=\"fix-multi\",\n   394→            condition=Condition.NONE,\n   395→            success=False,\n   396→            test_output=\"FAIL\",\n   397→            wall_clock_seconds=120.0,\n   398→            input_tokens=6000,\n   399→            output_tokens=2500,\n   400→            tool_calls=25,\n   401→            lines_changed=60,\n   402→            files_touched=[\"a.py\", \"b.py\"]\n   403→        ),\n   404→        TaskResult(\n   405→            task_id=\"fix-multi\",\n   406→            condition=Condition.NONE,\n   407→            success=True,\n   408→            test_output=\"PASS\",\n   409→            wall_clock_seconds=90.0,\n   410→            input_tokens=4500,\n   411→            output_tokens=1800,\n   412→            tool_calls=18,\n   413→            lines_changed=45,\n   414→            files_touched=[\"a.py\"]\n   415→        ),\n   416→    ]\n   417→\n   418→    result = reporter._serialize_condition(runs)\n   419→\n   420→    # Success rate: 2/3\n   421→    assert result[\"success_rate\"] == 0.67\n   422→    assert result[\"success\"] is True  # majority pass\n   423→    assert result[\"successes\"] == 2\n   424→    assert result[\"total_valid_runs\"] == 3\n   425→\n   426→    # Median efficiency (sorted: 90, 100, 120 → median 100)\n   427→    assert result[\"median\"][\"wall_clock_seconds\"] == 100.0\n   428→    assert result[\"median\"][\"tool_calls\"] == 20\n   429→\n   430→    # Individual runs preserved\n   431→    assert len(result[\"runs\"]) == 3\n   432→    assert result[\"runs\"][0][\"success\"] is True\n   433→    assert result[\"runs\"][1][\"success\"] is False\n   434→\n   435→\n   436→def test_multi_run_delta():\n   437→    \"\"\"Multiple runs compute deltas using medians and success rate.\"\"\"\n   438→    reporter = Reporter(output_dir=\"/tmp\")\n   439→\n   440→    baseline = [\n   441→        TaskResult(\n   442→            task_id=\"fix-delta\", condition=Condition.NONE, success=True,\n   443→            test_output=\"PASS\", wall_clock_seconds=100.0,\n   444→            input_tokens=5000, output_tokens=2000, tool_calls=20,\n   445→            lines_changed=50, files_touched=[\"a.py\"]\n   446→        ),\n   447→        TaskResult(\n   448→            task_id=\"fix-delta\", condition=Condition.NONE, success=False,\n   449→            test_output=\"FAIL\", wall_clock_seconds=100.0,\n   450→            input_tokens=5000, output_tokens=2000, tool_calls=20,\n   451→            lines_changed=50, files_touched=[\"a.py\"]\n   452→        ),\n   453→    ]\n   454→    treatment = [\n   455→        TaskResult(\n   456→            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n   457→            test_output=\"PASS\", wall_clock_seconds=80.0,\n   458→            input_tokens=4000, output_tokens=1500, tool_calls=15,\n   459→            lines_changed=30, files_touched=[\"a.py\"]\n   460→        ),\n   461→        TaskResult(\n   462→            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n   463→            test_output=\"PASS\", wall_clock_seconds=80.0,\n   464→            input_tokens=4000, output_tokens=1500, tool_calls=15,\n   465→            lines_changed=30, files_touched=[\"a.py\"]\n   466→        ),\n   467→    ]\n   468→\n   469→    delta = reporter._compute_delta(baseline, treatment)\n   470→\n   471→    # Success rate: baseline 50%, treatment 100%, delta +50%\n   472→    assert delta[\"success_rate_delta\"] == \"+50%\"\n   473→    # Time: (80 - 100) / 100 = -20%\n   474→    assert delta[\"time_percent\"] == \"-20.0%\"\n   475→    # Tokens: (5500 - 7000) / 7000 = -21.4%\n   476→    assert delta[\"tokens_percent\"] == \"-21.4%\"\n   477→\n   478→\n   479→def test_get_fix_metrics_all_formats():\n   480→    \"\"\"_get_fix_metrics extracts correctly from all three data shapes.\"\"\"\n   481→    # Multi-run median format\n   482→    multi = {\"median\": {\n   483→        \"wall_clock_seconds\": 50.0,\n   484→        \"input_tokens\": 3000, \"output_tokens\": 1000,\n   485→        \"tool_calls\": 10, \"lines_changed\": 25,\n   486→    }}\n   487→    m = Reporter._get_fix_metrics(multi)\n   488→    assert m[\"wall_clock_seconds\"] == 50.0\n   489→    assert m[\"tokens\"] == 4000\n   490→    assert m[\"tool_calls\"] == 10\n   491→    assert m[\"lines_changed\"] == 25\n   492→\n   493→    # Single-run with skill_generation (fix_only)\n   494→    single_sg = {\"fix_only\": {\n   495→        \"wall_clock_seconds\": 80.0,\n   496→        \"input_tokens\": 4000, \"output_tokens\": 1500,\n   497→        \"tool_calls\": 15, \"lines_changed\": 30,\n   498→    }}\n   499→    s = Reporter._get_fix_metrics(single_sg)\n   500→    assert s[\"wall_clock_seconds\"] == 80.0\n   501→    assert s[\"tokens\"] == 5500\n   502→\n   503→    # Single-run flat (no skill_generation)\n   504→    flat = {\n   505→        \"wall_clock_seconds\": 100.0,\n   506→        \"input_tokens\": 5000, \"output_tokens\": 2000,\n   507→        \"tool_calls\": 20, \"lines_changed\": 50,\n   508→    }\n   509→    f = Reporter._get_fix_metrics(flat)\n   510→    assert f[\"wall_clock_seconds\"] == 100.0\n   511→    assert f[\"tokens\"] == 7000\n   512→\n   513→\n   514→def test_serialize_includes_exit_code_and_timeout():\n   515→    \"\"\"JSON output includes exit_code and is_timeout when present.\"\"\"\n   516→    reporter = Reporter(output_dir=\"/tmp\")\n   517→\n   518→    result = TaskResult(\n   519→        task_id=\"fix-meta\",\n   520→        condition=Condition.NONE,\n   521→        success=False,\n   522→        test_output=\"FAIL\",\n   523→        wall_clock_seconds=300.0,\n   524→        input_tokens=50000,\n   525→        output_tokens=3000,\n   526→        tool_calls=5,\n   527→        lines_changed=0,\n   528→        files_touched=[],\n   529→        error=\"[timeout] Claude timed out after 300.0s\",\n   530→        exit_code=-1,\n   531→        is_timeout=True,\n   532→    )\n   533→\n   534→    serialized = reporter._serialize_single_result(result)\n   535→\n   536→    assert serialized[\"exit_code\"] == -1\n   537→    assert serialized[\"is_timeout\"] is True\n   538→    assert serialized[\"error\"] == \"[timeout] Claude timed out after 300.0s\"\n   539→\n   540→\n   541→def test_serialize_omits_exit_code_when_none():\n   542→    \"\"\"JSON output omits exit_code when it's None (backward compat).\"\"\"\n   543→    reporter = Reporter(output_dir=\"/tmp\")\n   544→\n   545→    result = TaskResult(\n   546→        task_id=\"fix-ok\",\n   547→        condition=Condition.NONE,\n   548→        success=True,\n   549→        test_output=\"PASS\",\n   550→        wall_clock_seconds=50.0,\n   551→        input_tokens=2000,\n   552→        output_tokens=1000,\n   553→        tool_calls=10,\n   554→        lines_changed=20,\n   555→        files_touched=[\"a.py\"],\n   556→    )\n   557→\n   558→    serialized = reporter._serialize_single_result(result)\n   559→\n   560→    assert \"exit_code\" not in serialized\n   561→    assert \"is_timeout\" not in serialized\n   562→\n   563→\n   564→def test_single_run_delta_uses_success_rate_delta_key():\n   565→    \"\"\"Single-run deltas use 'success_rate_delta' (not 'success').\"\"\"\n   566→    reporter = Reporter(output_dir=\"/tmp\")\n   567→    baseline = [TaskResult(\n   568→        task_id=\"fix-key\", condition=Condition.NONE, success=False,\n   569→        test_output=\"FAIL\", wall_clock_seconds=100.0,\n   570→        input_tokens=5000, output_tokens=2000, tool_calls=20,\n   571→        lines_changed=50, files_touched=[\"a.py\"]\n   572→    )]\n   573→    treatment = [TaskResult(\n   574→        task_id=\"fix-key\", condition=Condition.FLAT_LLM, success=True,\n   575→        test_output=\"PASS\", wall_clock_seconds=80.0,\n   576→        input_tokens=4000, output_tokens=1500, tool_calls=15,\n   577→        lines_changed=30, files_touched=[\"a.py\"]\n   578→    )]\n   579→    delta = reporter._compute_delta(baseline, treatment)\n   580→    assert \"success_rate_delta\" in delta\n   581→    assert \"success\" not in delta\n   582→    assert delta[\"success_rate_delta\"] == \"+1\"\n   583→"}]},"uuid":"544ab075-8cb9-4d78-9d6f-f28ffc429ef4","timestamp":"2026-02-17T20:03:51.325Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","content":"# tests/test_reporter.py\nimport pytest\nimport json\nfrom pathlib import Path\nfrom lib.reporter import Reporter, EvalResults\nfrom lib.task_runner import TaskResult, Condition, SkillGenerationMetrics\n\n\n@pytest.fixture\ndef three_condition_results():\n    return [\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=100.0,\n            input_tokens=5000,\n            output_tokens=2000,\n            tool_calls=20,\n            lines_changed=50,\n            files_touched=[\"a.py\", \"b.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.FLAT_LLM,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=80.0,\n            input_tokens=4000,\n            output_tokens=1500,\n            tool_calls=15,\n            lines_changed=30,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=20.0,\n                input_tokens=1000,\n                output_tokens=300\n            )\n        ),\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.INTENT_LAYER,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=60.0,\n            input_tokens=3000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=25,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=30.0,\n                input_tokens=2000,\n                output_tokens=500\n            )\n        ),\n    ]\n\n\ndef test_three_condition_compilation(three_condition_results):\n    \"\"\"All three conditions present — none, flat_llm, intent_layer.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    assert len(eval_results.results) == 1\n    task = eval_results.results[0]\n\n    assert task[\"task_id\"] == \"fix-123\"\n    assert task[\"none\"] is not None\n    assert task[\"flat_llm\"] is not None\n    assert task[\"intent_layer\"] is not None\n\n    # none should be flat structure (no skill_generation)\n    assert task[\"none\"][\"success\"] is False\n    assert task[\"none\"][\"wall_clock_seconds\"] == 100.0\n    assert \"fix_only\" not in task[\"none\"]\n\n    # flat_llm should have three-level structure\n    assert task[\"flat_llm\"][\"success\"] is True\n    assert \"fix_only\" in task[\"flat_llm\"]\n    assert task[\"flat_llm\"][\"fix_only\"][\"wall_clock_seconds\"] == 80.0\n\n    # intent_layer should have three-level structure\n    assert task[\"intent_layer\"][\"success\"] is True\n    assert \"fix_only\" in task[\"intent_layer\"]\n    assert task[\"intent_layer\"][\"fix_only\"][\"wall_clock_seconds\"] == 60.0\n\n\ndef test_deltas_relative_to_none(three_condition_results):\n    \"\"\"Both flat_llm and intent_layer get deltas against the none baseline.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    task = eval_results.results[0]\n    deltas = task[\"deltas\"]\n\n    # flat_llm delta: time (80 - 100) / 100 = -20%\n    assert deltas[\"flat_llm\"][\"time_percent\"] == \"-20.0%\"\n    # flat_llm delta: tokens (5500 - 7000) / 7000 = -21.4%\n    assert deltas[\"flat_llm\"][\"tokens_percent\"] == \"-21.4%\"\n    # flat_llm delta: success +1 (True - False), single run format\n    assert deltas[\"flat_llm\"][\"success_rate_delta\"] == \"+1\"\n\n    # intent_layer delta: time (60 - 100) / 100 = -40%\n    assert deltas[\"intent_layer\"][\"time_percent\"] == \"-40.0%\"\n    # intent_layer delta: tokens (4000 - 7000) / 7000 = -42.9%\n    assert deltas[\"intent_layer\"][\"tokens_percent\"] == \"-42.9%\"\n    assert deltas[\"intent_layer\"][\"success_rate_delta\"] == \"+1\"\n\n\ndef test_zero_baseline_delta():\n    \"\"\"If none has 0 time/tokens, don't divide by zero.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-zero\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=0.0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[]\n        ),\n        TaskResult(\n            task_id=\"fix-zero\",\n            condition=Condition.FLAT_LLM,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=3000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=10.0,\n                input_tokens=500,\n                output_tokens=100\n            )\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    task = eval_results.results[0]\n    delta = task[\"deltas\"][\"flat_llm\"]\n\n    # Should not raise ZeroDivisionError; uses 0 fallback\n    assert delta[\"time_percent\"] == \"+0.0%\"\n    assert delta[\"tokens_percent\"] == \"+0.0%\"\n    assert delta[\"tool_calls_percent\"] == \"+0.0%\"\n    assert delta[\"lines_changed_percent\"] == \"+0.0%\"\n\n\ndef test_missing_condition():\n    \"\"\"Only 2 of 3 conditions run — missing one is None.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-partial\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=90.0,\n            input_tokens=4000,\n            output_tokens=1500,\n            tool_calls=18,\n            lines_changed=40,\n            files_touched=[\"x.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-partial\",\n            condition=Condition.INTENT_LAYER,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=800,\n            tool_calls=8,\n            lines_changed=20,\n            files_touched=[\"x.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=25.0,\n                input_tokens=1500,\n                output_tokens=400\n            )\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    task = eval_results.results[0]\n    assert task[\"none\"] is not None\n    assert task[\"flat_llm\"] is None\n    assert task[\"intent_layer\"] is not None\n\n    # flat_llm delta should be empty (missing condition)\n    assert task[\"deltas\"][\"flat_llm\"] == {}\n    # intent_layer delta should exist\n    assert task[\"deltas\"][\"intent_layer\"][\"time_percent\"] == \"-44.4%\"\n\n\ndef test_summary_three_success_rates(three_condition_results):\n    \"\"\"Summary has all three success rates.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    summary = eval_results.summary\n    assert summary[\"total_tasks\"] == 1\n    assert summary[\"infrastructure_errors\"] == 0\n    assert summary[\"none_success_rate\"] == 0.0\n    assert summary[\"flat_llm_success_rate\"] == 1.0\n    assert summary[\"intent_layer_success_rate\"] == 1.0\n\n\ndef test_markdown_multi_row_layout(tmp_path, three_condition_results):\n    \"\"\"Markdown output has multi-row layout with correct structure.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    # Header has new columns\n    assert \"| Task | Condition | Success |\" in content\n    assert \"| Time (s) |\" in content\n\n    # Summary has all three rates\n    assert \"None success rate\" in content\n    assert \"Flat LLM success rate\" in content\n    assert \"Intent Layer success rate\" in content\n\n    lines = content.split(\"\\n\")\n\n    # Find data rows for fix-123\n    data_rows = [l for l in lines if l.startswith(\"| fix-123 |\")]\n    assert len(data_rows) == 3, f\"Expected 3 rows for fix-123, got {len(data_rows)}\"\n\n    # First row: none condition, baseline deltas show em-dash\n    none_row = data_rows[0]\n    assert \"| none |\" in none_row\n    assert \"| FAIL |\" in none_row\n    assert \"\\u2014\" in none_row  # em-dash for baseline\n\n    # Second row: flat_llm\n    flat_row = data_rows[1]\n    assert \"| flat_llm |\" in flat_row\n    assert \"| PASS |\" in flat_row\n    assert \"-20.0%\" in flat_row  # time delta\n\n    # Third row: intent_layer\n    il_row = data_rows[2]\n    assert \"| intent_layer |\" in il_row\n    assert \"| PASS |\" in il_row\n    assert \"-40.0%\" in il_row  # time delta\n\n\ndef test_json_output(tmp_path, three_condition_results):\n    \"\"\"JSON output writes correctly with new structure.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    json_path = reporter.write_json(eval_results)\n\n    assert Path(json_path).exists()\n    with open(json_path) as f:\n        data = json.load(f)\n\n    assert \"results\" in data\n    assert \"summary\" in data\n    assert len(data[\"results\"]) == 1\n\n    task = data[\"results\"][0]\n    assert \"none\" in task\n    assert \"flat_llm\" in task\n    assert \"intent_layer\" in task\n    assert \"deltas\" in task\n\n\ndef test_infrastructure_errors_excluded_from_success_rate():\n    \"\"\"Infrastructure errors should not count toward success rates.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-good\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-infra-fail\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[infrastructure] clone failed: network timeout\"\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    summary = eval_results.summary\n    assert summary[\"infrastructure_errors\"] == 1\n    # Only fix-good counts — 1 success out of 1 valid = 1.0\n    assert summary[\"none_success_rate\"] == 1.0\n\n\ndef test_pre_validation_and_skill_gen_errors_excluded():\n    \"\"\"Pre-validation and skill-generation errors are also excluded from stats.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-ok\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-preval\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[pre-validation] test doesn't fail at pre_fix_commit\"\n        ),\n        TaskResult(\n            task_id=\"fix-skillgen\",\n            condition=Condition.INTENT_LAYER,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[skill-generation] no files created\"\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    summary = eval_results.summary\n    assert summary[\"infrastructure_errors\"] == 2\n    assert summary[\"none_success_rate\"] == 1.0\n    assert summary[\"intent_layer_success_rate\"] == 0\n\n\ndef test_multi_run_serialize_condition():\n    \"\"\"Multiple runs produce success_rate, median, and runs array.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    runs = [\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=100.0,\n            input_tokens=5000,\n            output_tokens=2000,\n            tool_calls=20,\n            lines_changed=50,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=120.0,\n            input_tokens=6000,\n            output_tokens=2500,\n            tool_calls=25,\n            lines_changed=60,\n            files_touched=[\"a.py\", \"b.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=90.0,\n            input_tokens=4500,\n            output_tokens=1800,\n            tool_calls=18,\n            lines_changed=45,\n            files_touched=[\"a.py\"]\n        ),\n    ]\n\n    result = reporter._serialize_condition(runs)\n\n    # Success rate: 2/3\n    assert result[\"success_rate\"] == 0.67\n    assert result[\"success\"] is True  # majority pass\n    assert result[\"successes\"] == 2\n    assert result[\"total_valid_runs\"] == 3\n\n    # Median efficiency (sorted: 90, 100, 120 → median 100)\n    assert result[\"median\"][\"wall_clock_seconds\"] == 100.0\n    assert result[\"median\"][\"tool_calls\"] == 20\n\n    # Individual runs preserved\n    assert len(result[\"runs\"]) == 3\n    assert result[\"runs\"][0][\"success\"] is True\n    assert result[\"runs\"][1][\"success\"] is False\n\n\ndef test_multi_run_delta():\n    \"\"\"Multiple runs compute deltas using medians and success rate.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    baseline = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=False,\n            test_output=\"FAIL\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n    ]\n    treatment = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n    ]\n\n    delta = reporter._compute_delta(baseline, treatment)\n\n    # Success rate: baseline 50%, treatment 100%, delta +50%\n    assert delta[\"success_rate_delta\"] == \"+50%\"\n    # Time: (80 - 100) / 100 = -20%\n    assert delta[\"time_percent\"] == \"-20.0%\"\n    # Tokens: (5500 - 7000) / 7000 = -21.4%\n    assert delta[\"tokens_percent\"] == \"-21.4%\"\n\n\ndef test_get_fix_metrics_all_formats():\n    \"\"\"_get_fix_metrics extracts correctly from all three data shapes.\"\"\"\n    # Multi-run median format\n    multi = {\"median\": {\n        \"wall_clock_seconds\": 50.0,\n        \"input_tokens\": 3000, \"output_tokens\": 1000,\n        \"tool_calls\": 10, \"lines_changed\": 25,\n    }}\n    m = Reporter._get_fix_metrics(multi)\n    assert m[\"wall_clock_seconds\"] == 50.0\n    assert m[\"tokens\"] == 4000\n    assert m[\"tool_calls\"] == 10\n    assert m[\"lines_changed\"] == 25\n\n    # Single-run with skill_generation (fix_only)\n    single_sg = {\"fix_only\": {\n        \"wall_clock_seconds\": 80.0,\n        \"input_tokens\": 4000, \"output_tokens\": 1500,\n        \"tool_calls\": 15, \"lines_changed\": 30,\n    }}\n    s = Reporter._get_fix_metrics(single_sg)\n    assert s[\"wall_clock_seconds\"] == 80.0\n    assert s[\"tokens\"] == 5500\n\n    # Single-run flat (no skill_generation)\n    flat = {\n        \"wall_clock_seconds\": 100.0,\n        \"input_tokens\": 5000, \"output_tokens\": 2000,\n        \"tool_calls\": 20, \"lines_changed\": 50,\n    }\n    f = Reporter._get_fix_metrics(flat)\n    assert f[\"wall_clock_seconds\"] == 100.0\n    assert f[\"tokens\"] == 7000\n\n\ndef test_serialize_includes_exit_code_and_timeout():\n    \"\"\"JSON output includes exit_code and is_timeout when present.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-meta\",\n        condition=Condition.NONE,\n        success=False,\n        test_output=\"FAIL\",\n        wall_clock_seconds=300.0,\n        input_tokens=50000,\n        output_tokens=3000,\n        tool_calls=5,\n        lines_changed=0,\n        files_touched=[],\n        error=\"[timeout] Claude timed out after 300.0s\",\n        exit_code=-1,\n        is_timeout=True,\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert serialized[\"exit_code\"] == -1\n    assert serialized[\"is_timeout\"] is True\n    assert serialized[\"error\"] == \"[timeout] Claude timed out after 300.0s\"\n\n\ndef test_serialize_omits_exit_code_when_none():\n    \"\"\"JSON output omits exit_code when it's None (backward compat).\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-ok\",\n        condition=Condition.NONE,\n        success=True,\n        test_output=\"PASS\",\n        wall_clock_seconds=50.0,\n        input_tokens=2000,\n        output_tokens=1000,\n        tool_calls=10,\n        lines_changed=20,\n        files_touched=[\"a.py\"],\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert \"exit_code\" not in serialized\n    assert \"is_timeout\" not in serialized\n\n\ndef test_single_run_delta_uses_success_rate_delta_key():\n    \"\"\"Single-run deltas use 'success_rate_delta' (not 'success').\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    baseline = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.NONE, success=False,\n        test_output=\"FAIL\", wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=[\"a.py\"]\n    )]\n    treatment = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.FLAT_LLM, success=True,\n        test_output=\"PASS\", wall_clock_seconds=80.0,\n        input_tokens=4000, output_tokens=1500, tool_calls=15,\n        lines_changed=30, files_touched=[\"a.py\"]\n    )]\n    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"\n","numLines":583,"startLine":1,"totalLines":583}},"sourceToolAssistantUUID":"d8e7d004-3256-441a-aa23-e4d3d5340fed"}
{"parentUuid":"544ab075-8cb9-4d78-9d6f-f28ffc429ef4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_019TSM5qz4edpjzJYaSBSTP7","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Now I have a clear picture. Let me create the three deliverables:\n\n1. `eval-harness/lib/stats.py` — Wilson Score CI + inverse normal CDF\n2. `eval-harness/tests/test_stats.py` — unit tests\n3. Modify `eval-harness/lib/reporter.py` — integrate CIs into serialization and markdown output\n\nLet me create these files. The stats.py and test_stats.py are independent, and then I'll modify reporter.py.","signature":"ErYECkYICxgCKkCvRvFHrtbXvJrGM71ZUYVO3+fC9vf2PWfhWgjn72ZKA3GJsTDMessv0UK4GlCbDILeCW1yrCRkHKRm8lZknhuREgzbttVAnQ6a8lF6CLMaDCKqt5pr50RtnQvEgyIw5fOVidI/TWomAVRf/sbAPWlytcBv2PLg4ajMjeFkY1/wtJBqwOz1TM8QSgI4Q4GaKp0DUFM0SmlY9sifsnZWpWHMsEXAPmWX+8LAnZ3iV2wbKcgQjCb1AeUQI2gwvlJZFp2bvb7yJttsuqMuouwilqdT4pl7el6whWUYuNNAjWzeXUlp68BereyZKtJJKJD9+JCjFKNkMIyic63B564uMKS+oJc6HT9+1p8YGdJTttXNhghd9dEhfxQSHK3Doy33b4l8QoYUXjpXb41772qPpFzImgVbfp0boGWvlo5Z0h1G+M3M4zq4moSuJLtamE0u5rkTqBf0EfALU3FKduQYeRThy5QnCqs7BCsjf3GhYSOcqWeYyE6o0lRhjYQNdJd4cOUOP1K4hh/LyfezF3BaGsoumGV66TkRoyUHDvHALZFhSOQB8DiNb2mwkNJpKrZM/T3y+NFWPGLbCcq/ayHbFIdd5oubNoDutMGxdngIjD9wKMjQwom3hwmrSqdbBDTHsi4iJ/ht+8SS6kNlKZGLz4Ox5eDaRxH+D5ZLLQANRX6Z2cAAbIknZwWpKKUWaVJFDY09Y1bAF5C/Ui8f7fq31Zzl3XgqIsuoYyNsyRbaZg0YAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7680,"cache_read_input_tokens":72479,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7680},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsggF8Z7wooRNorFj7","type":"assistant","uuid":"1a424d14-3969-4c9b-afc1-6ea89e6008a8","timestamp":"2026-02-17T20:03:56.719Z"}
{"parentUuid":"1a424d14-3969-4c9b-afc1-6ea89e6008a8","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_019TSM5qz4edpjzJYaSBSTP7","type":"message","role":"assistant","content":[{"type":"text","text":"Good, I have everything I need. Let me create the stats module and tests in parallel, then modify the reporter.\n\n`★ Insight ─────────────────────────────────────`\nThe Cerberus TypeScript uses a three-region piecewise rational approximation for the inverse normal CDF. The lower and upper tails use the same coefficients (`c1-c6`, `d1-d4`) but the upper tail negates the result — this exploits the symmetry of the normal distribution: `Φ⁻¹(1-p) = -Φ⁻¹(p)`.\n`─────────────────────────────────────────────────`"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7680,"cache_read_input_tokens":72479,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7680},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsggF8Z7wooRNorFj7","type":"assistant","uuid":"8008fe9e-1496-4108-821e-c515b2d99681","timestamp":"2026-02-17T20:04:00.971Z"}
{"parentUuid":"8008fe9e-1496-4108-821e-c515b2d99681","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_019TSM5qz4edpjzJYaSBSTP7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014x3aW1JCsM2j9tzephi2zx","name":"Write","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/stats.py","content":"# lib/stats.py\n\"\"\"Statistical functions for eval harness confidence intervals.\n\nPorted from cerberus/src/stats.ts (lines 7-176). Wilson Score intervals\nare better than Wald for small samples and extreme proportions (0%/100%).\nThe inverse normal CDF approximation (Abramowitz & Stegun 26.2.23) avoids\na scipy dependency while maintaining ~4.5e-4 accuracy.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\n\n\ndef _inverse_normal_cdf(p: float) -> float:\n    \"\"\"Rational approximation of the inverse normal CDF.\n\n    Abramowitz & Stegun formula 26.2.23. Three-region piecewise:\n    lower tail (p < 0.02425), central region, upper tail (p > 0.97575).\n    Accurate to ~4.5e-4 absolute error.\n    \"\"\"\n    if p <= 0 or p >= 1:\n        raise ValueError(f\"p must be in (0, 1), got {p}\")\n\n    # Coefficients for central region rational approximation\n    a1 = -3.969683028665376e1\n    a2 = 2.209460984245205e2\n    a3 = -2.759285104469687e2\n    a4 = 1.383577518672690e2\n    a5 = -3.066479806614716e1\n    a6 = 2.506628277459239e0\n\n    b1 = -5.447609879822406e1\n    b2 = 1.615858368580409e2\n    b3 = -1.556989798598866e2\n    b4 = 6.680131188771972e1\n    b5 = -1.328068155288572e1\n\n    # Coefficients for tail regions\n    c1 = -7.784894002430293e-3\n    c2 = -3.223964580411365e-1\n    c3 = -2.400758277161838e0\n    c4 = -2.549732539343734e0\n    c5 = 4.374664141464968e0\n    c6 = 2.938163982698783e0\n\n    d1 = 7.784695709041462e-3\n    d2 = 3.224671290700398e-1\n    d3 = 2.445134137142996e0\n    d4 = 3.754408661907416e0\n\n    p_low = 0.02425\n    p_high = 1 - p_low\n\n    if p < p_low:\n        # Lower tail\n        q = math.sqrt(-2 * math.log(p))\n        return (\n            (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6)\n            / ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n        )\n    elif p <= p_high:\n        # Central region\n        q = p - 0.5\n        r = q * q\n        return (\n            ((((((a1 * r + a2) * r + a3) * r + a4) * r + a5) * r + a6) * q)\n            / (((((b1 * r + b2) * r + b3) * r + b4) * r + b5) * r + 1)\n        )\n    else:\n        # Upper tail (symmetric to lower)\n        q = math.sqrt(-2 * math.log(1 - p))\n        return -(\n            (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6)\n            / ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n        )\n\n\ndef wilson_score_interval(\n    successes: int, n: int, confidence: float = 0.90\n) -> tuple[float, float, float]:\n    \"\"\"Wilson Score confidence interval for a binomial proportion.\n\n    Returns (lower, upper, center) as floats in [0, 1].\n    Wilson intervals are preferred over Wald (normal approximation) because\n    they don't collapse at 0% or 100% and have better coverage for small n.\n    \"\"\"\n    if n == 0:\n        return (0.0, 1.0, 0.0)\n\n    z = _inverse_normal_cdf(1 - (1 - confidence) / 2)\n    z2 = z * z\n    p_hat = successes / n\n\n    denominator = 1 + z2 / n\n    center = (p_hat + z2 / (2 * n)) / denominator\n    spread = (z * math.sqrt(p_hat * (1 - p_hat) / n + z2 / (4 * n * n))) / denominator\n\n    lower = max(0.0, center - spread)\n    upper = min(1.0, center + spread)\n\n    return (round(lower, 4), round(upper, 4), round(center, 4))\n\n\ndef ci_overlap(ci_a: tuple[float, float], ci_b: tuple[float, float]) -> bool:\n    \"\"\"Check if two confidence intervals overlap.\n\n    Returns True if the intervals share any range. Non-overlapping CIs\n    at 90% confidence suggest a statistically meaningful difference.\n    \"\"\"\n    return ci_a[0] <= ci_b[1] and ci_b[0] <= ci_a[1]\n"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7680,"cache_read_input_tokens":72479,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7680},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsggF8Z7wooRNorFj7","type":"assistant","uuid":"7cebfe42-e174-42a9-8a4c-c9a75f0328f6","timestamp":"2026-02-17T20:04:20.715Z"}
{"parentUuid":"7cebfe42-e174-42a9-8a4c-c9a75f0328f6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","uuid":"c84a3984-aa80-4e9a-82dd-7e431ea95e40","timestamp":"2026-02-17T20:04:20.718Z"}
{"parentUuid":"c84a3984-aa80-4e9a-82dd-7e431ea95e40","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","uuid":"602f76cc-bcd0-407b-9acf-7083be01c37a","timestamp":"2026-02-17T20:04:20.718Z"}
{"parentUuid":"602f76cc-bcd0-407b-9acf-7083be01c37a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","uuid":"976725ec-f5d6-4947-beaf-dcdce3ae5d5f","timestamp":"2026-02-17T20:04:20.718Z"}
{"type":"file-history-snapshot","messageId":"7cebfe42-e174-42a9-8a4c-c9a75f0328f6","snapshot":{"messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","trackedFileBackups":{"eval-harness/lib/stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:20.833Z"}},"timestamp":"2026-02-17T20:03:32.850Z"},"isSnapshotUpdate":true}
{"parentUuid":"976725ec-f5d6-4947-beaf-dcdce3ae5d5f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","timestamp":"2026-02-17T20:04:20.836Z","uuid":"c5dbad59-e809-422b-8636-69aaac9b7a16"}
{"parentUuid":"c5dbad59-e809-422b-8636-69aaac9b7a16","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","timestamp":"2026-02-17T20:04:20.836Z","uuid":"f265c4b6-b5fc-4d3e-8212-cbd4d595e1ae"}
{"parentUuid":"f265c4b6-b5fc-4d3e-8212-cbd4d595e1ae","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","toolUseID":"toolu_014x3aW1JCsM2j9tzephi2zx","timestamp":"2026-02-17T20:04:20.836Z","uuid":"70daa42e-9305-4a66-91b3-04627e7aebbe"}
{"parentUuid":"7cebfe42-e174-42a9-8a4c-c9a75f0328f6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014x3aW1JCsM2j9tzephi2zx","type":"tool_result","content":"File created successfully at: /Users/ryo/dev/intent-layer/eval-harness/lib/stats.py"}]},"uuid":"453d1db2-e25a-4539-b5dc-06d1c388c4e0","timestamp":"2026-02-17T20:04:20.834Z","toolUseResult":{"type":"create","filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/stats.py","content":"# lib/stats.py\n\"\"\"Statistical functions for eval harness confidence intervals.\n\nPorted from cerberus/src/stats.ts (lines 7-176). Wilson Score intervals\nare better than Wald for small samples and extreme proportions (0%/100%).\nThe inverse normal CDF approximation (Abramowitz & Stegun 26.2.23) avoids\na scipy dependency while maintaining ~4.5e-4 accuracy.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\n\n\ndef _inverse_normal_cdf(p: float) -> float:\n    \"\"\"Rational approximation of the inverse normal CDF.\n\n    Abramowitz & Stegun formula 26.2.23. Three-region piecewise:\n    lower tail (p < 0.02425), central region, upper tail (p > 0.97575).\n    Accurate to ~4.5e-4 absolute error.\n    \"\"\"\n    if p <= 0 or p >= 1:\n        raise ValueError(f\"p must be in (0, 1), got {p}\")\n\n    # Coefficients for central region rational approximation\n    a1 = -3.969683028665376e1\n    a2 = 2.209460984245205e2\n    a3 = -2.759285104469687e2\n    a4 = 1.383577518672690e2\n    a5 = -3.066479806614716e1\n    a6 = 2.506628277459239e0\n\n    b1 = -5.447609879822406e1\n    b2 = 1.615858368580409e2\n    b3 = -1.556989798598866e2\n    b4 = 6.680131188771972e1\n    b5 = -1.328068155288572e1\n\n    # Coefficients for tail regions\n    c1 = -7.784894002430293e-3\n    c2 = -3.223964580411365e-1\n    c3 = -2.400758277161838e0\n    c4 = -2.549732539343734e0\n    c5 = 4.374664141464968e0\n    c6 = 2.938163982698783e0\n\n    d1 = 7.784695709041462e-3\n    d2 = 3.224671290700398e-1\n    d3 = 2.445134137142996e0\n    d4 = 3.754408661907416e0\n\n    p_low = 0.02425\n    p_high = 1 - p_low\n\n    if p < p_low:\n        # Lower tail\n        q = math.sqrt(-2 * math.log(p))\n        return (\n            (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6)\n            / ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n        )\n    elif p <= p_high:\n        # Central region\n        q = p - 0.5\n        r = q * q\n        return (\n            ((((((a1 * r + a2) * r + a3) * r + a4) * r + a5) * r + a6) * q)\n            / (((((b1 * r + b2) * r + b3) * r + b4) * r + b5) * r + 1)\n        )\n    else:\n        # Upper tail (symmetric to lower)\n        q = math.sqrt(-2 * math.log(1 - p))\n        return -(\n            (((((c1 * q + c2) * q + c3) * q + c4) * q + c5) * q + c6)\n            / ((((d1 * q + d2) * q + d3) * q + d4) * q + 1)\n        )\n\n\ndef wilson_score_interval(\n    successes: int, n: int, confidence: float = 0.90\n) -> tuple[float, float, float]:\n    \"\"\"Wilson Score confidence interval for a binomial proportion.\n\n    Returns (lower, upper, center) as floats in [0, 1].\n    Wilson intervals are preferred over Wald (normal approximation) because\n    they don't collapse at 0% or 100% and have better coverage for small n.\n    \"\"\"\n    if n == 0:\n        return (0.0, 1.0, 0.0)\n\n    z = _inverse_normal_cdf(1 - (1 - confidence) / 2)\n    z2 = z * z\n    p_hat = successes / n\n\n    denominator = 1 + z2 / n\n    center = (p_hat + z2 / (2 * n)) / denominator\n    spread = (z * math.sqrt(p_hat * (1 - p_hat) / n + z2 / (4 * n * n))) / denominator\n\n    lower = max(0.0, center - spread)\n    upper = min(1.0, center + spread)\n\n    return (round(lower, 4), round(upper, 4), round(center, 4))\n\n\ndef ci_overlap(ci_a: tuple[float, float], ci_b: tuple[float, float]) -> bool:\n    \"\"\"Check if two confidence intervals overlap.\n\n    Returns True if the intervals share any range. Non-overlapping CIs\n    at 90% confidence suggest a statistically meaningful difference.\n    \"\"\"\n    return ci_a[0] <= ci_b[1] and ci_b[0] <= ci_a[1]\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"7cebfe42-e174-42a9-8a4c-c9a75f0328f6"}
{"parentUuid":"453d1db2-e25a-4539-b5dc-06d1c388c4e0","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_019TSM5qz4edpjzJYaSBSTP7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","name":"Write","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_stats.py","content":"# tests/test_stats.py\nimport pytest\nfrom lib.stats import _inverse_normal_cdf, wilson_score_interval, ci_overlap\n\n\nclass TestInverseNormalCDF:\n    def test_median(self):\n        \"\"\"Φ⁻¹(0.5) = 0 (median of standard normal).\"\"\"\n        assert abs(_inverse_normal_cdf(0.5)) < 1e-6\n\n    def test_known_quantiles(self):\n        \"\"\"Check against well-known z-scores.\"\"\"\n        # z for 95th percentile ≈ 1.6449\n        assert abs(_inverse_normal_cdf(0.95) - 1.6449) < 0.001\n        # z for 97.5th percentile ≈ 1.9600\n        assert abs(_inverse_normal_cdf(0.975) - 1.9600) < 0.001\n        # z for 5th percentile ≈ -1.6449 (symmetry)\n        assert abs(_inverse_normal_cdf(0.05) + 1.6449) < 0.001\n\n    def test_symmetry(self):\n        \"\"\"Φ⁻¹(p) = -Φ⁻¹(1-p) by symmetry of normal distribution.\"\"\"\n        for p in [0.01, 0.1, 0.25, 0.4]:\n            assert abs(_inverse_normal_cdf(p) + _inverse_normal_cdf(1 - p)) < 1e-6\n\n    def test_boundary_errors(self):\n        \"\"\"p must be in open interval (0, 1).\"\"\"\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(0)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(1)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(-0.1)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(1.1)\n\n    def test_tail_regions(self):\n        \"\"\"Values in the low/high tail regions (p < 0.02425, p > 0.97575).\"\"\"\n        # Low tail\n        z_low = _inverse_normal_cdf(0.01)\n        assert z_low < -2.0\n        assert abs(z_low - (-2.3263)) < 0.001\n        # High tail\n        z_high = _inverse_normal_cdf(0.99)\n        assert z_high > 2.0\n        assert abs(z_high - 2.3263) < 0.001\n\n\nclass TestWilsonScoreInterval:\n    def test_zero_successes(self):\n        \"\"\"0/10 — lower bound near 0 but upper bound > 0.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 10, 0.90)\n        assert lower == 0.0\n        assert upper > 0.0\n        assert upper < 0.30  # shouldn't be too wide\n        assert center > 0.0  # Wilson center is pulled away from 0\n\n    def test_all_successes(self):\n        \"\"\"10/10 — upper bound near 1 but lower bound < 1.\"\"\"\n        lower, upper, center = wilson_score_interval(10, 10, 0.90)\n        assert lower > 0.70\n        assert lower < 1.0\n        assert upper == 1.0\n        assert center < 1.0  # Wilson center is pulled away from 1\n\n    def test_half_successes(self):\n        \"\"\"5/10 — center at 0.50, roughly symmetric bounds.\"\"\"\n        lower, upper, center = wilson_score_interval(5, 10, 0.90)\n        assert center == 0.5\n        # Bounds should be roughly symmetric around 0.5\n        assert abs((0.5 - lower) - (upper - 0.5)) < 0.01\n\n    def test_zero_trials(self):\n        \"\"\"0 trials — degenerate case returns full [0, 1] range.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 0, 0.90)\n        assert lower == 0.0\n        assert upper == 1.0\n        assert center == 0.0\n\n    def test_single_trial_pass(self):\n        \"\"\"1/1 — very wide CI because n=1 is barely informative.\"\"\"\n        lower, upper, center = wilson_score_interval(1, 1, 0.90)\n        assert lower > 0.0\n        assert upper == 1.0\n        assert center < 1.0\n\n    def test_single_trial_fail(self):\n        \"\"\"0/1 — wide CI, but upper bound should be well below 1.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 1, 0.90)\n        assert lower == 0.0\n        assert upper < 1.0\n        assert center > 0.0\n\n    def test_our_eval_data(self):\n        \"\"\"5/8 at 90% — matches our real eval scenario.\n\n        Expected: ~[36%, 84%] confirming paper's 2-4% differences\n        are well within noise.\n        \"\"\"\n        lower, upper, center = wilson_score_interval(5, 8, 0.90)\n        assert 0.30 < lower < 0.45\n        assert 0.75 < upper < 0.90\n        assert 0.55 < center < 0.70\n\n    def test_higher_confidence_widens_interval(self):\n        \"\"\"95% CI should be wider than 90% CI for same data.\"\"\"\n        l90, u90, _ = wilson_score_interval(5, 10, 0.90)\n        l95, u95, _ = wilson_score_interval(5, 10, 0.95)\n        assert l95 <= l90\n        assert u95 >= u90\n\n    def test_more_data_narrows_interval(self):\n        \"\"\"50/100 should give narrower CI than 5/10 (same proportion).\"\"\"\n        l_small, u_small, _ = wilson_score_interval(5, 10, 0.90)\n        l_large, u_large, _ = wilson_score_interval(50, 100, 0.90)\n        width_small = u_small - l_small\n        width_large = u_large - l_large\n        assert width_large < width_small\n\n\nclass TestCIOverlap:\n    def test_overlapping(self):\n        \"\"\"Two clearly overlapping intervals.\"\"\"\n        assert ci_overlap((0.3, 0.7), (0.5, 0.9)) is True\n\n    def test_non_overlapping(self):\n        \"\"\"Two clearly separated intervals.\"\"\"\n        assert ci_overlap((0.1, 0.3), (0.5, 0.8)) is False\n\n    def test_touching(self):\n        \"\"\"Intervals that share exactly one point still count as overlapping.\"\"\"\n        assert ci_overlap((0.1, 0.5), (0.5, 0.9)) is True\n\n    def test_contained(self):\n        \"\"\"One interval fully contained within another.\"\"\"\n        assert ci_overlap((0.2, 0.8), (0.4, 0.6)) is True\n\n    def test_identical(self):\n        \"\"\"Same interval overlaps with itself.\"\"\"\n        assert ci_overlap((0.3, 0.7), (0.3, 0.7)) is True\n"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7680,"cache_read_input_tokens":72479,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7680},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEsggF8Z7wooRNorFj7","type":"assistant","uuid":"9a69adef-6225-4899-a712-994aef1e73da","timestamp":"2026-02-17T20:04:42.608Z"}
{"parentUuid":"9a69adef-6225-4899-a712-994aef1e73da","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","uuid":"7cb5c0bd-34e6-407e-83bf-6bacd360e91c","timestamp":"2026-02-17T20:04:42.610Z"}
{"parentUuid":"7cb5c0bd-34e6-407e-83bf-6bacd360e91c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","uuid":"0639fd28-50a2-4bd8-bef8-49594631e34c","timestamp":"2026-02-17T20:04:42.611Z"}
{"parentUuid":"0639fd28-50a2-4bd8-bef8-49594631e34c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","uuid":"e7758d96-494c-4937-9c4e-e3b90bf324c1","timestamp":"2026-02-17T20:04:42.611Z"}
{"type":"file-history-snapshot","messageId":"9a69adef-6225-4899-a712-994aef1e73da","snapshot":{"messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","trackedFileBackups":{"eval-harness/lib/stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:20.833Z"},"eval-harness/tests/test_stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:42.720Z"}},"timestamp":"2026-02-17T20:03:32.850Z"},"isSnapshotUpdate":true}
{"parentUuid":"e7758d96-494c-4937-9c4e-e3b90bf324c1","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","timestamp":"2026-02-17T20:04:42.723Z","uuid":"1ee4ee76-ff8a-45ca-af79-3293e2e0872d"}
{"parentUuid":"1ee4ee76-ff8a-45ca-af79-3293e2e0872d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","timestamp":"2026-02-17T20:04:42.723Z","uuid":"84ca0349-b805-4d99-bafb-7cf610e643df"}
{"parentUuid":"84ca0349-b805-4d99-bafb-7cf610e643df","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","toolUseID":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","timestamp":"2026-02-17T20:04:42.723Z","uuid":"fa1a73f5-00c2-4026-8c93-c0a55b4df63a"}
{"parentUuid":"9a69adef-6225-4899-a712-994aef1e73da","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UbPVnEhtkRf3J5Tcvc2P9N","type":"tool_result","content":"File created successfully at: /Users/ryo/dev/intent-layer/eval-harness/tests/test_stats.py"}]},"uuid":"717d4a76-87df-46f1-aa0f-0f4c03618da8","timestamp":"2026-02-17T20:04:42.721Z","toolUseResult":{"type":"create","filePath":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_stats.py","content":"# tests/test_stats.py\nimport pytest\nfrom lib.stats import _inverse_normal_cdf, wilson_score_interval, ci_overlap\n\n\nclass TestInverseNormalCDF:\n    def test_median(self):\n        \"\"\"Φ⁻¹(0.5) = 0 (median of standard normal).\"\"\"\n        assert abs(_inverse_normal_cdf(0.5)) < 1e-6\n\n    def test_known_quantiles(self):\n        \"\"\"Check against well-known z-scores.\"\"\"\n        # z for 95th percentile ≈ 1.6449\n        assert abs(_inverse_normal_cdf(0.95) - 1.6449) < 0.001\n        # z for 97.5th percentile ≈ 1.9600\n        assert abs(_inverse_normal_cdf(0.975) - 1.9600) < 0.001\n        # z for 5th percentile ≈ -1.6449 (symmetry)\n        assert abs(_inverse_normal_cdf(0.05) + 1.6449) < 0.001\n\n    def test_symmetry(self):\n        \"\"\"Φ⁻¹(p) = -Φ⁻¹(1-p) by symmetry of normal distribution.\"\"\"\n        for p in [0.01, 0.1, 0.25, 0.4]:\n            assert abs(_inverse_normal_cdf(p) + _inverse_normal_cdf(1 - p)) < 1e-6\n\n    def test_boundary_errors(self):\n        \"\"\"p must be in open interval (0, 1).\"\"\"\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(0)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(1)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(-0.1)\n        with pytest.raises(ValueError):\n            _inverse_normal_cdf(1.1)\n\n    def test_tail_regions(self):\n        \"\"\"Values in the low/high tail regions (p < 0.02425, p > 0.97575).\"\"\"\n        # Low tail\n        z_low = _inverse_normal_cdf(0.01)\n        assert z_low < -2.0\n        assert abs(z_low - (-2.3263)) < 0.001\n        # High tail\n        z_high = _inverse_normal_cdf(0.99)\n        assert z_high > 2.0\n        assert abs(z_high - 2.3263) < 0.001\n\n\nclass TestWilsonScoreInterval:\n    def test_zero_successes(self):\n        \"\"\"0/10 — lower bound near 0 but upper bound > 0.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 10, 0.90)\n        assert lower == 0.0\n        assert upper > 0.0\n        assert upper < 0.30  # shouldn't be too wide\n        assert center > 0.0  # Wilson center is pulled away from 0\n\n    def test_all_successes(self):\n        \"\"\"10/10 — upper bound near 1 but lower bound < 1.\"\"\"\n        lower, upper, center = wilson_score_interval(10, 10, 0.90)\n        assert lower > 0.70\n        assert lower < 1.0\n        assert upper == 1.0\n        assert center < 1.0  # Wilson center is pulled away from 1\n\n    def test_half_successes(self):\n        \"\"\"5/10 — center at 0.50, roughly symmetric bounds.\"\"\"\n        lower, upper, center = wilson_score_interval(5, 10, 0.90)\n        assert center == 0.5\n        # Bounds should be roughly symmetric around 0.5\n        assert abs((0.5 - lower) - (upper - 0.5)) < 0.01\n\n    def test_zero_trials(self):\n        \"\"\"0 trials — degenerate case returns full [0, 1] range.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 0, 0.90)\n        assert lower == 0.0\n        assert upper == 1.0\n        assert center == 0.0\n\n    def test_single_trial_pass(self):\n        \"\"\"1/1 — very wide CI because n=1 is barely informative.\"\"\"\n        lower, upper, center = wilson_score_interval(1, 1, 0.90)\n        assert lower > 0.0\n        assert upper == 1.0\n        assert center < 1.0\n\n    def test_single_trial_fail(self):\n        \"\"\"0/1 — wide CI, but upper bound should be well below 1.\"\"\"\n        lower, upper, center = wilson_score_interval(0, 1, 0.90)\n        assert lower == 0.0\n        assert upper < 1.0\n        assert center > 0.0\n\n    def test_our_eval_data(self):\n        \"\"\"5/8 at 90% — matches our real eval scenario.\n\n        Expected: ~[36%, 84%] confirming paper's 2-4% differences\n        are well within noise.\n        \"\"\"\n        lower, upper, center = wilson_score_interval(5, 8, 0.90)\n        assert 0.30 < lower < 0.45\n        assert 0.75 < upper < 0.90\n        assert 0.55 < center < 0.70\n\n    def test_higher_confidence_widens_interval(self):\n        \"\"\"95% CI should be wider than 90% CI for same data.\"\"\"\n        l90, u90, _ = wilson_score_interval(5, 10, 0.90)\n        l95, u95, _ = wilson_score_interval(5, 10, 0.95)\n        assert l95 <= l90\n        assert u95 >= u90\n\n    def test_more_data_narrows_interval(self):\n        \"\"\"50/100 should give narrower CI than 5/10 (same proportion).\"\"\"\n        l_small, u_small, _ = wilson_score_interval(5, 10, 0.90)\n        l_large, u_large, _ = wilson_score_interval(50, 100, 0.90)\n        width_small = u_small - l_small\n        width_large = u_large - l_large\n        assert width_large < width_small\n\n\nclass TestCIOverlap:\n    def test_overlapping(self):\n        \"\"\"Two clearly overlapping intervals.\"\"\"\n        assert ci_overlap((0.3, 0.7), (0.5, 0.9)) is True\n\n    def test_non_overlapping(self):\n        \"\"\"Two clearly separated intervals.\"\"\"\n        assert ci_overlap((0.1, 0.3), (0.5, 0.8)) is False\n\n    def test_touching(self):\n        \"\"\"Intervals that share exactly one point still count as overlapping.\"\"\"\n        assert ci_overlap((0.1, 0.5), (0.5, 0.9)) is True\n\n    def test_contained(self):\n        \"\"\"One interval fully contained within another.\"\"\"\n        assert ci_overlap((0.2, 0.8), (0.4, 0.6)) is True\n\n    def test_identical(self):\n        \"\"\"Same interval overlaps with itself.\"\"\"\n        assert ci_overlap((0.3, 0.7), (0.3, 0.7)) is True\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"9a69adef-6225-4899-a712-994aef1e73da"}
{"parentUuid":"717d4a76-87df-46f1-aa0f-0f4c03618da8","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01DTryjHHsDJPhWHtQyZ4fqd","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nGood, both files created. Now let me run the stats tests to verify correctness before modifying the reporter."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6993,"cache_read_input_tokens":80159,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6993},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEwUCmTQmbRsw7LpXs4","type":"assistant","uuid":"39eaa39e-1e1c-454f-8be2-4fe0f258e74b","timestamp":"2026-02-17T20:04:46.061Z"}
{"parentUuid":"39eaa39e-1e1c-454f-8be2-4fe0f258e74b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01DTryjHHsDJPhWHtQyZ4fqd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AiATJZjXjnUWcDtLEAThFm","name":"Bash","input":{"command":"cd /Users/ryo/dev/intent-layer/eval-harness && python -m pytest tests/test_stats.py -v 2>&1","description":"Run stats unit tests"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":6993,"cache_read_input_tokens":80159,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6993},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEwUCmTQmbRsw7LpXs4","type":"assistant","uuid":"93bb6db8-465b-46cf-965c-4b9b4fdb7e2a","timestamp":"2026-02-17T20:04:46.857Z"}
{"parentUuid":"93bb6db8-465b-46cf-965c-4b9b4fdb7e2a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","parentToolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","uuid":"3e008e03-01f4-4547-a0e5-8d4d44064e2d","timestamp":"2026-02-17T20:04:46.861Z"}
{"parentUuid":"3e008e03-01f4-4547-a0e5-8d4d44064e2d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","parentToolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","uuid":"2ed4d58a-ba57-4904-96d3-de56f23db079","timestamp":"2026-02-17T20:04:46.861Z"}
{"parentUuid":"2ed4d58a-ba57-4904-96d3-de56f23db079","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","toolUseID":"toolu_01AiATJZjXjnUWcDtLEAThFm","timestamp":"2026-02-17T20:04:50.755Z","uuid":"16711cdc-0460-404b-a1d3-29c6da7238b7"}
{"parentUuid":"93bb6db8-465b-46cf-965c-4b9b4fdb7e2a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AiATJZjXjnUWcDtLEAThFm","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 19 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  5%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [ 10%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [ 15%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [ 21%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 31%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 36%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 42%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 47%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 52%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 57%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 63%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 68%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 73%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 78%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 84%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 89%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 94%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [100%]\n\n============================== 19 passed in 0.03s ==============================","is_error":false}]},"uuid":"446924c2-9089-4b03-9b3e-41db57e3ba89","timestamp":"2026-02-17T20:04:50.754Z","toolUseResult":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 19 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  5%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [ 10%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [ 15%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [ 21%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 31%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 36%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 42%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 47%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 52%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 57%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 63%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 68%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 73%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 78%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 84%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 89%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 94%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [100%]\n\n============================== 19 passed in 0.03s ==============================","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"93bb6db8-465b-46cf-965c-4b9b4fdb7e2a"}
{"parentUuid":"446924c2-9089-4b03-9b3e-41db57e3ba89","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01PktQir3zuDRmdhxH6BMa5s","type":"message","role":"assistant","content":[{"type":"text","text":"All 19 stats tests pass. Now let me integrate CIs into the reporter. I need to modify `_serialize_condition()`, `_compute_summary()`, and `write_markdown()`.\n\n`★ Insight ─────────────────────────────────────`\nThe reporter modification follows a clean separation: `_serialize_condition()` computes and stores the CI data, `_compute_summary()` adds aggregate CI info, and `write_markdown()` formats it. This means CIs appear in both JSON (machine-readable) and markdown (human-readable) output without duplicating the computation logic.\n`─────────────────────────────────────────────────`"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":957,"cache_read_input_tokens":87152,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":957},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEx4fwBH4av6PZjvXfi","type":"assistant","uuid":"67d2e369-d19c-47bc-af18-4ad071745086","timestamp":"2026-02-17T20:04:56.734Z"}
{"parentUuid":"67d2e369-d19c-47bc-af18-4ad071745086","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01PktQir3zuDRmdhxH6BMa5s","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UXfN7nV1QsJbtauPvWqfsf","name":"Edit","input":{"replace_all":false,"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","old_string":"from lib.task_runner import TaskResult, Condition","new_string":"from lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":957,"cache_read_input_tokens":87152,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":957},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEx4fwBH4av6PZjvXfi","type":"assistant","uuid":"37bdfbda-9c27-4f91-9465-d0bd99adfc87","timestamp":"2026-02-17T20:04:58.135Z"}
{"parentUuid":"37bdfbda-9c27-4f91-9465-d0bd99adfc87","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","uuid":"cdcc02fe-6bbd-41b4-a022-0dfbd203cf20","timestamp":"2026-02-17T20:04:58.136Z"}
{"parentUuid":"cdcc02fe-6bbd-41b4-a022-0dfbd203cf20","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","uuid":"73651376-efc3-4411-8da1-526175768879","timestamp":"2026-02-17T20:04:58.137Z"}
{"parentUuid":"73651376-efc3-4411-8da1-526175768879","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","uuid":"92a2b853-eb61-4284-a2c2-bc092ac7bf18","timestamp":"2026-02-17T20:04:58.137Z"}
{"type":"file-history-snapshot","messageId":"37bdfbda-9c27-4f91-9465-d0bd99adfc87","snapshot":{"messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","trackedFileBackups":{"eval-harness/lib/stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:20.833Z"},"eval-harness/tests/test_stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:42.720Z"},"eval-harness/lib/reporter.py":{"backupFileName":"77c006e5b2bb1d0f@v1","version":1,"backupTime":"2026-02-17T20:04:58.224Z"}},"timestamp":"2026-02-17T20:03:32.850Z"},"isSnapshotUpdate":true}
{"parentUuid":"92a2b853-eb61-4284-a2c2-bc092ac7bf18","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","timestamp":"2026-02-17T20:04:58.232Z","uuid":"f297c340-8a0f-4ccc-bfca-87fb4678dceb"}
{"parentUuid":"f297c340-8a0f-4ccc-bfca-87fb4678dceb","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","timestamp":"2026-02-17T20:04:58.232Z","uuid":"7302bcfc-3841-4f03-b5d5-88b1e7790f3d"}
{"parentUuid":"7302bcfc-3841-4f03-b5d5-88b1e7790f3d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","toolUseID":"toolu_01UXfN7nV1QsJbtauPvWqfsf","timestamp":"2026-02-17T20:04:58.232Z","uuid":"1147afe4-a012-456c-96bf-7977b515d801"}
{"parentUuid":"37bdfbda-9c27-4f91-9465-d0bd99adfc87","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UXfN7nV1QsJbtauPvWqfsf","type":"tool_result","content":"The file /Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py has been updated successfully."}]},"uuid":"97c68334-a4af-403f-853c-dcdcc7ec0286","timestamp":"2026-02-17T20:04:58.230Z","toolUseResult":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","oldString":"from lib.task_runner import TaskResult, Condition","newString":"from lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap","originalFile":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","structuredPatch":[{"oldStart":8,"oldLines":6,"newStart":8,"newLines":7,"lines":[" from typing import Any"," "," from lib.task_runner import TaskResult, Condition","+from lib.stats import wilson_score_interval, ci_overlap"," "," "," @dataclass"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"37bdfbda-9c27-4f91-9465-d0bd99adfc87"}
{"parentUuid":"97c68334-a4af-403f-853c-dcdcc7ec0286","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_016FwiLJUutgMeR15ka98aaP","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nNow add CIs to `_serialize_condition()` for multi-run results:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1902,"cache_read_input_tokens":88109,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1902},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEExca5Wr1QCSNcao5qW","type":"assistant","uuid":"1484577e-c643-40ce-bf26-33cd80bcf950","timestamp":"2026-02-17T20:05:01.138Z"}
{"parentUuid":"1484577e-c643-40ce-bf26-33cd80bcf950","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_016FwiLJUutgMeR15ka98aaP","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PV72ecdufE7w3DwHo2aWev","name":"Edit","input":{"replace_all":false,"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","old_string":"        # Efficiency medians (secondary metrics — from valid runs only)","new_string":"        # Wilson Score confidence interval (90%)\n        ci_lower, ci_upper, ci_center = wilson_score_interval(\n            successes, len(valid_runs), confidence=0.90\n        )\n        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n\n        # Efficiency medians (secondary metrics — from valid runs only)"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1902,"cache_read_input_tokens":88109,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1902},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEExca5Wr1QCSNcao5qW","type":"assistant","uuid":"7e6fc40c-6fc1-4fa2-bdef-b5ff80774f27","timestamp":"2026-02-17T20:05:04.130Z"}
{"parentUuid":"7e6fc40c-6fc1-4fa2-bdef-b5ff80774f27","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","uuid":"40cc2224-97a7-4bdf-ad43-1be5a1583a83","timestamp":"2026-02-17T20:05:04.133Z"}
{"parentUuid":"40cc2224-97a7-4bdf-ad43-1be5a1583a83","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","uuid":"d6b5bd94-7638-4a68-a885-77bf0260eb5b","timestamp":"2026-02-17T20:05:04.133Z"}
{"parentUuid":"d6b5bd94-7638-4a68-a885-77bf0260eb5b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","uuid":"05803f1a-1976-48a4-a50e-2e03ce56672b","timestamp":"2026-02-17T20:05:04.133Z"}
{"parentUuid":"05803f1a-1976-48a4-a50e-2e03ce56672b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","timestamp":"2026-02-17T20:05:04.251Z","uuid":"22545a1e-a07f-4bf5-87e3-2c45e4001dd7"}
{"parentUuid":"22545a1e-a07f-4bf5-87e3-2c45e4001dd7","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","timestamp":"2026-02-17T20:05:04.251Z","uuid":"35dbedf1-019e-429a-89f1-20b4c8d9fc2b"}
{"parentUuid":"35dbedf1-019e-429a-89f1-20b4c8d9fc2b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","toolUseID":"toolu_01PV72ecdufE7w3DwHo2aWev","timestamp":"2026-02-17T20:05:04.251Z","uuid":"0035041c-5aac-43d9-88f6-78d6cabda3f8"}
{"parentUuid":"7e6fc40c-6fc1-4fa2-bdef-b5ff80774f27","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PV72ecdufE7w3DwHo2aWev","type":"tool_result","content":"The file /Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py has been updated successfully."}]},"uuid":"772b14fd-a1bb-4dc0-ae45-744df4d0e651","timestamp":"2026-02-17T20:05:04.250Z","toolUseResult":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","oldString":"        # Efficiency medians (secondary metrics — from valid runs only)","newString":"        # Wilson Score confidence interval (90%)\n        ci_lower, ci_upper, ci_center = wilson_score_interval(\n            successes, len(valid_runs), confidence=0.90\n        )\n        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n\n        # Efficiency medians (secondary metrics — from valid runs only)","originalFile":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","structuredPatch":[{"oldStart":145,"oldLines":6,"newStart":145,"newLines":12,"lines":["         result[\"successes\"] = successes","         result[\"total_valid_runs\"] = len(valid_runs)"," ","+        # Wilson Score confidence interval (90%)","+        ci_lower, ci_upper, ci_center = wilson_score_interval(","+            successes, len(valid_runs), confidence=0.90","+        )","+        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}","+","         # Efficiency medians (secondary metrics — from valid runs only)","         if valid_runs:","             result[\"median\"] = {"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"7e6fc40c-6fc1-4fa2-bdef-b5ff80774f27"}
{"parentUuid":"772b14fd-a1bb-4dc0-ae45-744df4d0e651","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01V85o5qXYTMCDQMHr2ehsJo","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nNow update `_compute_summary()` to include per-condition CIs and significance flags:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1844,"cache_read_input_tokens":90011,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1844},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEy4K8zJoEdu5UksrPc","type":"assistant","uuid":"b23397ec-404b-40ba-a146-5c845d535224","timestamp":"2026-02-17T20:05:07.214Z"}
{"parentUuid":"b23397ec-404b-40ba-a146-5c845d535224","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01V85o5qXYTMCDQMHr2ehsJo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","name":"Edit","input":{"replace_all":false,"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","old_string":"        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }","new_string":"        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        summary: dict[str, Any] = {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n        # Add CIs when we have multi-run data\n        has_multi_run = any(\n            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n            for r in results\n        )\n        if has_multi_run:\n            for label, cond_results in [\n                (\"none\", none_results),\n                (\"flat_llm\", flat_results),\n                (\"intent_layer\", il_results),\n            ]:\n                valid = [r for r in cond_results if not self._is_infra_error(r)]\n                if valid:\n                    successes = sum(1 for r in valid if r.success)\n                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n                    summary[f\"{label}_ci_90\"] = {\n                        \"lower\": round(ci_lower, 3),\n                        \"upper\": round(ci_upper, 3),\n                    }\n\n            # Significance: check CI overlap between none and each treatment\n            none_ci = summary.get(\"none_ci_90\")\n            if none_ci:\n                for treatment in (\"flat_llm\", \"intent_layer\"):\n                    t_ci = summary.get(f\"{treatment}_ci_90\")\n                    if t_ci:\n                        overlaps = ci_overlap(\n                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n                        )\n                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n\n        return summary"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1844,"cache_read_input_tokens":90011,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1844},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEy4K8zJoEdu5UksrPc","type":"assistant","uuid":"e9bfc22e-a69b-43ba-939f-d4b2038609c2","timestamp":"2026-02-17T20:05:16.083Z"}
{"parentUuid":"e9bfc22e-a69b-43ba-939f-d4b2038609c2","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","uuid":"3068dc3e-7369-4405-b697-38de064ddd8b","timestamp":"2026-02-17T20:05:16.085Z"}
{"parentUuid":"3068dc3e-7369-4405-b697-38de064ddd8b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","uuid":"2dea184e-b180-403c-84b3-7e518e984d21","timestamp":"2026-02-17T20:05:16.085Z"}
{"parentUuid":"2dea184e-b180-403c-84b3-7e518e984d21","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","uuid":"1cf15e76-2931-4ca3-8a9b-4380de4002ee","timestamp":"2026-02-17T20:05:16.085Z"}
{"parentUuid":"1cf15e76-2931-4ca3-8a9b-4380de4002ee","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","timestamp":"2026-02-17T20:05:16.258Z","uuid":"67d8741e-0873-44f3-8a78-33881572fb3b"}
{"parentUuid":"67d8741e-0873-44f3-8a78-33881572fb3b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","timestamp":"2026-02-17T20:05:16.258Z","uuid":"257c5f02-fdfe-4705-a9b1-e9e4c280baba"}
{"parentUuid":"257c5f02-fdfe-4705-a9b1-e9e4c280baba","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","toolUseID":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","timestamp":"2026-02-17T20:05:16.258Z","uuid":"e085d11a-5ba4-4192-ba37-10844397a9e9"}
{"parentUuid":"e9bfc22e-a69b-43ba-939f-d4b2038609c2","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UQ4JPYwzmuNcf7HXQaMF3g","type":"tool_result","content":"The file /Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py has been updated successfully."}]},"uuid":"3ef9b06f-45b5-4781-ba83-9218e5e0c733","timestamp":"2026-02-17T20:05:16.257Z","toolUseResult":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","oldString":"        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }","newString":"        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        summary: dict[str, Any] = {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n        # Add CIs when we have multi-run data\n        has_multi_run = any(\n            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n            for r in results\n        )\n        if has_multi_run:\n            for label, cond_results in [\n                (\"none\", none_results),\n                (\"flat_llm\", flat_results),\n                (\"intent_layer\", il_results),\n            ]:\n                valid = [r for r in cond_results if not self._is_infra_error(r)]\n                if valid:\n                    successes = sum(1 for r in valid if r.success)\n                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n                    summary[f\"{label}_ci_90\"] = {\n                        \"lower\": round(ci_lower, 3),\n                        \"upper\": round(ci_upper, 3),\n                    }\n\n            # Significance: check CI overlap between none and each treatment\n            none_ci = summary.get(\"none_ci_90\")\n            if none_ci:\n                for treatment in (\"flat_llm\", \"intent_layer\"):\n                    t_ci = summary.get(f\"{treatment}_ci_90\")\n                    if t_ci:\n                        overlaps = ci_overlap(\n                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n                        )\n                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n\n        return summary","originalFile":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Wilson Score confidence interval (90%)\n        ci_lower, ci_upper, ci_center = wilson_score_interval(\n            successes, len(valid_runs), confidence=0.90\n        )\n        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        return {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","structuredPatch":[{"oldStart":275,"oldLines":7,"newStart":275,"newLines":7,"lines":[" ","         infra_errors = sum(1 for r in results if self._is_infra_error(r))"," ","-        return {","+        summary: dict[str, Any] = {","             \"total_tasks\": len(set(r.task_id for r in results)),","             \"infrastructure_errors\": infra_errors,","             \"none_success_rate\": success_rate(none_results),"]},{"oldStart":283,"oldLines":6,"newStart":283,"newLines":40,"lines":["             \"intent_layer_success_rate\": success_rate(il_results),","         }"," ","+        # Add CIs when we have multi-run data","+        has_multi_run = any(","+            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1","+            for r in results","+        )","+        if has_multi_run:","+            for label, cond_results in [","+                (\"none\", none_results),","+                (\"flat_llm\", flat_results),","+                (\"intent_layer\", il_results),","+            ]:","+                valid = [r for r in cond_results if not self._is_infra_error(r)]","+                if valid:","+                    successes = sum(1 for r in valid if r.success)","+                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)","+                    summary[f\"{label}_ci_90\"] = {","+                        \"lower\": round(ci_lower, 3),","+                        \"upper\": round(ci_upper, 3),","+                    }","+","+            # Significance: check CI overlap between none and each treatment","+            none_ci = summary.get(\"none_ci_90\")","+            if none_ci:","+                for treatment in (\"flat_llm\", \"intent_layer\"):","+                    t_ci = summary.get(f\"{treatment}_ci_90\")","+                    if t_ci:","+                        overlaps = ci_overlap(","+                            (none_ci[\"lower\"], none_ci[\"upper\"]),","+                            (t_ci[\"lower\"], t_ci[\"upper\"]),","+                        )","+                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps","+","+        return summary","+","     def write_json(self, results: EvalResults) -> str:","         \"\"\"Write results to JSON file.\"\"\"","         path = self.output_dir / f\"{results.eval_id}.json\""]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"e9bfc22e-a69b-43ba-939f-d4b2038609c2"}
{"parentUuid":"3ef9b06f-45b5-4781-ba83-9218e5e0c733","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BgDMPrBd9HarGnriAUa4ug","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nNow update the markdown output. I need to modify the success column to include CIs for multi-run, add a comparison column, and update the summary section."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2395,"cache_read_input_tokens":91855,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2395},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEywepTszzQ8xeMX8jW","type":"assistant","uuid":"73ff2a38-f766-4279-879c-a0fea0301592","timestamp":"2026-02-17T20:05:20.439Z"}
{"parentUuid":"73ff2a38-f766-4279-879c-a0fea0301592","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01BgDMPrBd9HarGnriAUa4ug","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012YqUpP9TLwTYnro1F8posy","name":"Edit","input":{"replace_all":false,"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","old_string":"    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)","new_string":"    @staticmethod\n    def _format_ci(ci: dict) -> str:\n        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        When multi-run data includes CIs, they're shown inline with pass rates.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n        summary = results.summary\n        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n        ]\n\n        # Per-condition summary with optional CIs\n        for label, display_name in [\n            (\"none\", \"None\"),\n            (\"flat_llm\", \"Flat LLM\"),\n            (\"intent_layer\", \"Intent Layer\"),\n        ]:\n            rate = summary[f\"{label}_success_rate\"]\n            ci = summary.get(f\"{label}_ci_90\")\n            if ci:\n                lines.append(\n                    f\"- **{display_name} success rate:** {rate:.0%} \"\n                    f\"90% CI {self._format_ci(ci)}\"\n                )\n            else:\n                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n\n        # Significance flags\n        if has_cis:\n            lines.append(\"\")\n            for treatment, display_name in [\n                (\"flat_llm\", \"Flat LLM\"),\n                (\"intent_layer\", \"Intent Layer\"),\n            ]:\n                sig_key = f\"{treatment}_vs_none_significant\"\n                if sig_key in summary:\n                    if summary[sig_key]:\n                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n                    else:\n                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n\n            # CI width as variance proxy\n            widths = []\n            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n                ci = summary.get(f\"{label}_ci_90\")\n                if ci:\n                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n            if widths:\n                lines.append(\"\")\n                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n                    f\"{label}: {w:.0%}\" for label, w in widths\n                ))\n\n        lines += [\n            \"\",\n            \"## Results\",\n            \"\",\n        ]\n\n        # Table header — add IL vs none column when multi-run CIs exist\n        if has_cis:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|------------|\"\n            )\n        else:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|\"\n            )\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            # Pre-compute per-task CI comparison for IL vs none\n            none_data = r.get(\"none\")\n            il_data = r.get(\"intent_layer\")\n            il_vs_none = \"\"\n            if has_cis and none_data and il_data:\n                none_ci = none_data.get(\"ci_90\")\n                il_ci = il_data.get(\"ci_90\")\n                if none_ci and il_ci:\n                    none_rate = none_data.get(\"success_rate\", 0)\n                    il_rate = il_data.get(\"success_rate\", 0)\n                    diff = il_rate - none_rate\n                    overlaps = ci_overlap(\n                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n                    )\n                    sig_label = \"overlap\" if overlaps else \"sig.\"\n                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    ci = cond_data.get(\"ci_90\")\n                    if ci:\n                        success = (\n                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n                            f\"{rate:.0%} {self._format_ci(ci)}\"\n                        )\n                    else:\n                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                row = (\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens}\"\n                )\n\n                if has_cis:\n                    # Show IL vs none comparison on the intent_layer row\n                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n                    row += f\" | {comparison} |\"\n                else:\n                    row += \" |\"\n\n                lines.append(row)\n\n            # Blank row between tasks\n            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n            lines.append(blank)\n\n        # Remove trailing blank row\n        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2395,"cache_read_input_tokens":91855,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2395},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEEywepTszzQ8xeMX8jW","type":"assistant","uuid":"7f7f8254-7695-462d-83f3-997abdd61bef","timestamp":"2026-02-17T20:05:56.772Z"}
{"parentUuid":"7f7f8254-7695-462d-83f3-997abdd61bef","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","uuid":"bbb775f0-43cd-4e4c-86c7-f64031d61542","timestamp":"2026-02-17T20:05:56.776Z"}
{"parentUuid":"bbb775f0-43cd-4e4c-86c7-f64031d61542","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","uuid":"5e7e4a12-8ede-4b39-bfec-18195be93d2d","timestamp":"2026-02-17T20:05:56.776Z"}
{"parentUuid":"5e7e4a12-8ede-4b39-bfec-18195be93d2d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","uuid":"a063dada-8257-43d2-8d04-aba86489aa19","timestamp":"2026-02-17T20:05:56.776Z"}
{"parentUuid":"a063dada-8257-43d2-8d04-aba86489aa19","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","timestamp":"2026-02-17T20:05:56.950Z","uuid":"31fba9e1-69aa-4777-bb4a-e40ee818c55f"}
{"parentUuid":"31fba9e1-69aa-4777-bb4a-e40ee818c55f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","timestamp":"2026-02-17T20:05:56.950Z","uuid":"f633db35-079c-41c0-98cc-80f33f1f3abd"}
{"parentUuid":"f633db35-079c-41c0-98cc-80f33f1f3abd","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","toolUseID":"toolu_012YqUpP9TLwTYnro1F8posy","timestamp":"2026-02-17T20:05:56.950Z","uuid":"146a2814-9d7d-4e57-b7cf-6be6e63664ca"}
{"parentUuid":"7f7f8254-7695-462d-83f3-997abdd61bef","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012YqUpP9TLwTYnro1F8posy","type":"tool_result","content":"The file /Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py has been updated successfully."}]},"uuid":"250a9e66-c1fb-4f57-b960-902deb47cfb2","timestamp":"2026-02-17T20:05:56.949Z","toolUseResult":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","oldString":"    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)","newString":"    @staticmethod\n    def _format_ci(ci: dict) -> str:\n        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        When multi-run data includes CIs, they're shown inline with pass rates.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n        summary = results.summary\n        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n        ]\n\n        # Per-condition summary with optional CIs\n        for label, display_name in [\n            (\"none\", \"None\"),\n            (\"flat_llm\", \"Flat LLM\"),\n            (\"intent_layer\", \"Intent Layer\"),\n        ]:\n            rate = summary[f\"{label}_success_rate\"]\n            ci = summary.get(f\"{label}_ci_90\")\n            if ci:\n                lines.append(\n                    f\"- **{display_name} success rate:** {rate:.0%} \"\n                    f\"90% CI {self._format_ci(ci)}\"\n                )\n            else:\n                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n\n        # Significance flags\n        if has_cis:\n            lines.append(\"\")\n            for treatment, display_name in [\n                (\"flat_llm\", \"Flat LLM\"),\n                (\"intent_layer\", \"Intent Layer\"),\n            ]:\n                sig_key = f\"{treatment}_vs_none_significant\"\n                if sig_key in summary:\n                    if summary[sig_key]:\n                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n                    else:\n                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n\n            # CI width as variance proxy\n            widths = []\n            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n                ci = summary.get(f\"{label}_ci_90\")\n                if ci:\n                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n            if widths:\n                lines.append(\"\")\n                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n                    f\"{label}: {w:.0%}\" for label, w in widths\n                ))\n\n        lines += [\n            \"\",\n            \"## Results\",\n            \"\",\n        ]\n\n        # Table header — add IL vs none column when multi-run CIs exist\n        if has_cis:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|------------|\"\n            )\n        else:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|\"\n            )\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            # Pre-compute per-task CI comparison for IL vs none\n            none_data = r.get(\"none\")\n            il_data = r.get(\"intent_layer\")\n            il_vs_none = \"\"\n            if has_cis and none_data and il_data:\n                none_ci = none_data.get(\"ci_90\")\n                il_ci = il_data.get(\"ci_90\")\n                if none_ci and il_ci:\n                    none_rate = none_data.get(\"success_rate\", 0)\n                    il_rate = il_data.get(\"success_rate\", 0)\n                    diff = il_rate - none_rate\n                    overlaps = ci_overlap(\n                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n                    )\n                    sig_label = \"overlap\" if overlaps else \"sig.\"\n                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    ci = cond_data.get(\"ci_90\")\n                    if ci:\n                        success = (\n                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n                            f\"{rate:.0%} {self._format_ci(ci)}\"\n                        )\n                    else:\n                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                row = (\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens}\"\n                )\n\n                if has_cis:\n                    # Show IL vs none comparison on the intent_layer row\n                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n                    row += f\" | {comparison} |\"\n                else:\n                    row += \" |\"\n\n                lines.append(row)\n\n            # Blank row between tasks\n            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n            lines.append(blank)\n\n        # Remove trailing blank row\n        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)","originalFile":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Wilson Score confidence interval (90%)\n        ci_lower, ci_upper, ci_center = wilson_score_interval(\n            successes, len(valid_runs), confidence=0.90\n        )\n        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        summary: dict[str, Any] = {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n        # Add CIs when we have multi-run data\n        has_multi_run = any(\n            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n            for r in results\n        )\n        if has_multi_run:\n            for label, cond_results in [\n                (\"none\", none_results),\n                (\"flat_llm\", flat_results),\n                (\"intent_layer\", il_results),\n            ]:\n                valid = [r for r in cond_results if not self._is_infra_error(r)]\n                if valid:\n                    successes = sum(1 for r in valid if r.success)\n                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n                    summary[f\"{label}_ci_90\"] = {\n                        \"lower\": round(ci_lower, 3),\n                        \"upper\": round(ci_upper, 3),\n                    }\n\n            # Significance: check CI overlap between none and each treatment\n            none_ci = summary.get(\"none_ci_90\")\n            if none_ci:\n                for treatment in (\"flat_llm\", \"intent_layer\"):\n                    t_ci = summary.get(f\"{treatment}_ci_90\")\n                    if t_ci:\n                        overlaps = ci_overlap(\n                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n                        )\n                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n\n        return summary\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n            \"\",\n            \"## Results\",\n            \"\",\n            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n        ]\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                lines.append(\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens} |\"\n                )\n\n            # Blank row between tasks\n            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n\n        # Remove trailing blank row\n        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","structuredPatch":[{"oldStart":324,"oldLines":13,"newStart":324,"newLines":21,"lines":["             json.dump(asdict(results), f, indent=2)","         return str(path)"," ","+    @staticmethod","+    def _format_ci(ci: dict) -> str:","+        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"","+        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"","+","     def write_markdown(self, results: EvalResults) -> str:","         \"\"\"Write results to Markdown file."," ","         Handles both single-run (backward-compatible) and multi-run formats.","         Uses _get_fix_metrics() to extract efficiency numbers from any format.","+        When multi-run data includes CIs, they're shown inline with pass rates.","         \"\"\"","         path = self.output_dir / f\"{results.eval_id}.md\"","+        summary = results.summary","+        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary"," ","         lines = [","             f\"# Eval Results: {results.eval_id}\","]},{"oldStart":339,"oldLines":31,"newStart":347,"newLines":116,"lines":["             \"\",","             \"## Summary\",","             \"\",","-            f\"- **Total tasks:** {results.summary['total_tasks']}\",","-            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",","-            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",","-            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",","-            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",","+            f\"- **Total tasks:** {summary['total_tasks']}\",","+            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",","+        ]","+","+        # Per-condition summary with optional CIs","+        for label, display_name in [","+            (\"none\", \"None\"),","+            (\"flat_llm\", \"Flat LLM\"),","+            (\"intent_layer\", \"Intent Layer\"),","+        ]:","+            rate = summary[f\"{label}_success_rate\"]","+            ci = summary.get(f\"{label}_ci_90\")","+            if ci:","+                lines.append(","+                    f\"- **{display_name} success rate:** {rate:.0%} \"","+                    f\"90% CI {self._format_ci(ci)}\"","+                )","+            else:","+                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")","+","+        # Significance flags","+        if has_cis:","+            lines.append(\"\")","+            for treatment, display_name in [","+                (\"flat_llm\", \"Flat LLM\"),","+                (\"intent_layer\", \"Intent Layer\"),","+            ]:","+                sig_key = f\"{treatment}_vs_none_significant\"","+                if sig_key in summary:","+                    if summary[sig_key]:","+                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")","+                    else:","+                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")","+","+            # CI width as variance proxy","+            widths = []","+            for label in (\"none\", \"flat_llm\", \"intent_layer\"):","+                ci = summary.get(f\"{label}_ci_90\")","+                if ci:","+                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))","+            if widths:","+                lines.append(\"\")","+                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(","+                    f\"{label}: {w:.0%}\" for label, w in widths","+                ))","+","+        lines += [","             \"\",","             \"## Results\",","             \"\",","-            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",","-            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",","         ]"," ","+        # Table header — add IL vs none column when multi-run CIs exist","+        if has_cis:","+            lines.append(","+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"","+                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"","+            )","+            lines.append(","+                \"|------|-----------|---------|----------|--------|------------|-------\"","+                \"|--------|----------|------------|\"","+            )","+        else:","+            lines.append(","+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"","+                \"| \\u0394 Time | \\u0394 Tokens |\"","+            )","+            lines.append(","+                \"|------|-----------|---------|----------|--------|------------|-------\"","+                \"|--------|----------|\"","+            )","+","         for r in results.results:","             task_id = r[\"task_id\"]","             deltas = r.get(\"deltas\", {})"," ","+            # Pre-compute per-task CI comparison for IL vs none","+            none_data = r.get(\"none\")","+            il_data = r.get(\"intent_layer\")","+            il_vs_none = \"\"","+            if has_cis and none_data and il_data:","+                none_ci = none_data.get(\"ci_90\")","+                il_ci = il_data.get(\"ci_90\")","+                if none_ci and il_ci:","+                    none_rate = none_data.get(\"success_rate\", 0)","+                    il_rate = il_data.get(\"success_rate\", 0)","+                    diff = il_rate - none_rate","+                    overlaps = ci_overlap(","+                        (none_ci[\"lower\"], none_ci[\"upper\"]),","+                        (il_ci[\"lower\"], il_ci[\"upper\"]),","+                    )","+                    sig_label = \"overlap\" if overlaps else \"sig.\"","+                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"","+","             for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):","                 cond_data = r.get(cond_key)","                 if cond_data is None:","                     continue"," ","-                # Success: multi-run shows rate, single-run shows PASS/FAIL","+                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL","                 if \"success_rate\" in cond_data:","                     rate = cond_data[\"success_rate\"]","-                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"","+                    ci = cond_data.get(\"ci_90\")","+                    if ci:","+                        success = (","+                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"","+                            f\"{rate:.0%} {self._format_ci(ci)}\"","+                        )","+                    else:","+                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"","                 else:","                     success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\""," "]},{"oldStart":385,"oldLines":17,"newStart":478,"newLines":27,"lines":["                     d_time = delta.get(\"time_percent\", \"N/A\")","                     d_tokens = delta.get(\"tokens_percent\", \"N/A\")"," ","-                lines.append(","+                row = (","                     f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"","                     f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"","-                    f\"{d_time} | {d_tokens} |\"","+                    f\"{d_time} | {d_tokens}\"","                 )"," ","+                if has_cis:","+                    # Show IL vs none comparison on the intent_layer row","+                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"","+                    row += f\" | {comparison} |\"","+                else:","+                    row += \" |\"","+","+                lines.append(row)","+","             # Blank row between tasks","-            lines.append(\"|  |  |  |  |  |  |  |  |  |\")","+            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")","+            lines.append(blank)"," ","         # Remove trailing blank row","-        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":","+        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":","             lines.pop()"," ","         with open(path, \"w\") as f:"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"7f7f8254-7695-462d-83f3-997abdd61bef"}
{"parentUuid":"250a9e66-c1fb-4f57-b960-902deb47cfb2","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HgC1cbqq8KREVsKvwodfA9","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nNow let me run the full test suite — both the existing reporter tests (to check for regressions) and the new stats tests:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":4796,"cache_read_input_tokens":94250,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4796},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF2wnqMPqVQXcqMf2FX","type":"assistant","uuid":"2d9729ba-1603-48ca-be55-c37d751b2c50","timestamp":"2026-02-17T20:06:00.639Z"}
{"parentUuid":"2d9729ba-1603-48ca-be55-c37d751b2c50","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HgC1cbqq8KREVsKvwodfA9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013QfetGuWDHBV8ec5tVtmfG","name":"Bash","input":{"command":"python -m pytest tests/test_stats.py tests/test_reporter.py -v 2>&1","description":"Run stats and reporter tests"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":4796,"cache_read_input_tokens":94250,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4796},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF2wnqMPqVQXcqMf2FX","type":"assistant","uuid":"707c5113-30e2-490e-ba74-c99167600c4b","timestamp":"2026-02-17T20:06:01.486Z"}
{"parentUuid":"707c5113-30e2-490e-ba74-c99167600c4b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","parentToolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","uuid":"07efda11-b77c-4869-82b7-00090843126f","timestamp":"2026-02-17T20:06:01.488Z"}
{"parentUuid":"07efda11-b77c-4869-82b7-00090843126f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","parentToolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","uuid":"eadc639e-2c68-4c16-87ff-ce657a048409","timestamp":"2026-02-17T20:06:01.489Z"}
{"parentUuid":"eadc639e-2c68-4c16-87ff-ce657a048409","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","toolUseID":"toolu_013QfetGuWDHBV8ec5tVtmfG","timestamp":"2026-02-17T20:06:05.955Z","uuid":"ab73a6c4-f453-4fbd-9316-0959680708c3"}
{"parentUuid":"707c5113-30e2-490e-ba74-c99167600c4b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013QfetGuWDHBV8ec5tVtmfG","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 34 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  2%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [  5%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [  8%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [ 11%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 14%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 17%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 20%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 23%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 29%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 32%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 35%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 38%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 41%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 44%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 47%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 50%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 52%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [ 55%]\ntests/test_reporter.py::test_three_condition_compilation PASSED          [ 58%]\ntests/test_reporter.py::test_deltas_relative_to_none PASSED              [ 61%]\ntests/test_reporter.py::test_zero_baseline_delta PASSED                  [ 64%]\ntests/test_reporter.py::test_missing_condition PASSED                    [ 67%]\ntests/test_reporter.py::test_summary_three_success_rates PASSED          [ 70%]\ntests/test_reporter.py::test_markdown_multi_row_layout PASSED            [ 73%]\ntests/test_reporter.py::test_json_output PASSED                          [ 76%]\ntests/test_reporter.py::test_infrastructure_errors_excluded_from_success_rate PASSED [ 79%]\ntests/test_reporter.py::test_pre_validation_and_skill_gen_errors_excluded PASSED [ 82%]\ntests/test_reporter.py::test_multi_run_serialize_condition PASSED        [ 85%]\ntests/test_reporter.py::test_multi_run_delta PASSED                      [ 88%]\ntests/test_reporter.py::test_get_fix_metrics_all_formats PASSED          [ 91%]\ntests/test_reporter.py::test_serialize_includes_exit_code_and_timeout PASSED [ 94%]\ntests/test_reporter.py::test_serialize_omits_exit_code_when_none PASSED  [ 97%]\ntests/test_reporter.py::test_single_run_delta_uses_success_rate_delta_key PASSED [100%]\n\n============================== 34 passed in 0.14s ==============================","is_error":false}]},"uuid":"2ef68398-aaac-4cde-a3df-e7bf7035c984","timestamp":"2026-02-17T20:06:05.955Z","toolUseResult":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 34 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  2%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [  5%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [  8%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [ 11%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 14%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 17%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 20%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 23%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 29%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 32%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 35%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 38%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 41%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 44%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 47%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 50%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 52%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [ 55%]\ntests/test_reporter.py::test_three_condition_compilation PASSED          [ 58%]\ntests/test_reporter.py::test_deltas_relative_to_none PASSED              [ 61%]\ntests/test_reporter.py::test_zero_baseline_delta PASSED                  [ 64%]\ntests/test_reporter.py::test_missing_condition PASSED                    [ 67%]\ntests/test_reporter.py::test_summary_three_success_rates PASSED          [ 70%]\ntests/test_reporter.py::test_markdown_multi_row_layout PASSED            [ 73%]\ntests/test_reporter.py::test_json_output PASSED                          [ 76%]\ntests/test_reporter.py::test_infrastructure_errors_excluded_from_success_rate PASSED [ 79%]\ntests/test_reporter.py::test_pre_validation_and_skill_gen_errors_excluded PASSED [ 82%]\ntests/test_reporter.py::test_multi_run_serialize_condition PASSED        [ 85%]\ntests/test_reporter.py::test_multi_run_delta PASSED                      [ 88%]\ntests/test_reporter.py::test_get_fix_metrics_all_formats PASSED          [ 91%]\ntests/test_reporter.py::test_serialize_includes_exit_code_and_timeout PASSED [ 94%]\ntests/test_reporter.py::test_serialize_omits_exit_code_when_none PASSED  [ 97%]\ntests/test_reporter.py::test_single_run_delta_uses_success_rate_delta_key PASSED [100%]\n\n============================== 34 passed in 0.14s ==============================","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"707c5113-30e2-490e-ba74-c99167600c4b"}
{"parentUuid":"2ef68398-aaac-4cde-a3df-e7bf7035c984","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_013p2pkMTaZXAi1AbkkrC2eU","type":"message","role":"assistant","content":[{"type":"text","text":"All 34 tests pass — 19 new stats tests + 15 existing reporter tests with zero regressions. Now let me add tests for the new reporter CI integration:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1358,"cache_read_input_tokens":99046,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1358},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF3cPYwR3tmuc5Miet6","type":"assistant","uuid":"912f6eb6-c2ed-45e3-bbca-d38d4c959cef","timestamp":"2026-02-17T20:06:10.530Z"}
{"parentUuid":"912f6eb6-c2ed-45e3-bbca-d38d4c959cef","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_013p2pkMTaZXAi1AbkkrC2eU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013yC1gy3HJm7PWZ7yZnTCna","name":"Read","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","offset":375},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1358,"cache_read_input_tokens":99046,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1358},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF3cPYwR3tmuc5Miet6","type":"assistant","uuid":"56e08c82-7343-4c03-9a9d-e44a248f048d","timestamp":"2026-02-17T20:06:11.291Z"}
{"parentUuid":"56e08c82-7343-4c03-9a9d-e44a248f048d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","parentToolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","uuid":"169600b1-ddd4-4ffc-be3d-8f99004351c7","timestamp":"2026-02-17T20:06:11.292Z"}
{"parentUuid":"169600b1-ddd4-4ffc-be3d-8f99004351c7","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","toolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","timestamp":"2026-02-17T20:06:11.353Z","uuid":"b28136d6-59e7-4e22-8907-722ef31fa887"}
{"parentUuid":"b28136d6-59e7-4e22-8907-722ef31fa887","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","toolUseID":"toolu_013yC1gy3HJm7PWZ7yZnTCna","timestamp":"2026-02-17T20:06:11.353Z","uuid":"c187eafe-3077-4228-8c9f-a7f06408c66d"}
{"parentUuid":"56e08c82-7343-4c03-9a9d-e44a248f048d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013yC1gy3HJm7PWZ7yZnTCna","type":"tool_result","content":"   375→def test_multi_run_serialize_condition():\n   376→    \"\"\"Multiple runs produce success_rate, median, and runs array.\"\"\"\n   377→    reporter = Reporter(output_dir=\"/tmp\")\n   378→\n   379→    runs = [\n   380→        TaskResult(\n   381→            task_id=\"fix-multi\",\n   382→            condition=Condition.NONE,\n   383→            success=True,\n   384→            test_output=\"PASS\",\n   385→            wall_clock_seconds=100.0,\n   386→            input_tokens=5000,\n   387→            output_tokens=2000,\n   388→            tool_calls=20,\n   389→            lines_changed=50,\n   390→            files_touched=[\"a.py\"]\n   391→        ),\n   392→        TaskResult(\n   393→            task_id=\"fix-multi\",\n   394→            condition=Condition.NONE,\n   395→            success=False,\n   396→            test_output=\"FAIL\",\n   397→            wall_clock_seconds=120.0,\n   398→            input_tokens=6000,\n   399→            output_tokens=2500,\n   400→            tool_calls=25,\n   401→            lines_changed=60,\n   402→            files_touched=[\"a.py\", \"b.py\"]\n   403→        ),\n   404→        TaskResult(\n   405→            task_id=\"fix-multi\",\n   406→            condition=Condition.NONE,\n   407→            success=True,\n   408→            test_output=\"PASS\",\n   409→            wall_clock_seconds=90.0,\n   410→            input_tokens=4500,\n   411→            output_tokens=1800,\n   412→            tool_calls=18,\n   413→            lines_changed=45,\n   414→            files_touched=[\"a.py\"]\n   415→        ),\n   416→    ]\n   417→\n   418→    result = reporter._serialize_condition(runs)\n   419→\n   420→    # Success rate: 2/3\n   421→    assert result[\"success_rate\"] == 0.67\n   422→    assert result[\"success\"] is True  # majority pass\n   423→    assert result[\"successes\"] == 2\n   424→    assert result[\"total_valid_runs\"] == 3\n   425→\n   426→    # Median efficiency (sorted: 90, 100, 120 → median 100)\n   427→    assert result[\"median\"][\"wall_clock_seconds\"] == 100.0\n   428→    assert result[\"median\"][\"tool_calls\"] == 20\n   429→\n   430→    # Individual runs preserved\n   431→    assert len(result[\"runs\"]) == 3\n   432→    assert result[\"runs\"][0][\"success\"] is True\n   433→    assert result[\"runs\"][1][\"success\"] is False\n   434→\n   435→\n   436→def test_multi_run_delta():\n   437→    \"\"\"Multiple runs compute deltas using medians and success rate.\"\"\"\n   438→    reporter = Reporter(output_dir=\"/tmp\")\n   439→\n   440→    baseline = [\n   441→        TaskResult(\n   442→            task_id=\"fix-delta\", condition=Condition.NONE, success=True,\n   443→            test_output=\"PASS\", wall_clock_seconds=100.0,\n   444→            input_tokens=5000, output_tokens=2000, tool_calls=20,\n   445→            lines_changed=50, files_touched=[\"a.py\"]\n   446→        ),\n   447→        TaskResult(\n   448→            task_id=\"fix-delta\", condition=Condition.NONE, success=False,\n   449→            test_output=\"FAIL\", wall_clock_seconds=100.0,\n   450→            input_tokens=5000, output_tokens=2000, tool_calls=20,\n   451→            lines_changed=50, files_touched=[\"a.py\"]\n   452→        ),\n   453→    ]\n   454→    treatment = [\n   455→        TaskResult(\n   456→            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n   457→            test_output=\"PASS\", wall_clock_seconds=80.0,\n   458→            input_tokens=4000, output_tokens=1500, tool_calls=15,\n   459→            lines_changed=30, files_touched=[\"a.py\"]\n   460→        ),\n   461→        TaskResult(\n   462→            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n   463→            test_output=\"PASS\", wall_clock_seconds=80.0,\n   464→            input_tokens=4000, output_tokens=1500, tool_calls=15,\n   465→            lines_changed=30, files_touched=[\"a.py\"]\n   466→        ),\n   467→    ]\n   468→\n   469→    delta = reporter._compute_delta(baseline, treatment)\n   470→\n   471→    # Success rate: baseline 50%, treatment 100%, delta +50%\n   472→    assert delta[\"success_rate_delta\"] == \"+50%\"\n   473→    # Time: (80 - 100) / 100 = -20%\n   474→    assert delta[\"time_percent\"] == \"-20.0%\"\n   475→    # Tokens: (5500 - 7000) / 7000 = -21.4%\n   476→    assert delta[\"tokens_percent\"] == \"-21.4%\"\n   477→\n   478→\n   479→def test_get_fix_metrics_all_formats():\n   480→    \"\"\"_get_fix_metrics extracts correctly from all three data shapes.\"\"\"\n   481→    # Multi-run median format\n   482→    multi = {\"median\": {\n   483→        \"wall_clock_seconds\": 50.0,\n   484→        \"input_tokens\": 3000, \"output_tokens\": 1000,\n   485→        \"tool_calls\": 10, \"lines_changed\": 25,\n   486→    }}\n   487→    m = Reporter._get_fix_metrics(multi)\n   488→    assert m[\"wall_clock_seconds\"] == 50.0\n   489→    assert m[\"tokens\"] == 4000\n   490→    assert m[\"tool_calls\"] == 10\n   491→    assert m[\"lines_changed\"] == 25\n   492→\n   493→    # Single-run with skill_generation (fix_only)\n   494→    single_sg = {\"fix_only\": {\n   495→        \"wall_clock_seconds\": 80.0,\n   496→        \"input_tokens\": 4000, \"output_tokens\": 1500,\n   497→        \"tool_calls\": 15, \"lines_changed\": 30,\n   498→    }}\n   499→    s = Reporter._get_fix_metrics(single_sg)\n   500→    assert s[\"wall_clock_seconds\"] == 80.0\n   501→    assert s[\"tokens\"] == 5500\n   502→\n   503→    # Single-run flat (no skill_generation)\n   504→    flat = {\n   505→        \"wall_clock_seconds\": 100.0,\n   506→        \"input_tokens\": 5000, \"output_tokens\": 2000,\n   507→        \"tool_calls\": 20, \"lines_changed\": 50,\n   508→    }\n   509→    f = Reporter._get_fix_metrics(flat)\n   510→    assert f[\"wall_clock_seconds\"] == 100.0\n   511→    assert f[\"tokens\"] == 7000\n   512→\n   513→\n   514→def test_serialize_includes_exit_code_and_timeout():\n   515→    \"\"\"JSON output includes exit_code and is_timeout when present.\"\"\"\n   516→    reporter = Reporter(output_dir=\"/tmp\")\n   517→\n   518→    result = TaskResult(\n   519→        task_id=\"fix-meta\",\n   520→        condition=Condition.NONE,\n   521→        success=False,\n   522→        test_output=\"FAIL\",\n   523→        wall_clock_seconds=300.0,\n   524→        input_tokens=50000,\n   525→        output_tokens=3000,\n   526→        tool_calls=5,\n   527→        lines_changed=0,\n   528→        files_touched=[],\n   529→        error=\"[timeout] Claude timed out after 300.0s\",\n   530→        exit_code=-1,\n   531→        is_timeout=True,\n   532→    )\n   533→\n   534→    serialized = reporter._serialize_single_result(result)\n   535→\n   536→    assert serialized[\"exit_code\"] == -1\n   537→    assert serialized[\"is_timeout\"] is True\n   538→    assert serialized[\"error\"] == \"[timeout] Claude timed out after 300.0s\"\n   539→\n   540→\n   541→def test_serialize_omits_exit_code_when_none():\n   542→    \"\"\"JSON output omits exit_code when it's None (backward compat).\"\"\"\n   543→    reporter = Reporter(output_dir=\"/tmp\")\n   544→\n   545→    result = TaskResult(\n   546→        task_id=\"fix-ok\",\n   547→        condition=Condition.NONE,\n   548→        success=True,\n   549→        test_output=\"PASS\",\n   550→        wall_clock_seconds=50.0,\n   551→        input_tokens=2000,\n   552→        output_tokens=1000,\n   553→        tool_calls=10,\n   554→        lines_changed=20,\n   555→        files_touched=[\"a.py\"],\n   556→    )\n   557→\n   558→    serialized = reporter._serialize_single_result(result)\n   559→\n   560→    assert \"exit_code\" not in serialized\n   561→    assert \"is_timeout\" not in serialized\n   562→\n   563→\n   564→def test_single_run_delta_uses_success_rate_delta_key():\n   565→    \"\"\"Single-run deltas use 'success_rate_delta' (not 'success').\"\"\"\n   566→    reporter = Reporter(output_dir=\"/tmp\")\n   567→    baseline = [TaskResult(\n   568→        task_id=\"fix-key\", condition=Condition.NONE, success=False,\n   569→        test_output=\"FAIL\", wall_clock_seconds=100.0,\n   570→        input_tokens=5000, output_tokens=2000, tool_calls=20,\n   571→        lines_changed=50, files_touched=[\"a.py\"]\n   572→    )]\n   573→    treatment = [TaskResult(\n   574→        task_id=\"fix-key\", condition=Condition.FLAT_LLM, success=True,\n   575→        test_output=\"PASS\", wall_clock_seconds=80.0,\n   576→        input_tokens=4000, output_tokens=1500, tool_calls=15,\n   577→        lines_changed=30, files_touched=[\"a.py\"]\n   578→    )]\n   579→    delta = reporter._compute_delta(baseline, treatment)\n   580→    assert \"success_rate_delta\" in delta\n   581→    assert \"success\" not in delta\n   582→    assert delta[\"success_rate_delta\"] == \"+1\"\n   583→"}]},"uuid":"ac8dd7a8-ff84-4661-8dc3-200701267009","timestamp":"2026-02-17T20:06:11.348Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","content":"def test_multi_run_serialize_condition():\n    \"\"\"Multiple runs produce success_rate, median, and runs array.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    runs = [\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=100.0,\n            input_tokens=5000,\n            output_tokens=2000,\n            tool_calls=20,\n            lines_changed=50,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=120.0,\n            input_tokens=6000,\n            output_tokens=2500,\n            tool_calls=25,\n            lines_changed=60,\n            files_touched=[\"a.py\", \"b.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=90.0,\n            input_tokens=4500,\n            output_tokens=1800,\n            tool_calls=18,\n            lines_changed=45,\n            files_touched=[\"a.py\"]\n        ),\n    ]\n\n    result = reporter._serialize_condition(runs)\n\n    # Success rate: 2/3\n    assert result[\"success_rate\"] == 0.67\n    assert result[\"success\"] is True  # majority pass\n    assert result[\"successes\"] == 2\n    assert result[\"total_valid_runs\"] == 3\n\n    # Median efficiency (sorted: 90, 100, 120 → median 100)\n    assert result[\"median\"][\"wall_clock_seconds\"] == 100.0\n    assert result[\"median\"][\"tool_calls\"] == 20\n\n    # Individual runs preserved\n    assert len(result[\"runs\"]) == 3\n    assert result[\"runs\"][0][\"success\"] is True\n    assert result[\"runs\"][1][\"success\"] is False\n\n\ndef test_multi_run_delta():\n    \"\"\"Multiple runs compute deltas using medians and success rate.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    baseline = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=False,\n            test_output=\"FAIL\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n    ]\n    treatment = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n    ]\n\n    delta = reporter._compute_delta(baseline, treatment)\n\n    # Success rate: baseline 50%, treatment 100%, delta +50%\n    assert delta[\"success_rate_delta\"] == \"+50%\"\n    # Time: (80 - 100) / 100 = -20%\n    assert delta[\"time_percent\"] == \"-20.0%\"\n    # Tokens: (5500 - 7000) / 7000 = -21.4%\n    assert delta[\"tokens_percent\"] == \"-21.4%\"\n\n\ndef test_get_fix_metrics_all_formats():\n    \"\"\"_get_fix_metrics extracts correctly from all three data shapes.\"\"\"\n    # Multi-run median format\n    multi = {\"median\": {\n        \"wall_clock_seconds\": 50.0,\n        \"input_tokens\": 3000, \"output_tokens\": 1000,\n        \"tool_calls\": 10, \"lines_changed\": 25,\n    }}\n    m = Reporter._get_fix_metrics(multi)\n    assert m[\"wall_clock_seconds\"] == 50.0\n    assert m[\"tokens\"] == 4000\n    assert m[\"tool_calls\"] == 10\n    assert m[\"lines_changed\"] == 25\n\n    # Single-run with skill_generation (fix_only)\n    single_sg = {\"fix_only\": {\n        \"wall_clock_seconds\": 80.0,\n        \"input_tokens\": 4000, \"output_tokens\": 1500,\n        \"tool_calls\": 15, \"lines_changed\": 30,\n    }}\n    s = Reporter._get_fix_metrics(single_sg)\n    assert s[\"wall_clock_seconds\"] == 80.0\n    assert s[\"tokens\"] == 5500\n\n    # Single-run flat (no skill_generation)\n    flat = {\n        \"wall_clock_seconds\": 100.0,\n        \"input_tokens\": 5000, \"output_tokens\": 2000,\n        \"tool_calls\": 20, \"lines_changed\": 50,\n    }\n    f = Reporter._get_fix_metrics(flat)\n    assert f[\"wall_clock_seconds\"] == 100.0\n    assert f[\"tokens\"] == 7000\n\n\ndef test_serialize_includes_exit_code_and_timeout():\n    \"\"\"JSON output includes exit_code and is_timeout when present.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-meta\",\n        condition=Condition.NONE,\n        success=False,\n        test_output=\"FAIL\",\n        wall_clock_seconds=300.0,\n        input_tokens=50000,\n        output_tokens=3000,\n        tool_calls=5,\n        lines_changed=0,\n        files_touched=[],\n        error=\"[timeout] Claude timed out after 300.0s\",\n        exit_code=-1,\n        is_timeout=True,\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert serialized[\"exit_code\"] == -1\n    assert serialized[\"is_timeout\"] is True\n    assert serialized[\"error\"] == \"[timeout] Claude timed out after 300.0s\"\n\n\ndef test_serialize_omits_exit_code_when_none():\n    \"\"\"JSON output omits exit_code when it's None (backward compat).\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-ok\",\n        condition=Condition.NONE,\n        success=True,\n        test_output=\"PASS\",\n        wall_clock_seconds=50.0,\n        input_tokens=2000,\n        output_tokens=1000,\n        tool_calls=10,\n        lines_changed=20,\n        files_touched=[\"a.py\"],\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert \"exit_code\" not in serialized\n    assert \"is_timeout\" not in serialized\n\n\ndef test_single_run_delta_uses_success_rate_delta_key():\n    \"\"\"Single-run deltas use 'success_rate_delta' (not 'success').\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    baseline = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.NONE, success=False,\n        test_output=\"FAIL\", wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=[\"a.py\"]\n    )]\n    treatment = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.FLAT_LLM, success=True,\n        test_output=\"PASS\", wall_clock_seconds=80.0,\n        input_tokens=4000, output_tokens=1500, tool_calls=15,\n        lines_changed=30, files_touched=[\"a.py\"]\n    )]\n    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"\n","numLines":209,"startLine":375,"totalLines":583}},"sourceToolAssistantUUID":"56e08c82-7343-4c03-9a9d-e44a248f048d"}
{"parentUuid":"ac8dd7a8-ff84-4661-8dc3-200701267009","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01ByepxzqhHZGq1npXZfmXCA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Vh2sBeVzqqRNLSDe76LZke","name":"Edit","input":{"replace_all":false,"file_path":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","old_string":"    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"","new_string":"    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"\n\n\n# ── CI integration tests ───────────────────────────────────────\n\n\ndef _make_multi_run_results():\n    \"\"\"Helper: 3 runs per condition across 2 tasks, with varying outcomes.\"\"\"\n    results = []\n    for task_id in (\"task-1\", \"task-2\"):\n        # none: 1/3 pass\n        for i, success in enumerate([True, False, False]):\n            results.append(TaskResult(\n                task_id=task_id, condition=Condition.NONE, success=success,\n                test_output=\"PASS\" if success else \"FAIL\",\n                wall_clock_seconds=100.0, input_tokens=5000, output_tokens=2000,\n                tool_calls=20, lines_changed=50, files_touched=[\"a.py\"],\n            ))\n        # intent_layer: 3/3 pass\n        for i in range(3):\n            results.append(TaskResult(\n                task_id=task_id, condition=Condition.INTENT_LAYER, success=True,\n                test_output=\"PASS\", wall_clock_seconds=60.0,\n                input_tokens=3000, output_tokens=1000, tool_calls=10,\n                lines_changed=25, files_touched=[\"a.py\"],\n            ))\n    return results\n\n\ndef test_multi_run_serialize_condition_has_ci():\n    \"\"\"Multi-run serialization includes ci_90 field.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    runs = [\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"],\n        ),\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=False,\n            test_output=\"FAIL\", wall_clock_seconds=120.0,\n            input_tokens=6000, output_tokens=2500, tool_calls=25,\n            lines_changed=60, files_touched=[\"a.py\"],\n        ),\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=90.0,\n            input_tokens=4500, output_tokens=1800, tool_calls=18,\n            lines_changed=45, files_touched=[\"a.py\"],\n        ),\n    ]\n\n    result = reporter._serialize_condition(runs)\n\n    assert \"ci_90\" in result\n    ci = result[\"ci_90\"]\n    assert \"lower\" in ci\n    assert \"upper\" in ci\n    assert 0 <= ci[\"lower\"] <= ci[\"upper\"] <= 1\n    # 2/3 successes: lower bound should be > 0, upper < 1\n    assert ci[\"lower\"] > 0\n    assert ci[\"upper\"] < 1\n\n\ndef test_single_run_has_no_ci():\n    \"\"\"Single-run serialization does not add CI (backward-compatible).\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    run = TaskResult(\n        task_id=\"fix-single\", condition=Condition.NONE, success=True,\n        test_output=\"PASS\", wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=[\"a.py\"],\n    )\n\n    result = reporter._serialize_condition([run])\n    assert \"ci_90\" not in result\n\n\ndef test_summary_has_cis_for_multi_run():\n    \"\"\"Summary includes per-condition CIs and significance flags for multi-run.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    summary = eval_results.summary\n\n    # CIs present\n    assert \"none_ci_90\" in summary\n    assert \"intent_layer_ci_90\" in summary\n\n    none_ci = summary[\"none_ci_90\"]\n    il_ci = summary[\"intent_layer_ci_90\"]\n\n    # Bounds are valid\n    assert 0 <= none_ci[\"lower\"] <= none_ci[\"upper\"] <= 1\n    assert 0 <= il_ci[\"lower\"] <= il_ci[\"upper\"] <= 1\n\n    # none: 2/6 pass → lower rate; IL: 6/6 → higher rate\n    assert none_ci[\"upper\"] < il_ci[\"upper\"]\n\n    # Significance flag present\n    assert \"intent_layer_vs_none_significant\" in summary\n\n\ndef test_summary_no_cis_for_single_run(three_condition_results):\n    \"\"\"Summary omits CIs when all results are single-run.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n    summary = eval_results.summary\n\n    assert \"none_ci_90\" not in summary\n    assert \"intent_layer_ci_90\" not in summary\n    assert \"intent_layer_vs_none_significant\" not in summary\n\n\ndef test_markdown_multi_run_has_ci_columns(tmp_path):\n    \"\"\"Markdown output includes CI brackets and IL vs none column for multi-run.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    # CI brackets appear in success column\n    assert \"[\" in content and \"]\" in content\n\n    # IL vs none column header present\n    assert \"IL vs none\" in content\n\n    # Significance labels appear\n    assert \"overlap\" in content or \"sig.\" in content\n\n    # Summary has CI notation\n    assert \"90% CI\" in content\n\n\ndef test_markdown_single_run_no_ci_column(tmp_path, three_condition_results):\n    \"\"\"Single-run markdown has no IL vs none column (backward-compatible).\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    assert \"IL vs none\" not in content\n    assert \"90% CI\" not in content\n\n\ndef test_json_multi_run_has_ci(tmp_path):\n    \"\"\"JSON output includes ci_90 in multi-run condition data.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    json_path = reporter.write_json(eval_results)\n\n    import json\n    with open(json_path) as f:\n        data = json.load(f)\n\n    task = data[\"results\"][0]\n    assert \"ci_90\" in task[\"none\"]\n    assert \"ci_90\" in task[\"intent_layer\"]\n\n    # Summary has CIs too\n    assert \"none_ci_90\" in data[\"summary\"]\n    assert \"intent_layer_ci_90\" in data[\"summary\"]"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2993,"cache_read_input_tokens":100404,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2993},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF41Qeda1x8f9xo3Bgv","type":"assistant","uuid":"72e62d57-59fa-413e-b3af-e102817049d1","timestamp":"2026-02-17T20:06:38.026Z"}
{"parentUuid":"72e62d57-59fa-413e-b3af-e102817049d1","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/pre-edit-check.sh"},"toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","uuid":"80d9c7dc-529d-41bb-92de-9db8ee6b2a19","timestamp":"2026-02-17T20:06:38.028Z"}
{"parentUuid":"80d9c7dc-529d-41bb-92de-9db8ee6b2a19","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/security_reminder_hook.py"},"toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","uuid":"268bce8e-b26b-471a-8f69-ce8e846cef27","timestamp":"2026-02-17T20:06:38.028Z"}
{"parentUuid":"268bce8e-b26b-471a-8f69-ce8e846cef27","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","uuid":"87844663-2545-4d58-945a-cba96219aa62","timestamp":"2026-02-17T20:06:38.028Z"}
{"type":"file-history-snapshot","messageId":"72e62d57-59fa-413e-b3af-e102817049d1","snapshot":{"messageId":"cea51334-b6a6-4f40-8b10-2c1e3090c96d","trackedFileBackups":{"eval-harness/lib/stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:20.833Z"},"eval-harness/tests/test_stats.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-17T20:04:42.720Z"},"eval-harness/lib/reporter.py":{"backupFileName":"77c006e5b2bb1d0f@v1","version":1,"backupTime":"2026-02-17T20:04:58.224Z"},"eval-harness/tests/test_reporter.py":{"backupFileName":"7dcb4b31c240ae3a@v1","version":1,"backupTime":"2026-02-17T20:06:38.150Z"}},"timestamp":"2026-02-17T20:03:32.850Z"},"isSnapshotUpdate":true}
{"parentUuid":"87844663-2545-4d58-945a-cba96219aa62","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"${CLAUDE_PLUGIN_ROOT}/scripts/post-edit-check.sh \"$TOOL_INPUT\""},"parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","timestamp":"2026-02-17T20:06:38.153Z","uuid":"2e0d2838-b194-4dd3-8c0d-5efa842ec189"}
{"parentUuid":"2e0d2838-b194-4dd3-8c0d-5efa842ec189","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","timestamp":"2026-02-17T20:06:38.153Z","uuid":"6c725253-a189-4d61-8439-172b4a3d7d12"}
{"parentUuid":"6c725253-a189-4d61-8439-172b4a3d7d12","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","toolUseID":"toolu_01Vh2sBeVzqqRNLSDe76LZke","timestamp":"2026-02-17T20:06:38.153Z","uuid":"a90d712d-cdde-407d-855f-0e52a6784b4e"}
{"parentUuid":"72e62d57-59fa-413e-b3af-e102817049d1","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Vh2sBeVzqqRNLSDe76LZke","type":"tool_result","content":"The file /Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py has been updated successfully."}]},"uuid":"3adfe52f-4465-46aa-8b57-cc6d0dc81ff6","timestamp":"2026-02-17T20:06:38.152Z","toolUseResult":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/tests/test_reporter.py","oldString":"    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"","newString":"    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"\n\n\n# ── CI integration tests ───────────────────────────────────────\n\n\ndef _make_multi_run_results():\n    \"\"\"Helper: 3 runs per condition across 2 tasks, with varying outcomes.\"\"\"\n    results = []\n    for task_id in (\"task-1\", \"task-2\"):\n        # none: 1/3 pass\n        for i, success in enumerate([True, False, False]):\n            results.append(TaskResult(\n                task_id=task_id, condition=Condition.NONE, success=success,\n                test_output=\"PASS\" if success else \"FAIL\",\n                wall_clock_seconds=100.0, input_tokens=5000, output_tokens=2000,\n                tool_calls=20, lines_changed=50, files_touched=[\"a.py\"],\n            ))\n        # intent_layer: 3/3 pass\n        for i in range(3):\n            results.append(TaskResult(\n                task_id=task_id, condition=Condition.INTENT_LAYER, success=True,\n                test_output=\"PASS\", wall_clock_seconds=60.0,\n                input_tokens=3000, output_tokens=1000, tool_calls=10,\n                lines_changed=25, files_touched=[\"a.py\"],\n            ))\n    return results\n\n\ndef test_multi_run_serialize_condition_has_ci():\n    \"\"\"Multi-run serialization includes ci_90 field.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    runs = [\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"],\n        ),\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=False,\n            test_output=\"FAIL\", wall_clock_seconds=120.0,\n            input_tokens=6000, output_tokens=2500, tool_calls=25,\n            lines_changed=60, files_touched=[\"a.py\"],\n        ),\n        TaskResult(\n            task_id=\"fix-ci\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=90.0,\n            input_tokens=4500, output_tokens=1800, tool_calls=18,\n            lines_changed=45, files_touched=[\"a.py\"],\n        ),\n    ]\n\n    result = reporter._serialize_condition(runs)\n\n    assert \"ci_90\" in result\n    ci = result[\"ci_90\"]\n    assert \"lower\" in ci\n    assert \"upper\" in ci\n    assert 0 <= ci[\"lower\"] <= ci[\"upper\"] <= 1\n    # 2/3 successes: lower bound should be > 0, upper < 1\n    assert ci[\"lower\"] > 0\n    assert ci[\"upper\"] < 1\n\n\ndef test_single_run_has_no_ci():\n    \"\"\"Single-run serialization does not add CI (backward-compatible).\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    run = TaskResult(\n        task_id=\"fix-single\", condition=Condition.NONE, success=True,\n        test_output=\"PASS\", wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=[\"a.py\"],\n    )\n\n    result = reporter._serialize_condition([run])\n    assert \"ci_90\" not in result\n\n\ndef test_summary_has_cis_for_multi_run():\n    \"\"\"Summary includes per-condition CIs and significance flags for multi-run.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    summary = eval_results.summary\n\n    # CIs present\n    assert \"none_ci_90\" in summary\n    assert \"intent_layer_ci_90\" in summary\n\n    none_ci = summary[\"none_ci_90\"]\n    il_ci = summary[\"intent_layer_ci_90\"]\n\n    # Bounds are valid\n    assert 0 <= none_ci[\"lower\"] <= none_ci[\"upper\"] <= 1\n    assert 0 <= il_ci[\"lower\"] <= il_ci[\"upper\"] <= 1\n\n    # none: 2/6 pass → lower rate; IL: 6/6 → higher rate\n    assert none_ci[\"upper\"] < il_ci[\"upper\"]\n\n    # Significance flag present\n    assert \"intent_layer_vs_none_significant\" in summary\n\n\ndef test_summary_no_cis_for_single_run(three_condition_results):\n    \"\"\"Summary omits CIs when all results are single-run.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n    summary = eval_results.summary\n\n    assert \"none_ci_90\" not in summary\n    assert \"intent_layer_ci_90\" not in summary\n    assert \"intent_layer_vs_none_significant\" not in summary\n\n\ndef test_markdown_multi_run_has_ci_columns(tmp_path):\n    \"\"\"Markdown output includes CI brackets and IL vs none column for multi-run.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    # CI brackets appear in success column\n    assert \"[\" in content and \"]\" in content\n\n    # IL vs none column header present\n    assert \"IL vs none\" in content\n\n    # Significance labels appear\n    assert \"overlap\" in content or \"sig.\" in content\n\n    # Summary has CI notation\n    assert \"90% CI\" in content\n\n\ndef test_markdown_single_run_no_ci_column(tmp_path, three_condition_results):\n    \"\"\"Single-run markdown has no IL vs none column (backward-compatible).\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    assert \"IL vs none\" not in content\n    assert \"90% CI\" not in content\n\n\ndef test_json_multi_run_has_ci(tmp_path):\n    \"\"\"JSON output includes ci_90 in multi-run condition data.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    results = _make_multi_run_results()\n    eval_results = reporter.compile_results(results)\n    json_path = reporter.write_json(eval_results)\n\n    import json\n    with open(json_path) as f:\n        data = json.load(f)\n\n    task = data[\"results\"][0]\n    assert \"ci_90\" in task[\"none\"]\n    assert \"ci_90\" in task[\"intent_layer\"]\n\n    # Summary has CIs too\n    assert \"none_ci_90\" in data[\"summary\"]\n    assert \"intent_layer_ci_90\" in data[\"summary\"]","originalFile":"# tests/test_reporter.py\nimport pytest\nimport json\nfrom pathlib import Path\nfrom lib.reporter import Reporter, EvalResults\nfrom lib.task_runner import TaskResult, Condition, SkillGenerationMetrics\n\n\n@pytest.fixture\ndef three_condition_results():\n    return [\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=100.0,\n            input_tokens=5000,\n            output_tokens=2000,\n            tool_calls=20,\n            lines_changed=50,\n            files_touched=[\"a.py\", \"b.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.FLAT_LLM,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=80.0,\n            input_tokens=4000,\n            output_tokens=1500,\n            tool_calls=15,\n            lines_changed=30,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=20.0,\n                input_tokens=1000,\n                output_tokens=300\n            )\n        ),\n        TaskResult(\n            task_id=\"fix-123\",\n            condition=Condition.INTENT_LAYER,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=60.0,\n            input_tokens=3000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=25,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=30.0,\n                input_tokens=2000,\n                output_tokens=500\n            )\n        ),\n    ]\n\n\ndef test_three_condition_compilation(three_condition_results):\n    \"\"\"All three conditions present — none, flat_llm, intent_layer.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    assert len(eval_results.results) == 1\n    task = eval_results.results[0]\n\n    assert task[\"task_id\"] == \"fix-123\"\n    assert task[\"none\"] is not None\n    assert task[\"flat_llm\"] is not None\n    assert task[\"intent_layer\"] is not None\n\n    # none should be flat structure (no skill_generation)\n    assert task[\"none\"][\"success\"] is False\n    assert task[\"none\"][\"wall_clock_seconds\"] == 100.0\n    assert \"fix_only\" not in task[\"none\"]\n\n    # flat_llm should have three-level structure\n    assert task[\"flat_llm\"][\"success\"] is True\n    assert \"fix_only\" in task[\"flat_llm\"]\n    assert task[\"flat_llm\"][\"fix_only\"][\"wall_clock_seconds\"] == 80.0\n\n    # intent_layer should have three-level structure\n    assert task[\"intent_layer\"][\"success\"] is True\n    assert \"fix_only\" in task[\"intent_layer\"]\n    assert task[\"intent_layer\"][\"fix_only\"][\"wall_clock_seconds\"] == 60.0\n\n\ndef test_deltas_relative_to_none(three_condition_results):\n    \"\"\"Both flat_llm and intent_layer get deltas against the none baseline.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    task = eval_results.results[0]\n    deltas = task[\"deltas\"]\n\n    # flat_llm delta: time (80 - 100) / 100 = -20%\n    assert deltas[\"flat_llm\"][\"time_percent\"] == \"-20.0%\"\n    # flat_llm delta: tokens (5500 - 7000) / 7000 = -21.4%\n    assert deltas[\"flat_llm\"][\"tokens_percent\"] == \"-21.4%\"\n    # flat_llm delta: success +1 (True - False), single run format\n    assert deltas[\"flat_llm\"][\"success_rate_delta\"] == \"+1\"\n\n    # intent_layer delta: time (60 - 100) / 100 = -40%\n    assert deltas[\"intent_layer\"][\"time_percent\"] == \"-40.0%\"\n    # intent_layer delta: tokens (4000 - 7000) / 7000 = -42.9%\n    assert deltas[\"intent_layer\"][\"tokens_percent\"] == \"-42.9%\"\n    assert deltas[\"intent_layer\"][\"success_rate_delta\"] == \"+1\"\n\n\ndef test_zero_baseline_delta():\n    \"\"\"If none has 0 time/tokens, don't divide by zero.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-zero\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=0.0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[]\n        ),\n        TaskResult(\n            task_id=\"fix-zero\",\n            condition=Condition.FLAT_LLM,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=3000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=10.0,\n                input_tokens=500,\n                output_tokens=100\n            )\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    task = eval_results.results[0]\n    delta = task[\"deltas\"][\"flat_llm\"]\n\n    # Should not raise ZeroDivisionError; uses 0 fallback\n    assert delta[\"time_percent\"] == \"+0.0%\"\n    assert delta[\"tokens_percent\"] == \"+0.0%\"\n    assert delta[\"tool_calls_percent\"] == \"+0.0%\"\n    assert delta[\"lines_changed_percent\"] == \"+0.0%\"\n\n\ndef test_missing_condition():\n    \"\"\"Only 2 of 3 conditions run — missing one is None.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-partial\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=90.0,\n            input_tokens=4000,\n            output_tokens=1500,\n            tool_calls=18,\n            lines_changed=40,\n            files_touched=[\"x.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-partial\",\n            condition=Condition.INTENT_LAYER,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=800,\n            tool_calls=8,\n            lines_changed=20,\n            files_touched=[\"x.py\"],\n            skill_generation=SkillGenerationMetrics(\n                wall_clock_seconds=25.0,\n                input_tokens=1500,\n                output_tokens=400\n            )\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    task = eval_results.results[0]\n    assert task[\"none\"] is not None\n    assert task[\"flat_llm\"] is None\n    assert task[\"intent_layer\"] is not None\n\n    # flat_llm delta should be empty (missing condition)\n    assert task[\"deltas\"][\"flat_llm\"] == {}\n    # intent_layer delta should exist\n    assert task[\"deltas\"][\"intent_layer\"][\"time_percent\"] == \"-44.4%\"\n\n\ndef test_summary_three_success_rates(three_condition_results):\n    \"\"\"Summary has all three success rates.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(three_condition_results)\n\n    summary = eval_results.summary\n    assert summary[\"total_tasks\"] == 1\n    assert summary[\"infrastructure_errors\"] == 0\n    assert summary[\"none_success_rate\"] == 0.0\n    assert summary[\"flat_llm_success_rate\"] == 1.0\n    assert summary[\"intent_layer_success_rate\"] == 1.0\n\n\ndef test_markdown_multi_row_layout(tmp_path, three_condition_results):\n    \"\"\"Markdown output has multi-row layout with correct structure.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    md_path = reporter.write_markdown(eval_results)\n\n    with open(md_path) as f:\n        content = f.read()\n\n    # Header has new columns\n    assert \"| Task | Condition | Success |\" in content\n    assert \"| Time (s) |\" in content\n\n    # Summary has all three rates\n    assert \"None success rate\" in content\n    assert \"Flat LLM success rate\" in content\n    assert \"Intent Layer success rate\" in content\n\n    lines = content.split(\"\\n\")\n\n    # Find data rows for fix-123\n    data_rows = [l for l in lines if l.startswith(\"| fix-123 |\")]\n    assert len(data_rows) == 3, f\"Expected 3 rows for fix-123, got {len(data_rows)}\"\n\n    # First row: none condition, baseline deltas show em-dash\n    none_row = data_rows[0]\n    assert \"| none |\" in none_row\n    assert \"| FAIL |\" in none_row\n    assert \"\\u2014\" in none_row  # em-dash for baseline\n\n    # Second row: flat_llm\n    flat_row = data_rows[1]\n    assert \"| flat_llm |\" in flat_row\n    assert \"| PASS |\" in flat_row\n    assert \"-20.0%\" in flat_row  # time delta\n\n    # Third row: intent_layer\n    il_row = data_rows[2]\n    assert \"| intent_layer |\" in il_row\n    assert \"| PASS |\" in il_row\n    assert \"-40.0%\" in il_row  # time delta\n\n\ndef test_json_output(tmp_path, three_condition_results):\n    \"\"\"JSON output writes correctly with new structure.\"\"\"\n    reporter = Reporter(output_dir=str(tmp_path))\n    eval_results = reporter.compile_results(three_condition_results)\n    json_path = reporter.write_json(eval_results)\n\n    assert Path(json_path).exists()\n    with open(json_path) as f:\n        data = json.load(f)\n\n    assert \"results\" in data\n    assert \"summary\" in data\n    assert len(data[\"results\"]) == 1\n\n    task = data[\"results\"][0]\n    assert \"none\" in task\n    assert \"flat_llm\" in task\n    assert \"intent_layer\" in task\n    assert \"deltas\" in task\n\n\ndef test_infrastructure_errors_excluded_from_success_rate():\n    \"\"\"Infrastructure errors should not count toward success rates.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-good\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-infra-fail\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[infrastructure] clone failed: network timeout\"\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    summary = eval_results.summary\n    assert summary[\"infrastructure_errors\"] == 1\n    # Only fix-good counts — 1 success out of 1 valid = 1.0\n    assert summary[\"none_success_rate\"] == 1.0\n\n\ndef test_pre_validation_and_skill_gen_errors_excluded():\n    \"\"\"Pre-validation and skill-generation errors are also excluded from stats.\"\"\"\n    results = [\n        TaskResult(\n            task_id=\"fix-ok\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=50.0,\n            input_tokens=2000,\n            output_tokens=1000,\n            tool_calls=10,\n            lines_changed=20,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-preval\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[pre-validation] test doesn't fail at pre_fix_commit\"\n        ),\n        TaskResult(\n            task_id=\"fix-skillgen\",\n            condition=Condition.INTENT_LAYER,\n            success=False,\n            test_output=\"\",\n            wall_clock_seconds=0,\n            input_tokens=0,\n            output_tokens=0,\n            tool_calls=0,\n            lines_changed=0,\n            files_touched=[],\n            error=\"[skill-generation] no files created\"\n        ),\n    ]\n\n    reporter = Reporter(output_dir=\"/tmp\")\n    eval_results = reporter.compile_results(results)\n\n    summary = eval_results.summary\n    assert summary[\"infrastructure_errors\"] == 2\n    assert summary[\"none_success_rate\"] == 1.0\n    assert summary[\"intent_layer_success_rate\"] == 0\n\n\ndef test_multi_run_serialize_condition():\n    \"\"\"Multiple runs produce success_rate, median, and runs array.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    runs = [\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=100.0,\n            input_tokens=5000,\n            output_tokens=2000,\n            tool_calls=20,\n            lines_changed=50,\n            files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=False,\n            test_output=\"FAIL\",\n            wall_clock_seconds=120.0,\n            input_tokens=6000,\n            output_tokens=2500,\n            tool_calls=25,\n            lines_changed=60,\n            files_touched=[\"a.py\", \"b.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-multi\",\n            condition=Condition.NONE,\n            success=True,\n            test_output=\"PASS\",\n            wall_clock_seconds=90.0,\n            input_tokens=4500,\n            output_tokens=1800,\n            tool_calls=18,\n            lines_changed=45,\n            files_touched=[\"a.py\"]\n        ),\n    ]\n\n    result = reporter._serialize_condition(runs)\n\n    # Success rate: 2/3\n    assert result[\"success_rate\"] == 0.67\n    assert result[\"success\"] is True  # majority pass\n    assert result[\"successes\"] == 2\n    assert result[\"total_valid_runs\"] == 3\n\n    # Median efficiency (sorted: 90, 100, 120 → median 100)\n    assert result[\"median\"][\"wall_clock_seconds\"] == 100.0\n    assert result[\"median\"][\"tool_calls\"] == 20\n\n    # Individual runs preserved\n    assert len(result[\"runs\"]) == 3\n    assert result[\"runs\"][0][\"success\"] is True\n    assert result[\"runs\"][1][\"success\"] is False\n\n\ndef test_multi_run_delta():\n    \"\"\"Multiple runs compute deltas using medians and success rate.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    baseline = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=True,\n            test_output=\"PASS\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.NONE, success=False,\n            test_output=\"FAIL\", wall_clock_seconds=100.0,\n            input_tokens=5000, output_tokens=2000, tool_calls=20,\n            lines_changed=50, files_touched=[\"a.py\"]\n        ),\n    ]\n    treatment = [\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n        TaskResult(\n            task_id=\"fix-delta\", condition=Condition.FLAT_LLM, success=True,\n            test_output=\"PASS\", wall_clock_seconds=80.0,\n            input_tokens=4000, output_tokens=1500, tool_calls=15,\n            lines_changed=30, files_touched=[\"a.py\"]\n        ),\n    ]\n\n    delta = reporter._compute_delta(baseline, treatment)\n\n    # Success rate: baseline 50%, treatment 100%, delta +50%\n    assert delta[\"success_rate_delta\"] == \"+50%\"\n    # Time: (80 - 100) / 100 = -20%\n    assert delta[\"time_percent\"] == \"-20.0%\"\n    # Tokens: (5500 - 7000) / 7000 = -21.4%\n    assert delta[\"tokens_percent\"] == \"-21.4%\"\n\n\ndef test_get_fix_metrics_all_formats():\n    \"\"\"_get_fix_metrics extracts correctly from all three data shapes.\"\"\"\n    # Multi-run median format\n    multi = {\"median\": {\n        \"wall_clock_seconds\": 50.0,\n        \"input_tokens\": 3000, \"output_tokens\": 1000,\n        \"tool_calls\": 10, \"lines_changed\": 25,\n    }}\n    m = Reporter._get_fix_metrics(multi)\n    assert m[\"wall_clock_seconds\"] == 50.0\n    assert m[\"tokens\"] == 4000\n    assert m[\"tool_calls\"] == 10\n    assert m[\"lines_changed\"] == 25\n\n    # Single-run with skill_generation (fix_only)\n    single_sg = {\"fix_only\": {\n        \"wall_clock_seconds\": 80.0,\n        \"input_tokens\": 4000, \"output_tokens\": 1500,\n        \"tool_calls\": 15, \"lines_changed\": 30,\n    }}\n    s = Reporter._get_fix_metrics(single_sg)\n    assert s[\"wall_clock_seconds\"] == 80.0\n    assert s[\"tokens\"] == 5500\n\n    # Single-run flat (no skill_generation)\n    flat = {\n        \"wall_clock_seconds\": 100.0,\n        \"input_tokens\": 5000, \"output_tokens\": 2000,\n        \"tool_calls\": 20, \"lines_changed\": 50,\n    }\n    f = Reporter._get_fix_metrics(flat)\n    assert f[\"wall_clock_seconds\"] == 100.0\n    assert f[\"tokens\"] == 7000\n\n\ndef test_serialize_includes_exit_code_and_timeout():\n    \"\"\"JSON output includes exit_code and is_timeout when present.\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-meta\",\n        condition=Condition.NONE,\n        success=False,\n        test_output=\"FAIL\",\n        wall_clock_seconds=300.0,\n        input_tokens=50000,\n        output_tokens=3000,\n        tool_calls=5,\n        lines_changed=0,\n        files_touched=[],\n        error=\"[timeout] Claude timed out after 300.0s\",\n        exit_code=-1,\n        is_timeout=True,\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert serialized[\"exit_code\"] == -1\n    assert serialized[\"is_timeout\"] is True\n    assert serialized[\"error\"] == \"[timeout] Claude timed out after 300.0s\"\n\n\ndef test_serialize_omits_exit_code_when_none():\n    \"\"\"JSON output omits exit_code when it's None (backward compat).\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n\n    result = TaskResult(\n        task_id=\"fix-ok\",\n        condition=Condition.NONE,\n        success=True,\n        test_output=\"PASS\",\n        wall_clock_seconds=50.0,\n        input_tokens=2000,\n        output_tokens=1000,\n        tool_calls=10,\n        lines_changed=20,\n        files_touched=[\"a.py\"],\n    )\n\n    serialized = reporter._serialize_single_result(result)\n\n    assert \"exit_code\" not in serialized\n    assert \"is_timeout\" not in serialized\n\n\ndef test_single_run_delta_uses_success_rate_delta_key():\n    \"\"\"Single-run deltas use 'success_rate_delta' (not 'success').\"\"\"\n    reporter = Reporter(output_dir=\"/tmp\")\n    baseline = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.NONE, success=False,\n        test_output=\"FAIL\", wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=[\"a.py\"]\n    )]\n    treatment = [TaskResult(\n        task_id=\"fix-key\", condition=Condition.FLAT_LLM, success=True,\n        test_output=\"PASS\", wall_clock_seconds=80.0,\n        input_tokens=4000, output_tokens=1500, tool_calls=15,\n        lines_changed=30, files_touched=[\"a.py\"]\n    )]\n    delta = reporter._compute_delta(baseline, treatment)\n    assert \"success_rate_delta\" in delta\n    assert \"success\" not in delta\n    assert delta[\"success_rate_delta\"] == \"+1\"\n","structuredPatch":[{"oldStart":580,"oldLines":3,"newStart":580,"newLines":173,"lines":["     assert \"success_rate_delta\" in delta","     assert \"success\" not in delta","     assert delta[\"success_rate_delta\"] == \"+1\"","+","+","+# ── CI integration tests ───────────────────────────────────────","+","+","+def _make_multi_run_results():","+    \"\"\"Helper: 3 runs per condition across 2 tasks, with varying outcomes.\"\"\"","+    results = []","+    for task_id in (\"task-1\", \"task-2\"):","+        # none: 1/3 pass","+        for i, success in enumerate([True, False, False]):","+            results.append(TaskResult(","+                task_id=task_id, condition=Condition.NONE, success=success,","+                test_output=\"PASS\" if success else \"FAIL\",","+                wall_clock_seconds=100.0, input_tokens=5000, output_tokens=2000,","+                tool_calls=20, lines_changed=50, files_touched=[\"a.py\"],","+            ))","+        # intent_layer: 3/3 pass","+        for i in range(3):","+            results.append(TaskResult(","+                task_id=task_id, condition=Condition.INTENT_LAYER, success=True,","+                test_output=\"PASS\", wall_clock_seconds=60.0,","+                input_tokens=3000, output_tokens=1000, tool_calls=10,","+                lines_changed=25, files_touched=[\"a.py\"],","+            ))","+    return results","+","+","+def test_multi_run_serialize_condition_has_ci():","+    \"\"\"Multi-run serialization includes ci_90 field.\"\"\"","+    reporter = Reporter(output_dir=\"/tmp\")","+","+    runs = [","+        TaskResult(","+            task_id=\"fix-ci\", condition=Condition.NONE, success=True,","+            test_output=\"PASS\", wall_clock_seconds=100.0,","+            input_tokens=5000, output_tokens=2000, tool_calls=20,","+            lines_changed=50, files_touched=[\"a.py\"],","+        ),","+        TaskResult(","+            task_id=\"fix-ci\", condition=Condition.NONE, success=False,","+            test_output=\"FAIL\", wall_clock_seconds=120.0,","+            input_tokens=6000, output_tokens=2500, tool_calls=25,","+            lines_changed=60, files_touched=[\"a.py\"],","+        ),","+        TaskResult(","+            task_id=\"fix-ci\", condition=Condition.NONE, success=True,","+            test_output=\"PASS\", wall_clock_seconds=90.0,","+            input_tokens=4500, output_tokens=1800, tool_calls=18,","+            lines_changed=45, files_touched=[\"a.py\"],","+        ),","+    ]","+","+    result = reporter._serialize_condition(runs)","+","+    assert \"ci_90\" in result","+    ci = result[\"ci_90\"]","+    assert \"lower\" in ci","+    assert \"upper\" in ci","+    assert 0 <= ci[\"lower\"] <= ci[\"upper\"] <= 1","+    # 2/3 successes: lower bound should be > 0, upper < 1","+    assert ci[\"lower\"] > 0","+    assert ci[\"upper\"] < 1","+","+","+def test_single_run_has_no_ci():","+    \"\"\"Single-run serialization does not add CI (backward-compatible).\"\"\"","+    reporter = Reporter(output_dir=\"/tmp\")","+","+    run = TaskResult(","+        task_id=\"fix-single\", condition=Condition.NONE, success=True,","+        test_output=\"PASS\", wall_clock_seconds=100.0,","+        input_tokens=5000, output_tokens=2000, tool_calls=20,","+        lines_changed=50, files_touched=[\"a.py\"],","+    )","+","+    result = reporter._serialize_condition([run])","+    assert \"ci_90\" not in result","+","+","+def test_summary_has_cis_for_multi_run():","+    \"\"\"Summary includes per-condition CIs and significance flags for multi-run.\"\"\"","+    reporter = Reporter(output_dir=\"/tmp\")","+    results = _make_multi_run_results()","+    eval_results = reporter.compile_results(results)","+    summary = eval_results.summary","+","+    # CIs present","+    assert \"none_ci_90\" in summary","+    assert \"intent_layer_ci_90\" in summary","+","+    none_ci = summary[\"none_ci_90\"]","+    il_ci = summary[\"intent_layer_ci_90\"]","+","+    # Bounds are valid","+    assert 0 <= none_ci[\"lower\"] <= none_ci[\"upper\"] <= 1","+    assert 0 <= il_ci[\"lower\"] <= il_ci[\"upper\"] <= 1","+","+    # none: 2/6 pass → lower rate; IL: 6/6 → higher rate","+    assert none_ci[\"upper\"] < il_ci[\"upper\"]","+","+    # Significance flag present","+    assert \"intent_layer_vs_none_significant\" in summary","+","+","+def test_summary_no_cis_for_single_run(three_condition_results):","+    \"\"\"Summary omits CIs when all results are single-run.\"\"\"","+    reporter = Reporter(output_dir=\"/tmp\")","+    eval_results = reporter.compile_results(three_condition_results)","+    summary = eval_results.summary","+","+    assert \"none_ci_90\" not in summary","+    assert \"intent_layer_ci_90\" not in summary","+    assert \"intent_layer_vs_none_significant\" not in summary","+","+","+def test_markdown_multi_run_has_ci_columns(tmp_path):","+    \"\"\"Markdown output includes CI brackets and IL vs none column for multi-run.\"\"\"","+    reporter = Reporter(output_dir=str(tmp_path))","+    results = _make_multi_run_results()","+    eval_results = reporter.compile_results(results)","+    md_path = reporter.write_markdown(eval_results)","+","+    with open(md_path) as f:","+        content = f.read()","+","+    # CI brackets appear in success column","+    assert \"[\" in content and \"]\" in content","+","+    # IL vs none column header present","+    assert \"IL vs none\" in content","+","+    # Significance labels appear","+    assert \"overlap\" in content or \"sig.\" in content","+","+    # Summary has CI notation","+    assert \"90% CI\" in content","+","+","+def test_markdown_single_run_no_ci_column(tmp_path, three_condition_results):","+    \"\"\"Single-run markdown has no IL vs none column (backward-compatible).\"\"\"","+    reporter = Reporter(output_dir=str(tmp_path))","+    eval_results = reporter.compile_results(three_condition_results)","+    md_path = reporter.write_markdown(eval_results)","+","+    with open(md_path) as f:","+        content = f.read()","+","+    assert \"IL vs none\" not in content","+    assert \"90% CI\" not in content","+","+","+def test_json_multi_run_has_ci(tmp_path):","+    \"\"\"JSON output includes ci_90 in multi-run condition data.\"\"\"","+    reporter = Reporter(output_dir=str(tmp_path))","+    results = _make_multi_run_results()","+    eval_results = reporter.compile_results(results)","+    json_path = reporter.write_json(eval_results)","+","+    import json","+    with open(json_path) as f:","+        data = json.load(f)","+","+    task = data[\"results\"][0]","+    assert \"ci_90\" in task[\"none\"]","+    assert \"ci_90\" in task[\"intent_layer\"]","+","+    # Summary has CIs too","+    assert \"none_ci_90\" in data[\"summary\"]","+    assert \"intent_layer_ci_90\" in data[\"summary\"]"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"72e62d57-59fa-413e-b3af-e102817049d1"}
{"parentUuid":"3adfe52f-4465-46aa-8b57-cc6d0dc81ff6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01S8KK9xkhhrvRzetGSDJpAg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01R7C7d1cVxqrQTr37taUZiy","name":"Bash","input":{"command":"python -m pytest tests/test_stats.py tests/test_reporter.py -v 2>&1","description":"Run all stats and reporter tests"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":3632,"cache_read_input_tokens":103397,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3632},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF5yikpcHhx4VgGf6n6","type":"assistant","uuid":"229a4056-7ce6-438f-b56f-d6f197046430","timestamp":"2026-02-17T20:06:42.316Z"}
{"parentUuid":"229a4056-7ce6-438f-b56f-d6f197046430","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","parentToolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","uuid":"90929cda-c20d-4d1f-8865-463bbc6356ce","timestamp":"2026-02-17T20:06:42.318Z"}
{"parentUuid":"90929cda-c20d-4d1f-8865-463bbc6356ce","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","parentToolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","uuid":"3f18f41e-f7e5-4cf7-94a6-ac29b70174e1","timestamp":"2026-02-17T20:06:42.318Z"}
{"parentUuid":"3f18f41e-f7e5-4cf7-94a6-ac29b70174e1","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","toolUseID":"toolu_01R7C7d1cVxqrQTr37taUZiy","timestamp":"2026-02-17T20:06:43.100Z","uuid":"0180f401-61cf-455b-83fa-b3dd946dc613"}
{"parentUuid":"229a4056-7ce6-438f-b56f-d6f197046430","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01R7C7d1cVxqrQTr37taUZiy","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 41 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  2%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [  4%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [  7%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [  9%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 12%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 14%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 17%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 19%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 21%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 24%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 29%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 31%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 34%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 36%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 39%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 41%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 43%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [ 46%]\ntests/test_reporter.py::test_three_condition_compilation PASSED          [ 48%]\ntests/test_reporter.py::test_deltas_relative_to_none PASSED              [ 51%]\ntests/test_reporter.py::test_zero_baseline_delta PASSED                  [ 53%]\ntests/test_reporter.py::test_missing_condition PASSED                    [ 56%]\ntests/test_reporter.py::test_summary_three_success_rates PASSED          [ 58%]\ntests/test_reporter.py::test_markdown_multi_row_layout PASSED            [ 60%]\ntests/test_reporter.py::test_json_output PASSED                          [ 63%]\ntests/test_reporter.py::test_infrastructure_errors_excluded_from_success_rate PASSED [ 65%]\ntests/test_reporter.py::test_pre_validation_and_skill_gen_errors_excluded PASSED [ 68%]\ntests/test_reporter.py::test_multi_run_serialize_condition PASSED        [ 70%]\ntests/test_reporter.py::test_multi_run_delta PASSED                      [ 73%]\ntests/test_reporter.py::test_get_fix_metrics_all_formats PASSED          [ 75%]\ntests/test_reporter.py::test_serialize_includes_exit_code_and_timeout PASSED [ 78%]\ntests/test_reporter.py::test_serialize_omits_exit_code_when_none PASSED  [ 80%]\ntests/test_reporter.py::test_single_run_delta_uses_success_rate_delta_key PASSED [ 82%]\ntests/test_reporter.py::test_multi_run_serialize_condition_has_ci PASSED [ 85%]\ntests/test_reporter.py::test_single_run_has_no_ci PASSED                 [ 87%]\ntests/test_reporter.py::test_summary_has_cis_for_multi_run PASSED        [ 90%]\ntests/test_reporter.py::test_summary_no_cis_for_single_run PASSED        [ 92%]\ntests/test_reporter.py::test_markdown_multi_run_has_ci_columns PASSED    [ 95%]\ntests/test_reporter.py::test_markdown_single_run_no_ci_column PASSED     [ 97%]\ntests/test_reporter.py::test_json_multi_run_has_ci PASSED                [100%]\n\n============================== 41 passed in 0.10s ==============================","is_error":false}]},"uuid":"1d9cc687-e809-4afe-ae56-1dddd932e3cf","timestamp":"2026-02-17T20:06:43.099Z","toolUseResult":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.13.11, pytest-9.0.2, pluggy-1.5.0 -- /opt/homebrew/Caskroom/miniconda/base/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/ryo/dev/intent-layer/eval-harness\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, httpx-0.36.0, timeout-2.4.0, flakefinder-1.1.0, retry-1.7.0, asyncio-1.3.0, inline-snapshot-0.32.0, cov-7.0.0\nasyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 41 items\n\ntests/test_stats.py::TestInverseNormalCDF::test_median PASSED            [  2%]\ntests/test_stats.py::TestInverseNormalCDF::test_known_quantiles PASSED   [  4%]\ntests/test_stats.py::TestInverseNormalCDF::test_symmetry PASSED          [  7%]\ntests/test_stats.py::TestInverseNormalCDF::test_boundary_errors PASSED   [  9%]\ntests/test_stats.py::TestInverseNormalCDF::test_tail_regions PASSED      [ 12%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_successes PASSED [ 14%]\ntests/test_stats.py::TestWilsonScoreInterval::test_all_successes PASSED  [ 17%]\ntests/test_stats.py::TestWilsonScoreInterval::test_half_successes PASSED [ 19%]\ntests/test_stats.py::TestWilsonScoreInterval::test_zero_trials PASSED    [ 21%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_pass PASSED [ 24%]\ntests/test_stats.py::TestWilsonScoreInterval::test_single_trial_fail PASSED [ 26%]\ntests/test_stats.py::TestWilsonScoreInterval::test_our_eval_data PASSED  [ 29%]\ntests/test_stats.py::TestWilsonScoreInterval::test_higher_confidence_widens_interval PASSED [ 31%]\ntests/test_stats.py::TestWilsonScoreInterval::test_more_data_narrows_interval PASSED [ 34%]\ntests/test_stats.py::TestCIOverlap::test_overlapping PASSED              [ 36%]\ntests/test_stats.py::TestCIOverlap::test_non_overlapping PASSED          [ 39%]\ntests/test_stats.py::TestCIOverlap::test_touching PASSED                 [ 41%]\ntests/test_stats.py::TestCIOverlap::test_contained PASSED                [ 43%]\ntests/test_stats.py::TestCIOverlap::test_identical PASSED                [ 46%]\ntests/test_reporter.py::test_three_condition_compilation PASSED          [ 48%]\ntests/test_reporter.py::test_deltas_relative_to_none PASSED              [ 51%]\ntests/test_reporter.py::test_zero_baseline_delta PASSED                  [ 53%]\ntests/test_reporter.py::test_missing_condition PASSED                    [ 56%]\ntests/test_reporter.py::test_summary_three_success_rates PASSED          [ 58%]\ntests/test_reporter.py::test_markdown_multi_row_layout PASSED            [ 60%]\ntests/test_reporter.py::test_json_output PASSED                          [ 63%]\ntests/test_reporter.py::test_infrastructure_errors_excluded_from_success_rate PASSED [ 65%]\ntests/test_reporter.py::test_pre_validation_and_skill_gen_errors_excluded PASSED [ 68%]\ntests/test_reporter.py::test_multi_run_serialize_condition PASSED        [ 70%]\ntests/test_reporter.py::test_multi_run_delta PASSED                      [ 73%]\ntests/test_reporter.py::test_get_fix_metrics_all_formats PASSED          [ 75%]\ntests/test_reporter.py::test_serialize_includes_exit_code_and_timeout PASSED [ 78%]\ntests/test_reporter.py::test_serialize_omits_exit_code_when_none PASSED  [ 80%]\ntests/test_reporter.py::test_single_run_delta_uses_success_rate_delta_key PASSED [ 82%]\ntests/test_reporter.py::test_multi_run_serialize_condition_has_ci PASSED [ 85%]\ntests/test_reporter.py::test_single_run_has_no_ci PASSED                 [ 87%]\ntests/test_reporter.py::test_summary_has_cis_for_multi_run PASSED        [ 90%]\ntests/test_reporter.py::test_summary_no_cis_for_single_run PASSED        [ 92%]\ntests/test_reporter.py::test_markdown_multi_run_has_ci_columns PASSED    [ 95%]\ntests/test_reporter.py::test_markdown_single_run_no_ci_column PASSED     [ 97%]\ntests/test_reporter.py::test_json_multi_run_has_ci PASSED                [100%]\n\n============================== 41 passed in 0.10s ==============================","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"229a4056-7ce6-438f-b56f-d6f197046430"}
{"parentUuid":"1d9cc687-e809-4afe-ae56-1dddd932e3cf","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_0123w5cH2wwqMGDSBr4X8juE","type":"message","role":"assistant","content":[{"type":"text","text":"All 41 tests pass. Let me do a quick sanity check on the generated markdown output to make sure it reads well:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1537,"cache_read_input_tokens":107029,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1537},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF6M1foqRgUvniaw4Ph","type":"assistant","uuid":"49ed4d3b-20c0-417d-b3d0-2ff7478573f6","timestamp":"2026-02-17T20:06:46.395Z"}
{"parentUuid":"49ed4d3b-20c0-417d-b3d0-2ff7478573f6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_0123w5cH2wwqMGDSBr4X8juE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","name":"Bash","input":{"command":"python -c \"\nfrom lib.reporter import Reporter\nfrom lib.task_runner import TaskResult, Condition\n\n# Simulate a multi-run eval with 2 tasks, 3 reps each, 2 conditions\nresults = []\nfor task_id in ('fastmcp/fix-123', 'fastmcp/fix-456'):\n    for i, success in enumerate([True, False, True]):\n        results.append(TaskResult(\n            task_id=task_id, condition=Condition.NONE, success=success,\n            test_output='PASS' if success else 'FAIL',\n            wall_clock_seconds=100.0, input_tokens=5000, output_tokens=2000,\n            tool_calls=20, lines_changed=50, files_touched=['a.py'],\n        ))\n    for i, success in enumerate([True, True, True]):\n        results.append(TaskResult(\n            task_id=task_id, condition=Condition.INTENT_LAYER, success=success,\n            test_output='PASS', wall_clock_seconds=60.0,\n            input_tokens=3000, output_tokens=1000, tool_calls=10,\n            lines_changed=25, files_touched=['a.py'],\n        ))\n\nimport tempfile\nreporter = Reporter(output_dir=tempfile.mkdtemp())\neval_results = reporter.compile_results(results)\nmd_path = reporter.write_markdown(eval_results)\nwith open(md_path) as f:\n    print(f.read())\n\" 2>&1","description":"Preview multi-run markdown output with CIs"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1537,"cache_read_input_tokens":107029,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1537},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF6M1foqRgUvniaw4Ph","type":"assistant","uuid":"6997de09-39b0-48be-8a1f-e44152917b1c","timestamp":"2026-02-17T20:06:52.320Z"}
{"parentUuid":"6997de09-39b0-48be-8a1f-e44152917b1c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","parentToolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","uuid":"5152eddd-778c-45c9-aebf-b01ab4efde3f","timestamp":"2026-02-17T20:06:52.324Z"}
{"parentUuid":"5152eddd-778c-45c9-aebf-b01ab4efde3f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","parentToolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","uuid":"f4ba5737-bff7-4305-bffa-d2f701c1b9e1","timestamp":"2026-02-17T20:06:52.324Z"}
{"parentUuid":"f4ba5737-bff7-4305-bffa-d2f701c1b9e1","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","toolUseID":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","timestamp":"2026-02-17T20:06:55.833Z","uuid":"82d642c3-d1c2-43cc-9941-add534408028"}
{"parentUuid":"6997de09-39b0-48be-8a1f-e44152917b1c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XmSwjMmT6Sy7k2KbxfokM2","type":"tool_result","content":"# Eval Results: 2026-02-17-120655\n\n**Timestamp:** 2026-02-17T12:06:55Z\n\n## Summary\n\n- **Total tasks:** 2\n- **Infrastructure errors:** 0\n- **None success rate:** 67% 90% CI [35%,88%]\n- **Flat LLM success rate:** 0%\n- **Intent Layer success rate:** 100% 90% CI [69%,100%]\n\n- **Intent Layer vs None:** not significant (overlapping CIs)\n\n**CI widths (variance proxy):** none: 54%, intent_layer: 31%\n\n## Results\n\n| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | Δ Time | Δ Tokens | IL vs none |\n|------|-----------|---------|----------|--------|------------|-------|--------|----------|------------|\n| fastmcp/fix-123 | none | 2/3 67% [25%,92%] | 100.0 | 7.0k | 20 | 50 | — | — |  |\n| fastmcp/fix-123 | intent_layer | 3/3 100% [53%,100%] | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% | +33% (overlap) |\n|  |  |  |  |  |  |  |  |  |  |\n| fastmcp/fix-456 | none | 2/3 67% [25%,92%] | 100.0 | 7.0k | 20 | 50 | — | — |  |\n| fastmcp/fix-456 | intent_layer | 3/3 100% [53%,100%] | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% | +33% (overlap) |","is_error":false}]},"uuid":"77127fe2-b1dd-47d0-9bb6-af8d88a6b755","timestamp":"2026-02-17T20:06:55.832Z","toolUseResult":{"stdout":"# Eval Results: 2026-02-17-120655\n\n**Timestamp:** 2026-02-17T12:06:55Z\n\n## Summary\n\n- **Total tasks:** 2\n- **Infrastructure errors:** 0\n- **None success rate:** 67% 90% CI [35%,88%]\n- **Flat LLM success rate:** 0%\n- **Intent Layer success rate:** 100% 90% CI [69%,100%]\n\n- **Intent Layer vs None:** not significant (overlapping CIs)\n\n**CI widths (variance proxy):** none: 54%, intent_layer: 31%\n\n## Results\n\n| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | Δ Time | Δ Tokens | IL vs none |\n|------|-----------|---------|----------|--------|------------|-------|--------|----------|------------|\n| fastmcp/fix-123 | none | 2/3 67% [25%,92%] | 100.0 | 7.0k | 20 | 50 | — | — |  |\n| fastmcp/fix-123 | intent_layer | 3/3 100% [53%,100%] | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% | +33% (overlap) |\n|  |  |  |  |  |  |  |  |  |  |\n| fastmcp/fix-456 | none | 2/3 67% [25%,92%] | 100.0 | 7.0k | 20 | 50 | — | — |  |\n| fastmcp/fix-456 | intent_layer | 3/3 100% [53%,100%] | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% | +33% (overlap) |","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"6997de09-39b0-48be-8a1f-e44152917b1c"}
{"parentUuid":"77127fe2-b1dd-47d0-9bb6-af8d88a6b755","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HxQTkSCyxAwsz1MEczKne6","type":"message","role":"assistant","content":[{"type":"text","text":"Output looks clean and informative. Let me also verify backward compatibility by checking the single-run output hasn't changed:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":970,"cache_read_input_tokens":108566,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":970},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF7HNRWAQmJe2ETu55P","type":"assistant","uuid":"e4a7ab10-9a83-491c-bde1-e589ed959af6","timestamp":"2026-02-17T20:06:58.931Z"}
{"parentUuid":"e4a7ab10-9a83-491c-bde1-e589ed959af6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HxQTkSCyxAwsz1MEczKne6","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017mKHn7v8va4zPgRqLuPctU","name":"Bash","input":{"command":"python -c \"\nfrom lib.reporter import Reporter\nfrom lib.task_runner import TaskResult, Condition\n\n# Single-run (existing behavior)\nresults = [\n    TaskResult(task_id='fix-123', condition=Condition.NONE, success=False,\n        test_output='FAIL', wall_clock_seconds=100.0,\n        input_tokens=5000, output_tokens=2000, tool_calls=20,\n        lines_changed=50, files_touched=['a.py']),\n    TaskResult(task_id='fix-123', condition=Condition.INTENT_LAYER, success=True,\n        test_output='PASS', wall_clock_seconds=60.0,\n        input_tokens=3000, output_tokens=1000, tool_calls=10,\n        lines_changed=25, files_touched=['a.py']),\n]\n\nimport tempfile\nreporter = Reporter(output_dir=tempfile.mkdtemp())\neval_results = reporter.compile_results(results)\nmd_path = reporter.write_markdown(eval_results)\nwith open(md_path) as f:\n    print(f.read())\n\" 2>&1","description":"Preview single-run markdown (backward compat check)"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":970,"cache_read_input_tokens":108566,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":970},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF7HNRWAQmJe2ETu55P","type":"assistant","uuid":"b12021e5-6793-46ea-8590-4dfb56cd037e","timestamp":"2026-02-17T20:07:01.732Z"}
{"parentUuid":"b12021e5-6793-46ea-8590-4dfb56cd037e","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","parentToolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","uuid":"bb4d2152-1565-4999-adb5-8c04647d5d03","timestamp":"2026-02-17T20:07:01.734Z"}
{"parentUuid":"bb4d2152-1565-4999-adb5-8c04647d5d03","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","parentToolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","uuid":"9bd8a556-2ffa-4822-9b71-24220514f801","timestamp":"2026-02-17T20:07:01.734Z"}
{"parentUuid":"9bd8a556-2ffa-4822-9b71-24220514f801","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","toolUseID":"toolu_017mKHn7v8va4zPgRqLuPctU","timestamp":"2026-02-17T20:07:04.986Z","uuid":"6c09f0d6-b24b-4c67-839c-a7e45cbb79b6"}
{"parentUuid":"b12021e5-6793-46ea-8590-4dfb56cd037e","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017mKHn7v8va4zPgRqLuPctU","type":"tool_result","content":"# Eval Results: 2026-02-17-120704\n\n**Timestamp:** 2026-02-17T12:07:04Z\n\n## Summary\n\n- **Total tasks:** 1\n- **Infrastructure errors:** 0\n- **None success rate:** 0%\n- **Flat LLM success rate:** 0%\n- **Intent Layer success rate:** 100%\n\n## Results\n\n| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | Δ Time | Δ Tokens |\n|------|-----------|---------|----------|--------|------------|-------|--------|----------|\n| fix-123 | none | FAIL | 100.0 | 7.0k | 20 | 50 | — | — |\n| fix-123 | intent_layer | PASS | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% |","is_error":false}]},"uuid":"ea47744a-a440-4bce-95ea-a5c19ced59f3","timestamp":"2026-02-17T20:07:04.985Z","toolUseResult":{"stdout":"# Eval Results: 2026-02-17-120704\n\n**Timestamp:** 2026-02-17T12:07:04Z\n\n## Summary\n\n- **Total tasks:** 1\n- **Infrastructure errors:** 0\n- **None success rate:** 0%\n- **Flat LLM success rate:** 0%\n- **Intent Layer success rate:** 100%\n\n## Results\n\n| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | Δ Time | Δ Tokens |\n|------|-----------|---------|----------|--------|------------|-------|--------|----------|\n| fix-123 | none | FAIL | 100.0 | 7.0k | 20 | 50 | — | — |\n| fix-123 | intent_layer | PASS | 60.0 | 4.0k | 10 | 25 | -40.0% | -42.9% |","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"b12021e5-6793-46ea-8590-4dfb56cd037e"}
{"parentUuid":"ea47744a-a440-4bce-95ea-a5c19ced59f3","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HgvpYLq1QLQjob6D4dkw6A","type":"message","role":"assistant","content":[{"type":"text","text":"Single-run output is identical to the old format — no CIs, no extra column, PASS/FAIL labels. Backward compatibility is preserved.\n\nLet me verify the final state of the modified reporter file:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":732,"cache_read_input_tokens":109536,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":732},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF7xTfMu7op9AJg8WJ9","type":"assistant","uuid":"635d8660-726e-4a56-b97b-b07b3ff60a11","timestamp":"2026-02-17T20:07:08.609Z"}
{"parentUuid":"635d8660-726e-4a56-b97b-b07b3ff60a11","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01HgvpYLq1QLQjob6D4dkw6A","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Spqnuv5zrznwm241pq1nZo","name":"Read","input":{"file_path":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":732,"cache_read_input_tokens":109536,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":732},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF7xTfMu7op9AJg8WJ9","type":"assistant","uuid":"fbddadfb-3a3b-41a6-be14-542e4075d897","timestamp":"2026-02-17T20:07:09.128Z"}
{"parentUuid":"fbddadfb-3a3b-41a6-be14-542e4075d897","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","parentToolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","uuid":"5dfc054b-bef4-43f0-a353-b7c7a69dac55","timestamp":"2026-02-17T20:07:09.131Z"}
{"parentUuid":"5dfc054b-bef4-43f0-a353-b7c7a69dac55","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","toolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","timestamp":"2026-02-17T20:07:09.244Z","uuid":"44a71921-fbeb-4a90-bd61-f1988fdfd867"}
{"parentUuid":"44a71921-fbeb-4a90-bd61-f1988fdfd867","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","toolUseID":"toolu_01Spqnuv5zrznwm241pq1nZo","timestamp":"2026-02-17T20:07:09.244Z","uuid":"6675ab9d-d118-45a5-ab8a-b030ccd1b89c"}
{"parentUuid":"fbddadfb-3a3b-41a6-be14-542e4075d897","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Spqnuv5zrznwm241pq1nZo","type":"tool_result","content":"     1→# lib/reporter.py\n     2→from __future__ import annotations\n     3→import json\n     4→import statistics\n     5→from dataclasses import dataclass, asdict\n     6→from datetime import datetime\n     7→from pathlib import Path\n     8→from typing import Any\n     9→\n    10→from lib.task_runner import TaskResult, Condition\n    11→from lib.stats import wilson_score_interval, ci_overlap\n    12→\n    13→\n    14→@dataclass\n    15→class EvalResults:\n    16→    eval_id: str\n    17→    timestamp: str\n    18→    results: list[dict[str, Any]]\n    19→    summary: dict[str, Any]\n    20→\n    21→\n    22→class Reporter:\n    23→    def __init__(self, output_dir: str):\n    24→        self.output_dir = Path(output_dir)\n    25→        self.output_dir.mkdir(parents=True, exist_ok=True)\n    26→\n    27→    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n    28→        \"\"\"Compile task results into structured eval results.\n    29→\n    30→        When repetitions > 1, groups multiple runs of the same task+condition\n    31→        and reports medians for efficiency metrics alongside success rates.\n    32→        Individual runs are preserved in a \"runs\" array.\n    33→        \"\"\"\n    34→        # Group by (task_id, condition) — allows multiple runs per pair\n    35→        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n    36→        for r in results:\n    37→            if r.task_id not in grouped:\n    38→                grouped[r.task_id] = {}\n    39→            cond = r.condition.value\n    40→            if cond not in grouped[r.task_id]:\n    41→                grouped[r.task_id][cond] = []\n    42→            grouped[r.task_id][cond].append(r)\n    43→\n    44→        compiled = []\n    45→        for task_id, conditions in grouped.items():\n    46→            none_runs = conditions.get(\"none\", [])\n    47→            flat_runs = conditions.get(\"flat_llm\", [])\n    48→            il_runs = conditions.get(\"intent_layer\", [])\n    49→\n    50→            task_result = {\n    51→                \"task_id\": task_id,\n    52→                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n    53→                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n    54→                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n    55→                \"deltas\": {\n    56→                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n    57→                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n    58→                }\n    59→            }\n    60→            compiled.append(task_result)\n    61→\n    62→        summary = self._compute_summary(results)\n    63→\n    64→        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    65→        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n    66→\n    67→        return EvalResults(\n    68→            eval_id=eval_id,\n    69→            timestamp=timestamp,\n    70→            results=compiled,\n    71→            summary=summary\n    72→        )\n    73→\n    74→    def _serialize_single_result(self, r: TaskResult) -> dict:\n    75→        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n    76→        result = {\n    77→            \"success\": r.success,\n    78→            \"test_output\": r.test_output[:1000],  # Truncate\n    79→        }\n    80→\n    81→        if r.skill_generation:\n    82→            result[\"fix_only\"] = {\n    83→                \"wall_clock_seconds\": r.wall_clock_seconds,\n    84→                \"input_tokens\": r.input_tokens,\n    85→                \"output_tokens\": r.output_tokens,\n    86→                \"tool_calls\": r.tool_calls,\n    87→                \"lines_changed\": r.lines_changed,\n    88→                \"files_touched\": r.files_touched,\n    89→            }\n    90→            result[\"skill_generation\"] = {\n    91→                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n    92→                \"input_tokens\": r.skill_generation.input_tokens,\n    93→                \"output_tokens\": r.skill_generation.output_tokens,\n    94→                \"cache_hit\": r.skill_generation.cache_hit,\n    95→            }\n    96→            result[\"total\"] = {\n    97→                \"wall_clock_seconds\": (\n    98→                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n    99→                ),\n   100→                \"input_tokens\": (\n   101→                    r.input_tokens + r.skill_generation.input_tokens\n   102→                ),\n   103→                \"output_tokens\": (\n   104→                    r.output_tokens + r.skill_generation.output_tokens\n   105→                ),\n   106→            }\n   107→        else:\n   108→            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n   109→            result[\"input_tokens\"] = r.input_tokens\n   110→            result[\"output_tokens\"] = r.output_tokens\n   111→            result[\"tool_calls\"] = r.tool_calls\n   112→            result[\"lines_changed\"] = r.lines_changed\n   113→            result[\"files_touched\"] = r.files_touched\n   114→\n   115→        if r.error:\n   116→            result[\"error\"] = r.error\n   117→\n   118→        if r.exit_code is not None:\n   119→            result[\"exit_code\"] = r.exit_code\n   120→        if r.is_timeout:\n   121→            result[\"is_timeout\"] = r.is_timeout\n   122→\n   123→        return result\n   124→\n   125→    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n   126→        \"\"\"Serialize one or more runs for a single condition.\n   127→\n   128→        Single run: backward-compatible flat structure.\n   129→        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n   130→        success rate and efficiency metrics.\n   131→        \"\"\"\n   132→        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n   133→\n   134→        if len(runs) == 1:\n   135→            # Single run: backward-compatible\n   136→            return self._serialize_single_result(runs[0])\n   137→\n   138→        # Multiple runs: include individual results + aggregated medians\n   139→        result: dict[str, Any] = {}\n   140→\n   141→        # Success rate (primary metric — separated from efficiency)\n   142→        successes = sum(1 for r in valid_runs if r.success)\n   143→        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n   144→        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n   145→        result[\"successes\"] = successes\n   146→        result[\"total_valid_runs\"] = len(valid_runs)\n   147→\n   148→        # Wilson Score confidence interval (90%)\n   149→        ci_lower, ci_upper, ci_center = wilson_score_interval(\n   150→            successes, len(valid_runs), confidence=0.90\n   151→        )\n   152→        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n   153→\n   154→        # Efficiency medians (secondary metrics — from valid runs only)\n   155→        if valid_runs:\n   156→            result[\"median\"] = {\n   157→                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n   158→                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n   159→                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n   160→                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n   161→                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n   162→            }\n   163→\n   164→        # Individual runs for drill-down\n   165→        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n   166→\n   167→        return result\n   168→\n   169→    @staticmethod\n   170→    def _get_fix_metrics(cond_data: dict) -> dict:\n   171→        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n   172→\n   173→        Works with both single-run format and multi-run median format.\n   174→        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n   175→        tool_calls, lines_changed.\n   176→        \"\"\"\n   177→        # Multi-run: use median\n   178→        if \"median\" in cond_data:\n   179→            m = cond_data[\"median\"]\n   180→            return {\n   181→                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n   182→                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n   183→                \"tool_calls\": m[\"tool_calls\"],\n   184→                \"lines_changed\": m[\"lines_changed\"],\n   185→            }\n   186→        # Single run with skill_generation\n   187→        if \"fix_only\" in cond_data:\n   188→            fix = cond_data[\"fix_only\"]\n   189→            return {\n   190→                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n   191→                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n   192→                \"tool_calls\": fix[\"tool_calls\"],\n   193→                \"lines_changed\": fix[\"lines_changed\"],\n   194→            }\n   195→        # Single run without skill_generation\n   196→        return {\n   197→            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n   198→            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n   199→            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n   200→            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n   201→        }\n   202→\n   203→    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n   204→        \"\"\"Compute delta between baseline and treatment using median metrics.\n   205→\n   206→        For single runs, behaves identically to the old _compute_single_delta.\n   207→        For multiple runs, uses medians for efficiency and success rate delta.\n   208→        \"\"\"\n   209→        if not baseline_runs or not treatment_runs:\n   210→            return {}\n   211→\n   212→        # Filter out infra errors for stats\n   213→        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n   214→        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n   215→\n   216→        if not b_valid or not t_valid:\n   217→            return {}\n   218→\n   219→        # Success rate delta\n   220→        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n   221→        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n   222→        rate_delta = t_rate - b_rate\n   223→\n   224→        # Median efficiency metrics\n   225→        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n   226→        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n   227→        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n   228→        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n   229→        b_tools = statistics.median(r.tool_calls for r in b_valid)\n   230→        t_tools = statistics.median(r.tool_calls for r in t_valid)\n   231→        b_lines = statistics.median(r.lines_changed for r in b_valid)\n   232→        t_lines = statistics.median(r.lines_changed for r in t_valid)\n   233→\n   234→        def pct(baseline_val, treatment_val):\n   235→            if baseline_val:\n   236→                return (treatment_val - baseline_val) / baseline_val * 100\n   237→            return 0\n   238→\n   239→        return {\n   240→            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n   241→                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n   242→            ),\n   243→            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n   244→            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n   245→            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n   246→            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n   247→        }\n   248→\n   249→    @staticmethod\n   250→    def _is_infra_error(r: TaskResult) -> bool:\n   251→        \"\"\"Check if result is a non-experimental error (excluded from stats).\n   252→\n   253→        Infrastructure errors, pre-validation failures, and skill generation\n   254→        failures all indicate harness problems, not experimental outcomes.\n   255→        \"\"\"\n   256→        if r.error is None:\n   257→            return False\n   258→        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n   259→\n   260→    def _compute_summary(self, results: list[TaskResult]) -> dict:\n   261→        \"\"\"Compute overall summary statistics.\n   262→\n   263→        Infrastructure errors (clone/docker/workspace failures) are excluded\n   264→        from success rate calculations to avoid corrupting experimental data.\n   265→        \"\"\"\n   266→        def success_rate(task_results: list[TaskResult]) -> float:\n   267→            valid = [r for r in task_results if not self._is_infra_error(r)]\n   268→            if not valid:\n   269→                return 0\n   270→            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n   271→\n   272→        none_results = [r for r in results if r.condition == Condition.NONE]\n   273→        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n   274→        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n   275→\n   276→        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n   277→\n   278→        summary: dict[str, Any] = {\n   279→            \"total_tasks\": len(set(r.task_id for r in results)),\n   280→            \"infrastructure_errors\": infra_errors,\n   281→            \"none_success_rate\": success_rate(none_results),\n   282→            \"flat_llm_success_rate\": success_rate(flat_results),\n   283→            \"intent_layer_success_rate\": success_rate(il_results),\n   284→        }\n   285→\n   286→        # Add CIs when we have multi-run data\n   287→        has_multi_run = any(\n   288→            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n   289→            for r in results\n   290→        )\n   291→        if has_multi_run:\n   292→            for label, cond_results in [\n   293→                (\"none\", none_results),\n   294→                (\"flat_llm\", flat_results),\n   295→                (\"intent_layer\", il_results),\n   296→            ]:\n   297→                valid = [r for r in cond_results if not self._is_infra_error(r)]\n   298→                if valid:\n   299→                    successes = sum(1 for r in valid if r.success)\n   300→                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n   301→                    summary[f\"{label}_ci_90\"] = {\n   302→                        \"lower\": round(ci_lower, 3),\n   303→                        \"upper\": round(ci_upper, 3),\n   304→                    }\n   305→\n   306→            # Significance: check CI overlap between none and each treatment\n   307→            none_ci = summary.get(\"none_ci_90\")\n   308→            if none_ci:\n   309→                for treatment in (\"flat_llm\", \"intent_layer\"):\n   310→                    t_ci = summary.get(f\"{treatment}_ci_90\")\n   311→                    if t_ci:\n   312→                        overlaps = ci_overlap(\n   313→                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n   314→                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n   315→                        )\n   316→                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n   317→\n   318→        return summary\n   319→\n   320→    def write_json(self, results: EvalResults) -> str:\n   321→        \"\"\"Write results to JSON file.\"\"\"\n   322→        path = self.output_dir / f\"{results.eval_id}.json\"\n   323→        with open(path, \"w\") as f:\n   324→            json.dump(asdict(results), f, indent=2)\n   325→        return str(path)\n   326→\n   327→    @staticmethod\n   328→    def _format_ci(ci: dict) -> str:\n   329→        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n   330→        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n   331→\n   332→    def write_markdown(self, results: EvalResults) -> str:\n   333→        \"\"\"Write results to Markdown file.\n   334→\n   335→        Handles both single-run (backward-compatible) and multi-run formats.\n   336→        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n   337→        When multi-run data includes CIs, they're shown inline with pass rates.\n   338→        \"\"\"\n   339→        path = self.output_dir / f\"{results.eval_id}.md\"\n   340→        summary = results.summary\n   341→        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n   342→\n   343→        lines = [\n   344→            f\"# Eval Results: {results.eval_id}\",\n   345→            \"\",\n   346→            f\"**Timestamp:** {results.timestamp}\",\n   347→            \"\",\n   348→            \"## Summary\",\n   349→            \"\",\n   350→            f\"- **Total tasks:** {summary['total_tasks']}\",\n   351→            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n   352→        ]\n   353→\n   354→        # Per-condition summary with optional CIs\n   355→        for label, display_name in [\n   356→            (\"none\", \"None\"),\n   357→            (\"flat_llm\", \"Flat LLM\"),\n   358→            (\"intent_layer\", \"Intent Layer\"),\n   359→        ]:\n   360→            rate = summary[f\"{label}_success_rate\"]\n   361→            ci = summary.get(f\"{label}_ci_90\")\n   362→            if ci:\n   363→                lines.append(\n   364→                    f\"- **{display_name} success rate:** {rate:.0%} \"\n   365→                    f\"90% CI {self._format_ci(ci)}\"\n   366→                )\n   367→            else:\n   368→                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n   369→\n   370→        # Significance flags\n   371→        if has_cis:\n   372→            lines.append(\"\")\n   373→            for treatment, display_name in [\n   374→                (\"flat_llm\", \"Flat LLM\"),\n   375→                (\"intent_layer\", \"Intent Layer\"),\n   376→            ]:\n   377→                sig_key = f\"{treatment}_vs_none_significant\"\n   378→                if sig_key in summary:\n   379→                    if summary[sig_key]:\n   380→                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n   381→                    else:\n   382→                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n   383→\n   384→            # CI width as variance proxy\n   385→            widths = []\n   386→            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n   387→                ci = summary.get(f\"{label}_ci_90\")\n   388→                if ci:\n   389→                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n   390→            if widths:\n   391→                lines.append(\"\")\n   392→                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n   393→                    f\"{label}: {w:.0%}\" for label, w in widths\n   394→                ))\n   395→\n   396→        lines += [\n   397→            \"\",\n   398→            \"## Results\",\n   399→            \"\",\n   400→        ]\n   401→\n   402→        # Table header — add IL vs none column when multi-run CIs exist\n   403→        if has_cis:\n   404→            lines.append(\n   405→                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n   406→                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n   407→            )\n   408→            lines.append(\n   409→                \"|------|-----------|---------|----------|--------|------------|-------\"\n   410→                \"|--------|----------|------------|\"\n   411→            )\n   412→        else:\n   413→            lines.append(\n   414→                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n   415→                \"| \\u0394 Time | \\u0394 Tokens |\"\n   416→            )\n   417→            lines.append(\n   418→                \"|------|-----------|---------|----------|--------|------------|-------\"\n   419→                \"|--------|----------|\"\n   420→            )\n   421→\n   422→        for r in results.results:\n   423→            task_id = r[\"task_id\"]\n   424→            deltas = r.get(\"deltas\", {})\n   425→\n   426→            # Pre-compute per-task CI comparison for IL vs none\n   427→            none_data = r.get(\"none\")\n   428→            il_data = r.get(\"intent_layer\")\n   429→            il_vs_none = \"\"\n   430→            if has_cis and none_data and il_data:\n   431→                none_ci = none_data.get(\"ci_90\")\n   432→                il_ci = il_data.get(\"ci_90\")\n   433→                if none_ci and il_ci:\n   434→                    none_rate = none_data.get(\"success_rate\", 0)\n   435→                    il_rate = il_data.get(\"success_rate\", 0)\n   436→                    diff = il_rate - none_rate\n   437→                    overlaps = ci_overlap(\n   438→                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n   439→                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n   440→                    )\n   441→                    sig_label = \"overlap\" if overlaps else \"sig.\"\n   442→                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n   443→\n   444→            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n   445→                cond_data = r.get(cond_key)\n   446→                if cond_data is None:\n   447→                    continue\n   448→\n   449→                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n   450→                if \"success_rate\" in cond_data:\n   451→                    rate = cond_data[\"success_rate\"]\n   452→                    ci = cond_data.get(\"ci_90\")\n   453→                    if ci:\n   454→                        success = (\n   455→                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n   456→                            f\"{rate:.0%} {self._format_ci(ci)}\"\n   457→                        )\n   458→                    else:\n   459→                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n   460→                else:\n   461→                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n   462→\n   463→                # Extract efficiency metrics via unified helper\n   464→                metrics = self._get_fix_metrics(cond_data)\n   465→                time_s = metrics[\"wall_clock_seconds\"]\n   466→                tokens = metrics[\"tokens\"]\n   467→                tool_calls = metrics[\"tool_calls\"]\n   468→                lines_changed = metrics[\"lines_changed\"]\n   469→\n   470→                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n   471→\n   472→                # Deltas: none is baseline, shows \"—\"\n   473→                if cond_key == \"none\":\n   474→                    d_time = \"\\u2014\"\n   475→                    d_tokens = \"\\u2014\"\n   476→                else:\n   477→                    delta = deltas.get(cond_key, {})\n   478→                    d_time = delta.get(\"time_percent\", \"N/A\")\n   479→                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n   480→\n   481→                row = (\n   482→                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n   483→                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n   484→                    f\"{d_time} | {d_tokens}\"\n   485→                )\n   486→\n   487→                if has_cis:\n   488→                    # Show IL vs none comparison on the intent_layer row\n   489→                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n   490→                    row += f\" | {comparison} |\"\n   491→                else:\n   492→                    row += \" |\"\n   493→\n   494→                lines.append(row)\n   495→\n   496→            # Blank row between tasks\n   497→            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n   498→            lines.append(blank)\n   499→\n   500→        # Remove trailing blank row\n   501→        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n   502→            lines.pop()\n   503→\n   504→        with open(path, \"w\") as f:\n   505→            f.write(\"\\n\".join(lines))\n   506→\n   507→        return str(path)\n   508→"}]},"uuid":"51bf2a2c-69a1-46ba-a9cd-06af89704db3","timestamp":"2026-02-17T20:07:09.243Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/ryo/dev/intent-layer/eval-harness/lib/reporter.py","content":"# lib/reporter.py\nfrom __future__ import annotations\nimport json\nimport statistics\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom lib.task_runner import TaskResult, Condition\nfrom lib.stats import wilson_score_interval, ci_overlap\n\n\n@dataclass\nclass EvalResults:\n    eval_id: str\n    timestamp: str\n    results: list[dict[str, Any]]\n    summary: dict[str, Any]\n\n\nclass Reporter:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def compile_results(self, results: list[TaskResult]) -> EvalResults:\n        \"\"\"Compile task results into structured eval results.\n\n        When repetitions > 1, groups multiple runs of the same task+condition\n        and reports medians for efficiency metrics alongside success rates.\n        Individual runs are preserved in a \"runs\" array.\n        \"\"\"\n        # Group by (task_id, condition) — allows multiple runs per pair\n        grouped: dict[str, dict[str, list[TaskResult]]] = {}\n        for r in results:\n            if r.task_id not in grouped:\n                grouped[r.task_id] = {}\n            cond = r.condition.value\n            if cond not in grouped[r.task_id]:\n                grouped[r.task_id][cond] = []\n            grouped[r.task_id][cond].append(r)\n\n        compiled = []\n        for task_id, conditions in grouped.items():\n            none_runs = conditions.get(\"none\", [])\n            flat_runs = conditions.get(\"flat_llm\", [])\n            il_runs = conditions.get(\"intent_layer\", [])\n\n            task_result = {\n                \"task_id\": task_id,\n                \"none\": self._serialize_condition(none_runs) if none_runs else None,\n                \"flat_llm\": self._serialize_condition(flat_runs) if flat_runs else None,\n                \"intent_layer\": self._serialize_condition(il_runs) if il_runs else None,\n                \"deltas\": {\n                    \"flat_llm\": self._compute_delta(none_runs, flat_runs),\n                    \"intent_layer\": self._compute_delta(none_runs, il_runs),\n                }\n            }\n            compiled.append(task_result)\n\n        summary = self._compute_summary(results)\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        eval_id = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n\n        return EvalResults(\n            eval_id=eval_id,\n            timestamp=timestamp,\n            results=compiled,\n            summary=summary\n        )\n\n    def _serialize_single_result(self, r: TaskResult) -> dict:\n        \"\"\"Serialize a single TaskResult to dict.\"\"\"\n        result = {\n            \"success\": r.success,\n            \"test_output\": r.test_output[:1000],  # Truncate\n        }\n\n        if r.skill_generation:\n            result[\"fix_only\"] = {\n                \"wall_clock_seconds\": r.wall_clock_seconds,\n                \"input_tokens\": r.input_tokens,\n                \"output_tokens\": r.output_tokens,\n                \"tool_calls\": r.tool_calls,\n                \"lines_changed\": r.lines_changed,\n                \"files_touched\": r.files_touched,\n            }\n            result[\"skill_generation\"] = {\n                \"wall_clock_seconds\": r.skill_generation.wall_clock_seconds,\n                \"input_tokens\": r.skill_generation.input_tokens,\n                \"output_tokens\": r.skill_generation.output_tokens,\n                \"cache_hit\": r.skill_generation.cache_hit,\n            }\n            result[\"total\"] = {\n                \"wall_clock_seconds\": (\n                    r.wall_clock_seconds + r.skill_generation.wall_clock_seconds\n                ),\n                \"input_tokens\": (\n                    r.input_tokens + r.skill_generation.input_tokens\n                ),\n                \"output_tokens\": (\n                    r.output_tokens + r.skill_generation.output_tokens\n                ),\n            }\n        else:\n            result[\"wall_clock_seconds\"] = r.wall_clock_seconds\n            result[\"input_tokens\"] = r.input_tokens\n            result[\"output_tokens\"] = r.output_tokens\n            result[\"tool_calls\"] = r.tool_calls\n            result[\"lines_changed\"] = r.lines_changed\n            result[\"files_touched\"] = r.files_touched\n\n        if r.error:\n            result[\"error\"] = r.error\n\n        if r.exit_code is not None:\n            result[\"exit_code\"] = r.exit_code\n        if r.is_timeout:\n            result[\"is_timeout\"] = r.is_timeout\n\n        return result\n\n    def _serialize_condition(self, runs: list[TaskResult]) -> dict:\n        \"\"\"Serialize one or more runs for a single condition.\n\n        Single run: backward-compatible flat structure.\n        Multiple runs: adds \"runs\" array + \"median\" summary with separated\n        success rate and efficiency metrics.\n        \"\"\"\n        valid_runs = [r for r in runs if not self._is_infra_error(r)]\n\n        if len(runs) == 1:\n            # Single run: backward-compatible\n            return self._serialize_single_result(runs[0])\n\n        # Multiple runs: include individual results + aggregated medians\n        result: dict[str, Any] = {}\n\n        # Success rate (primary metric — separated from efficiency)\n        successes = sum(1 for r in valid_runs if r.success)\n        result[\"success_rate\"] = round(successes / len(valid_runs), 2) if valid_runs else 0\n        result[\"success\"] = result[\"success_rate\"] >= 0.5  # majority pass\n        result[\"successes\"] = successes\n        result[\"total_valid_runs\"] = len(valid_runs)\n\n        # Wilson Score confidence interval (90%)\n        ci_lower, ci_upper, ci_center = wilson_score_interval(\n            successes, len(valid_runs), confidence=0.90\n        )\n        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n\n        # Efficiency medians (secondary metrics — from valid runs only)\n        if valid_runs:\n            result[\"median\"] = {\n                \"wall_clock_seconds\": round(statistics.median(r.wall_clock_seconds for r in valid_runs), 1),\n                \"input_tokens\": int(statistics.median(r.input_tokens for r in valid_runs)),\n                \"output_tokens\": int(statistics.median(r.output_tokens for r in valid_runs)),\n                \"tool_calls\": int(statistics.median(r.tool_calls for r in valid_runs)),\n                \"lines_changed\": int(statistics.median(r.lines_changed for r in valid_runs)),\n            }\n\n        # Individual runs for drill-down\n        result[\"runs\"] = [self._serialize_single_result(r) for r in runs]\n\n        return result\n\n    @staticmethod\n    def _get_fix_metrics(cond_data: dict) -> dict:\n        \"\"\"Extract fix-only metrics from a condition result (single or multi-run).\n\n        Works with both single-run format and multi-run median format.\n        Returns dict with wall_clock_seconds, input_tokens, output_tokens,\n        tool_calls, lines_changed.\n        \"\"\"\n        # Multi-run: use median\n        if \"median\" in cond_data:\n            m = cond_data[\"median\"]\n            return {\n                \"wall_clock_seconds\": m[\"wall_clock_seconds\"],\n                \"tokens\": m[\"input_tokens\"] + m[\"output_tokens\"],\n                \"tool_calls\": m[\"tool_calls\"],\n                \"lines_changed\": m[\"lines_changed\"],\n            }\n        # Single run with skill_generation\n        if \"fix_only\" in cond_data:\n            fix = cond_data[\"fix_only\"]\n            return {\n                \"wall_clock_seconds\": fix[\"wall_clock_seconds\"],\n                \"tokens\": fix[\"input_tokens\"] + fix[\"output_tokens\"],\n                \"tool_calls\": fix[\"tool_calls\"],\n                \"lines_changed\": fix[\"lines_changed\"],\n            }\n        # Single run without skill_generation\n        return {\n            \"wall_clock_seconds\": cond_data.get(\"wall_clock_seconds\", 0),\n            \"tokens\": cond_data.get(\"input_tokens\", 0) + cond_data.get(\"output_tokens\", 0),\n            \"tool_calls\": cond_data.get(\"tool_calls\", 0),\n            \"lines_changed\": cond_data.get(\"lines_changed\", 0),\n        }\n\n    def _compute_delta(self, baseline_runs: list[TaskResult], treatment_runs: list[TaskResult]) -> dict:\n        \"\"\"Compute delta between baseline and treatment using median metrics.\n\n        For single runs, behaves identically to the old _compute_single_delta.\n        For multiple runs, uses medians for efficiency and success rate delta.\n        \"\"\"\n        if not baseline_runs or not treatment_runs:\n            return {}\n\n        # Filter out infra errors for stats\n        b_valid = [r for r in baseline_runs if not self._is_infra_error(r)]\n        t_valid = [r for r in treatment_runs if not self._is_infra_error(r)]\n\n        if not b_valid or not t_valid:\n            return {}\n\n        # Success rate delta\n        b_rate = sum(1 for r in b_valid if r.success) / len(b_valid)\n        t_rate = sum(1 for r in t_valid if r.success) / len(t_valid)\n        rate_delta = t_rate - b_rate\n\n        # Median efficiency metrics\n        b_time = statistics.median(r.wall_clock_seconds for r in b_valid)\n        t_time = statistics.median(r.wall_clock_seconds for r in t_valid)\n        b_tokens = statistics.median(r.input_tokens + r.output_tokens for r in b_valid)\n        t_tokens = statistics.median(r.input_tokens + r.output_tokens for r in t_valid)\n        b_tools = statistics.median(r.tool_calls for r in b_valid)\n        t_tools = statistics.median(r.tool_calls for r in t_valid)\n        b_lines = statistics.median(r.lines_changed for r in b_valid)\n        t_lines = statistics.median(r.lines_changed for r in t_valid)\n\n        def pct(baseline_val, treatment_val):\n            if baseline_val:\n                return (treatment_val - baseline_val) / baseline_val * 100\n            return 0\n\n        return {\n            \"success_rate_delta\": f\"{rate_delta:+.0%}\" if len(b_valid) > 1 else (\n                f\"+{int(t_rate - b_rate)}\" if rate_delta >= 0 else str(int(t_rate - b_rate))\n            ),\n            \"time_percent\": f\"{pct(b_time, t_time):+.1f}%\",\n            \"tokens_percent\": f\"{pct(b_tokens, t_tokens):+.1f}%\",\n            \"tool_calls_percent\": f\"{pct(b_tools, t_tools):+.1f}%\",\n            \"lines_changed_percent\": f\"{pct(b_lines, t_lines):+.1f}%\"\n        }\n\n    @staticmethod\n    def _is_infra_error(r: TaskResult) -> bool:\n        \"\"\"Check if result is a non-experimental error (excluded from stats).\n\n        Infrastructure errors, pre-validation failures, and skill generation\n        failures all indicate harness problems, not experimental outcomes.\n        \"\"\"\n        if r.error is None:\n            return False\n        return r.error.startswith((\"[infrastructure]\", \"[pre-validation]\", \"[skill-generation]\", \"[empty-run]\", \"[timeout]\"))\n\n    def _compute_summary(self, results: list[TaskResult]) -> dict:\n        \"\"\"Compute overall summary statistics.\n\n        Infrastructure errors (clone/docker/workspace failures) are excluded\n        from success rate calculations to avoid corrupting experimental data.\n        \"\"\"\n        def success_rate(task_results: list[TaskResult]) -> float:\n            valid = [r for r in task_results if not self._is_infra_error(r)]\n            if not valid:\n                return 0\n            return round(sum(1 for r in valid if r.success) / len(valid), 2)\n\n        none_results = [r for r in results if r.condition == Condition.NONE]\n        flat_results = [r for r in results if r.condition == Condition.FLAT_LLM]\n        il_results = [r for r in results if r.condition == Condition.INTENT_LAYER]\n\n        infra_errors = sum(1 for r in results if self._is_infra_error(r))\n\n        summary: dict[str, Any] = {\n            \"total_tasks\": len(set(r.task_id for r in results)),\n            \"infrastructure_errors\": infra_errors,\n            \"none_success_rate\": success_rate(none_results),\n            \"flat_llm_success_rate\": success_rate(flat_results),\n            \"intent_layer_success_rate\": success_rate(il_results),\n        }\n\n        # Add CIs when we have multi-run data\n        has_multi_run = any(\n            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n            for r in results\n        )\n        if has_multi_run:\n            for label, cond_results in [\n                (\"none\", none_results),\n                (\"flat_llm\", flat_results),\n                (\"intent_layer\", il_results),\n            ]:\n                valid = [r for r in cond_results if not self._is_infra_error(r)]\n                if valid:\n                    successes = sum(1 for r in valid if r.success)\n                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n                    summary[f\"{label}_ci_90\"] = {\n                        \"lower\": round(ci_lower, 3),\n                        \"upper\": round(ci_upper, 3),\n                    }\n\n            # Significance: check CI overlap between none and each treatment\n            none_ci = summary.get(\"none_ci_90\")\n            if none_ci:\n                for treatment in (\"flat_llm\", \"intent_layer\"):\n                    t_ci = summary.get(f\"{treatment}_ci_90\")\n                    if t_ci:\n                        overlaps = ci_overlap(\n                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n                        )\n                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n\n        return summary\n\n    def write_json(self, results: EvalResults) -> str:\n        \"\"\"Write results to JSON file.\"\"\"\n        path = self.output_dir / f\"{results.eval_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(results), f, indent=2)\n        return str(path)\n\n    @staticmethod\n    def _format_ci(ci: dict) -> str:\n        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n\n    def write_markdown(self, results: EvalResults) -> str:\n        \"\"\"Write results to Markdown file.\n\n        Handles both single-run (backward-compatible) and multi-run formats.\n        Uses _get_fix_metrics() to extract efficiency numbers from any format.\n        When multi-run data includes CIs, they're shown inline with pass rates.\n        \"\"\"\n        path = self.output_dir / f\"{results.eval_id}.md\"\n        summary = results.summary\n        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n\n        lines = [\n            f\"# Eval Results: {results.eval_id}\",\n            \"\",\n            f\"**Timestamp:** {results.timestamp}\",\n            \"\",\n            \"## Summary\",\n            \"\",\n            f\"- **Total tasks:** {summary['total_tasks']}\",\n            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n        ]\n\n        # Per-condition summary with optional CIs\n        for label, display_name in [\n            (\"none\", \"None\"),\n            (\"flat_llm\", \"Flat LLM\"),\n            (\"intent_layer\", \"Intent Layer\"),\n        ]:\n            rate = summary[f\"{label}_success_rate\"]\n            ci = summary.get(f\"{label}_ci_90\")\n            if ci:\n                lines.append(\n                    f\"- **{display_name} success rate:** {rate:.0%} \"\n                    f\"90% CI {self._format_ci(ci)}\"\n                )\n            else:\n                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n\n        # Significance flags\n        if has_cis:\n            lines.append(\"\")\n            for treatment, display_name in [\n                (\"flat_llm\", \"Flat LLM\"),\n                (\"intent_layer\", \"Intent Layer\"),\n            ]:\n                sig_key = f\"{treatment}_vs_none_significant\"\n                if sig_key in summary:\n                    if summary[sig_key]:\n                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n                    else:\n                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n\n            # CI width as variance proxy\n            widths = []\n            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n                ci = summary.get(f\"{label}_ci_90\")\n                if ci:\n                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n            if widths:\n                lines.append(\"\")\n                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n                    f\"{label}: {w:.0%}\" for label, w in widths\n                ))\n\n        lines += [\n            \"\",\n            \"## Results\",\n            \"\",\n        ]\n\n        # Table header — add IL vs none column when multi-run CIs exist\n        if has_cis:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|------------|\"\n            )\n        else:\n            lines.append(\n                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n                \"| \\u0394 Time | \\u0394 Tokens |\"\n            )\n            lines.append(\n                \"|------|-----------|---------|----------|--------|------------|-------\"\n                \"|--------|----------|\"\n            )\n\n        for r in results.results:\n            task_id = r[\"task_id\"]\n            deltas = r.get(\"deltas\", {})\n\n            # Pre-compute per-task CI comparison for IL vs none\n            none_data = r.get(\"none\")\n            il_data = r.get(\"intent_layer\")\n            il_vs_none = \"\"\n            if has_cis and none_data and il_data:\n                none_ci = none_data.get(\"ci_90\")\n                il_ci = il_data.get(\"ci_90\")\n                if none_ci and il_ci:\n                    none_rate = none_data.get(\"success_rate\", 0)\n                    il_rate = il_data.get(\"success_rate\", 0)\n                    diff = il_rate - none_rate\n                    overlaps = ci_overlap(\n                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n                    )\n                    sig_label = \"overlap\" if overlaps else \"sig.\"\n                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n\n            for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                cond_data = r.get(cond_key)\n                if cond_data is None:\n                    continue\n\n                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n                if \"success_rate\" in cond_data:\n                    rate = cond_data[\"success_rate\"]\n                    ci = cond_data.get(\"ci_90\")\n                    if ci:\n                        success = (\n                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n                            f\"{rate:.0%} {self._format_ci(ci)}\"\n                        )\n                    else:\n                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                else:\n                    success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n\n                # Extract efficiency metrics via unified helper\n                metrics = self._get_fix_metrics(cond_data)\n                time_s = metrics[\"wall_clock_seconds\"]\n                tokens = metrics[\"tokens\"]\n                tool_calls = metrics[\"tool_calls\"]\n                lines_changed = metrics[\"lines_changed\"]\n\n                tokens_fmt = f\"{tokens / 1000:.1f}k\"\n\n                # Deltas: none is baseline, shows \"—\"\n                if cond_key == \"none\":\n                    d_time = \"\\u2014\"\n                    d_tokens = \"\\u2014\"\n                else:\n                    delta = deltas.get(cond_key, {})\n                    d_time = delta.get(\"time_percent\", \"N/A\")\n                    d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n\n                row = (\n                    f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                    f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n                    f\"{d_time} | {d_tokens}\"\n                )\n\n                if has_cis:\n                    # Show IL vs none comparison on the intent_layer row\n                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n                    row += f\" | {comparison} |\"\n                else:\n                    row += \" |\"\n\n                lines.append(row)\n\n            # Blank row between tasks\n            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n            lines.append(blank)\n\n        # Remove trailing blank row\n        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n            lines.pop()\n\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        return str(path)\n","numLines":508,"startLine":1,"totalLines":508}},"sourceToolAssistantUUID":"fbddadfb-3a3b-41a6-be14-542e4075d897"}
{"parentUuid":"51bf2a2c-69a1-46ba-a9cd-06af89704db3","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01VV3s1FUWZ1cHLBMHErRods","type":"message","role":"assistant","content":[{"type":"text","text":"Everything looks good. Here's a summary of what was done:\n\n## Changes\n\n### Created: `eval-harness/lib/stats.py` (97 lines)\n- `_inverse_normal_cdf(p)` — Abramowitz & Stegun 26.2.23 rational approximation, ported from `cerberus/src/stats.ts` lines 7-67\n- `wilson_score_interval(successes, n, confidence)` — returns `(lower, upper, center)`, ported from lines 152-176\n- `ci_overlap(ci_a, ci_b)` — simple interval overlap check\n\n### Created: `eval-harness/tests/test_stats.py` (19 tests)\n- Inverse CDF: median, known quantiles, symmetry, boundary errors, tail regions\n- Wilson: zero/all/half successes, degenerate cases, our real 5/8 data, confidence/sample size effects\n- Overlap: overlapping, non-overlapping, touching, contained, identical\n\n### Modified: `eval-harness/lib/reporter.py`\n- `_serialize_condition()`: adds `ci_90: {lower, upper}` to multi-run output\n- `_compute_summary()`: adds per-condition CIs, significance flags (`treatment_vs_none_significant`), when multi-run data present\n- `write_markdown()`: CIs inline with pass rates (`2/3 67% [25%,92%]`), new \"IL vs none\" column with `+33% (overlap)` or `+33% (sig.)`, significance summary, CI width as variance proxy\n\n### Added to: `eval-harness/tests/test_reporter.py` (7 new tests)\n- CI present in multi-run, absent in single-run\n- Summary CIs and significance flags for multi-run\n- Summary clean for single-run (backward compat)\n- Markdown has CI columns / brackets for multi-run\n- Markdown unchanged for single-run\n- JSON includes CIs end-to-end\n\n**41 tests pass, 0 regressions.** Single-run output is byte-identical to before.\n\n`★ Insight ─────────────────────────────────────`\nThe `has_multi_run` detection in `_compute_summary()` scans for any task+condition pair with >1 result. This is an O(n²) check, but n is small (typically 8 tasks × 3 conditions × 5 reps = 120). It avoids threading multi-run state through the call chain or adding a flag to `compile_results()`. The `has_cis` flag in `write_markdown()` then uses the summary's presence of CI keys to decide column layout — keeping the two formats (single vs multi-run) cleanly separated by data shape rather than explicit flags.\n`─────────────────────────────────────────────────`"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7571,"cache_read_input_tokens":110268,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7571},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEF8GpDBcz7Ho3E5pMEq","type":"assistant","uuid":"325e2fe8-0c75-4295-bb39-e1bdda4ff552","timestamp":"2026-02-17T20:07:27.649Z"}
{"parentUuid":"325e2fe8-0c75-4295-bb39-e1bdda4ff552","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","timestamp":"2026-02-17T20:07:27.726Z","uuid":"a864f877-221b-46f3-a721-7728a8e5f86e"}
{"parentUuid":"a864f877-221b-46f3-a721-7728a8e5f86e","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"osascript -e 'display notification \"Task complete\" with title \"Claude Code\"'"},"parentToolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","timestamp":"2026-02-17T20:07:27.727Z","uuid":"fa0f3d4b-1072-487e-b455-7f5ef8e97eb6"}
{"parentUuid":"fa0f3d4b-1072-487e-b455-7f5ef8e97eb6","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh"},"parentToolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","timestamp":"2026-02-17T20:07:27.734Z","uuid":"059d0c28-9a38-4e04-828a-88233471ab28"}
{"parentUuid":"059d0c28-9a38-4e04-828a-88233471ab28","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"${CLAUDE_PLUGIN_ROOT}/scripts/stop-learning-check.sh"},"parentToolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","timestamp":"2026-02-17T20:07:27.735Z","uuid":"d4c738c3-1918-4a8d-9b84-2d6972925a5d"}
{"parentUuid":"d4c738c3-1918-4a8d-9b84-2d6972925a5d","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/stop.py"},"parentToolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b","timestamp":"2026-02-17T20:07:27.736Z","uuid":"e5d0918f-3260-4a21-bb27-fc9ba471f4ba"}
{"parentUuid":"e5d0918f-3260-4a21-bb27-fc9ba471f4ba","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"system","subtype":"stop_hook_summary","hookCount":5,"hookInfos":[{"command":"entire hooks claude-code stop"},{"command":"osascript -e 'display notification \"Task complete\" with title \"Claude Code\"'"},{"command":"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh"},{"command":"${CLAUDE_PLUGIN_ROOT}/scripts/stop-learning-check.sh"},{"command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/stop.py"}],"hookErrors":[],"preventedContinuation":false,"stopReason":"","hasOutput":true,"level":"suggestion","timestamp":"2026-02-17T20:07:28.205Z","uuid":"2136893b-e8aa-4d8e-a92e-27c562bb40ab","toolUseID":"773110f2-2611-4ac5-a783-ac5e34de249b"}
{"parentUuid":"2136893b-e8aa-4d8e-a92e-27c562bb40ab","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"system","subtype":"turn_duration","durationMs":235355,"timestamp":"2026-02-17T20:07:28.206Z","uuid":"e66caee8-2492-4860-8f3d-75395e2ab0de","isMeta":false}
{"type":"file-history-snapshot","messageId":"ae9bef2b-70f8-447b-9d4a-a463a2353f7c","snapshot":{"messageId":"ae9bef2b-70f8-447b-9d4a-a463a2353f7c","trackedFileBackups":{"eval-harness/lib/stats.py":{"backupFileName":"8d8e84f1455463fb@v2","version":2,"backupTime":"2026-02-17T20:07:45.981Z"},"eval-harness/tests/test_stats.py":{"backupFileName":"1dd3c9763d8ecd0f@v2","version":2,"backupTime":"2026-02-17T20:07:45.982Z"},"eval-harness/lib/reporter.py":{"backupFileName":"77c006e5b2bb1d0f@v2","version":2,"backupTime":"2026-02-17T20:07:45.982Z"},"eval-harness/tests/test_reporter.py":{"backupFileName":"7dcb4b31c240ae3a@v2","version":2,"backupTime":"2026-02-17T20:07:45.982Z"}},"timestamp":"2026-02-17T20:07:45.981Z"},"isSnapshotUpdate":false}
{"parentUuid":"e66caee8-2492-4860-8f3d-75395e2ab0de","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":"commit this"},"uuid":"ae9bef2b-70f8-447b-9d4a-a463a2353f7c","timestamp":"2026-02-17T20:07:45.693Z","thinkingMetadata":{"maxThinkingTokens":31999},"permissionMode":"bypassPermissions"}
{"parentUuid":"ae9bef2b-70f8-447b-9d4a-a463a2353f7c","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_0138ShfE1nSU5UiaFZgXXhQZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MeFysB32phA7NjWSWCy657","name":"Bash","input":{"command":"cd /Users/ryo/dev/intent-layer && git status","description":"Show working tree status"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":98043,"cache_read_input_tokens":20500,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":98043},"output_tokens":27,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFAyv3udkQw9wMffSmm","type":"assistant","uuid":"876e8439-3582-4592-a92e-9411cfa137f3","timestamp":"2026-02-17T20:07:50.906Z"}
{"parentUuid":"876e8439-3582-4592-a92e-9411cfa137f3","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01MeFysB32phA7NjWSWCy657","parentToolUseID":"toolu_01MeFysB32phA7NjWSWCy657","uuid":"da89170a-56c8-48c4-8c0f-c80e615f7f07","timestamp":"2026-02-17T20:07:50.908Z"}
{"parentUuid":"da89170a-56c8-48c4-8c0f-c80e615f7f07","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer/eval-harness","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01MeFysB32phA7NjWSWCy657","parentToolUseID":"toolu_01MeFysB32phA7NjWSWCy657","uuid":"697ab30e-99e6-42f7-9756-cb203dafc913","timestamp":"2026-02-17T20:07:50.908Z"}
{"parentUuid":"697ab30e-99e6-42f7-9756-cb203dafc913","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01MeFysB32phA7NjWSWCy657","toolUseID":"toolu_01MeFysB32phA7NjWSWCy657","timestamp":"2026-02-17T20:07:51.065Z","uuid":"70d2ab05-0fda-4a60-9990-bc14123ae7b9"}
{"parentUuid":"876e8439-3582-4592-a92e-9411cfa137f3","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MeFysB32phA7NjWSWCy657","type":"tool_result","content":"On branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   eval-harness/AGENTS.md\n\tmodified:   eval-harness/lib/reporter.py\n\tmodified:   eval-harness/tests/test_reporter.py\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t.claude/\n\t.entire/\n\t.intent-layer/\n\t.superset/\n\teval-harness/docs/brainstorms/\n\teval-harness/docs/plans/2026-02-16-eval-data-quality-optimizations.md\n\teval-harness/docs/reference/\n\teval-harness/lib/stats.py\n\teval-harness/results/2026-02-16-133336.json\n\teval-harness/results/2026-02-16-133336.md\n\teval-harness/results/2026-02-16-134621.json\n\teval-harness/results/2026-02-16-134621.md\n\teval-harness/results/2026-02-16-150803.json\n\teval-harness/results/2026-02-16-150803.md\n\teval-harness/results/2026-02-16-155213.json\n\teval-harness/results/2026-02-16-155213.md\n\teval-harness/results/2026-02-16-160655.json\n\teval-harness/results/2026-02-16-160655.md\n\teval-harness/results/2026-02-16-172007.json\n\teval-harness/results/2026-02-16-172007.md\n\teval-harness/results/2026-02-16-173014.json\n\teval-harness/results/2026-02-16-173014.md\n\teval-harness/results/2026-02-16-173750.json\n\teval-harness/results/2026-02-16-173750.md\n\teval-harness/results/2026-02-16-173936.json\n\teval-harness/results/2026-02-16-173936.md\n\teval-harness/results/2026-02-16-174503.json\n\teval-harness/results/2026-02-16-174503.md\n\teval-harness/results/2026-02-16-201138.json\n\teval-harness/results/2026-02-16-201138.md\n\teval-harness/results/2026-02-16-205339.json\n\teval-harness/results/2026-02-16-205339.md\n\teval-harness/results/2026-02-17-095051.json\n\teval-harness/results/2026-02-17-095051.md\n\teval-harness/results/2026-02-17-105514.json\n\teval-harness/results/2026-02-17-105514.md\n\teval-harness/tests/test_stats.py\n\teval-harness/uv.lock\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")","is_error":false}]},"uuid":"ce955f53-1c2c-4132-ae8d-274096d797f3","timestamp":"2026-02-17T20:07:51.064Z","toolUseResult":{"stdout":"On branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   eval-harness/AGENTS.md\n\tmodified:   eval-harness/lib/reporter.py\n\tmodified:   eval-harness/tests/test_reporter.py\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t.claude/\n\t.entire/\n\t.intent-layer/\n\t.superset/\n\teval-harness/docs/brainstorms/\n\teval-harness/docs/plans/2026-02-16-eval-data-quality-optimizations.md\n\teval-harness/docs/reference/\n\teval-harness/lib/stats.py\n\teval-harness/results/2026-02-16-133336.json\n\teval-harness/results/2026-02-16-133336.md\n\teval-harness/results/2026-02-16-134621.json\n\teval-harness/results/2026-02-16-134621.md\n\teval-harness/results/2026-02-16-150803.json\n\teval-harness/results/2026-02-16-150803.md\n\teval-harness/results/2026-02-16-155213.json\n\teval-harness/results/2026-02-16-155213.md\n\teval-harness/results/2026-02-16-160655.json\n\teval-harness/results/2026-02-16-160655.md\n\teval-harness/results/2026-02-16-172007.json\n\teval-harness/results/2026-02-16-172007.md\n\teval-harness/results/2026-02-16-173014.json\n\teval-harness/results/2026-02-16-173014.md\n\teval-harness/results/2026-02-16-173750.json\n\teval-harness/results/2026-02-16-173750.md\n\teval-harness/results/2026-02-16-173936.json\n\teval-harness/results/2026-02-16-173936.md\n\teval-harness/results/2026-02-16-174503.json\n\teval-harness/results/2026-02-16-174503.md\n\teval-harness/results/2026-02-16-201138.json\n\teval-harness/results/2026-02-16-201138.md\n\teval-harness/results/2026-02-16-205339.json\n\teval-harness/results/2026-02-16-205339.md\n\teval-harness/results/2026-02-17-095051.json\n\teval-harness/results/2026-02-17-095051.md\n\teval-harness/results/2026-02-17-105514.json\n\teval-harness/results/2026-02-17-105514.md\n\teval-harness/tests/test_stats.py\n\teval-harness/uv.lock\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"876e8439-3582-4592-a92e-9411cfa137f3"}
{"parentUuid":"ce955f53-1c2c-4132-ae8d-274096d797f3","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_0138ShfE1nSU5UiaFZgXXhQZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FLQckiDfEv1ETDR8M9APH8","name":"Bash","input":{"command":"git diff eval-harness/lib/reporter.py","description":"Show staged and unstaged changes to reporter"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":98043,"cache_read_input_tokens":20500,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":98043},"output_tokens":27,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFAyv3udkQw9wMffSmm","type":"assistant","uuid":"d52c695d-1521-4915-ba96-be731c15df34","timestamp":"2026-02-17T20:07:51.746Z"}
{"parentUuid":"d52c695d-1521-4915-ba96-be731c15df34","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","parentToolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","uuid":"669a1945-712f-4df5-acae-2e130b2755cf","timestamp":"2026-02-17T20:07:51.748Z"}
{"parentUuid":"669a1945-712f-4df5-acae-2e130b2755cf","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","parentToolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","uuid":"79a3fe20-37c7-42b8-8b4e-438cefd1bd83","timestamp":"2026-02-17T20:07:51.749Z"}
{"parentUuid":"79a3fe20-37c7-42b8-8b4e-438cefd1bd83","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","toolUseID":"toolu_01FLQckiDfEv1ETDR8M9APH8","timestamp":"2026-02-17T20:07:51.900Z","uuid":"635df028-3528-4a59-a35f-618eca0e9f73"}
{"parentUuid":"d52c695d-1521-4915-ba96-be731c15df34","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FLQckiDfEv1ETDR8M9APH8","type":"tool_result","content":"diff --git a/eval-harness/lib/reporter.py b/eval-harness/lib/reporter.py\nindex cbc1901..85ee82e 100644\n--- a/eval-harness/lib/reporter.py\n+++ b/eval-harness/lib/reporter.py\n@@ -8,6 +8,7 @@ from pathlib import Path\n from typing import Any\n \n from lib.task_runner import TaskResult, Condition\n+from lib.stats import wilson_score_interval, ci_overlap\n \n \n @dataclass\n@@ -144,6 +145,12 @@ class Reporter:\n         result[\"successes\"] = successes\n         result[\"total_valid_runs\"] = len(valid_runs)\n \n+        # Wilson Score confidence interval (90%)\n+        ci_lower, ci_upper, ci_center = wilson_score_interval(\n+            successes, len(valid_runs), confidence=0.90\n+        )\n+        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n+\n         # Efficiency medians (secondary metrics — from valid runs only)\n         if valid_runs:\n             result[\"median\"] = {\n@@ -268,7 +275,7 @@ class Reporter:\n \n         infra_errors = sum(1 for r in results if self._is_infra_error(r))\n \n-        return {\n+        summary: dict[str, Any] = {\n             \"total_tasks\": len(set(r.task_id for r in results)),\n             \"infrastructure_errors\": infra_errors,\n             \"none_success_rate\": success_rate(none_results),\n@@ -276,6 +283,40 @@ class Reporter:\n             \"intent_layer_success_rate\": success_rate(il_results),\n         }\n \n+        # Add CIs when we have multi-run data\n+        has_multi_run = any(\n+            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n+            for r in results\n+        )\n+        if has_multi_run:\n+            for label, cond_results in [\n+                (\"none\", none_results),\n+                (\"flat_llm\", flat_results),\n+                (\"intent_layer\", il_results),\n+            ]:\n+                valid = [r for r in cond_results if not self._is_infra_error(r)]\n+                if valid:\n+                    successes = sum(1 for r in valid if r.success)\n+                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n+                    summary[f\"{label}_ci_90\"] = {\n+                        \"lower\": round(ci_lower, 3),\n+                        \"upper\": round(ci_upper, 3),\n+                    }\n+\n+            # Significance: check CI overlap between none and each treatment\n+            none_ci = summary.get(\"none_ci_90\")\n+            if none_ci:\n+                for treatment in (\"flat_llm\", \"intent_layer\"):\n+                    t_ci = summary.get(f\"{treatment}_ci_90\")\n+                    if t_ci:\n+                        overlaps = ci_overlap(\n+                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n+                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n+                        )\n+                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n+\n+        return summary\n+\n     def write_json(self, results: EvalResults) -> str:\n         \"\"\"Write results to JSON file.\"\"\"\n         path = self.output_dir / f\"{results.eval_id}.json\"\n@@ -283,13 +324,21 @@ class Reporter:\n             json.dump(asdict(results), f, indent=2)\n         return str(path)\n \n+    @staticmethod\n+    def _format_ci(ci: dict) -> str:\n+        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n+        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n+\n     def write_markdown(self, results: EvalResults) -> str:\n         \"\"\"Write results to Markdown file.\n \n         Handles both single-run (backward-compatible) and multi-run formats.\n         Uses _get_fix_metrics() to extract efficiency numbers from any format.\n+        When multi-run data includes CIs, they're shown inline with pass rates.\n         \"\"\"\n         path = self.output_dir / f\"{results.eval_id}.md\"\n+        summary = results.summary\n+        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n \n         lines = [\n             f\"# Eval Results: {results.eval_id}\",\n@@ -298,31 +347,116 @@ class Reporter:\n             \"\",\n             \"## Summary\",\n             \"\",\n-            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n-            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n-            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n-            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n-            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n+            f\"- **Total tasks:** {summary['total_tasks']}\",\n+            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n+        ]\n+\n+        # Per-condition summary with optional CIs\n+        for label, display_name in [\n+            (\"none\", \"None\"),\n+            (\"flat_llm\", \"Flat LLM\"),\n+            (\"intent_layer\", \"Intent Layer\"),\n+        ]:\n+            rate = summary[f\"{label}_success_rate\"]\n+            ci = summary.get(f\"{label}_ci_90\")\n+            if ci:\n+                lines.append(\n+                    f\"- **{display_name} success rate:** {rate:.0%} \"\n+                    f\"90% CI {self._format_ci(ci)}\"\n+                )\n+            else:\n+                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n+\n+        # Significance flags\n+        if has_cis:\n+            lines.append(\"\")\n+            for treatment, display_name in [\n+                (\"flat_llm\", \"Flat LLM\"),\n+                (\"intent_layer\", \"Intent Layer\"),\n+            ]:\n+                sig_key = f\"{treatment}_vs_none_significant\"\n+                if sig_key in summary:\n+                    if summary[sig_key]:\n+                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n+                    else:\n+                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n+\n+            # CI width as variance proxy\n+            widths = []\n+            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n+                ci = summary.get(f\"{label}_ci_90\")\n+                if ci:\n+                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n+            if widths:\n+                lines.append(\"\")\n+                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n+                    f\"{label}: {w:.0%}\" for label, w in widths\n+                ))\n+\n+        lines += [\n             \"\",\n             \"## Results\",\n             \"\",\n-            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n-            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n         ]\n \n+        # Table header — add IL vs none column when multi-run CIs exist\n+        if has_cis:\n+            lines.append(\n+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n+                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n+            )\n+            lines.append(\n+                \"|------|-----------|---------|----------|--------|------------|-------\"\n+                \"|--------|----------|------------|\"\n+            )\n+        else:\n+            lines.append(\n+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n+                \"| \\u0394 Time | \\u0394 Tokens |\"\n+            )\n+            lines.append(\n+                \"|------|-----------|---------|----------|--------|------------|-------\"\n+                \"|--------|----------|\"\n+            )\n+\n         for r in results.results:\n             task_id = r[\"task_id\"]\n             deltas = r.get(\"deltas\", {})\n \n+            # Pre-compute per-task CI comparison for IL vs none\n+            none_data = r.get(\"none\")\n+            il_data = r.get(\"intent_layer\")\n+            il_vs_none = \"\"\n+            if has_cis and none_data and il_data:\n+                none_ci = none_data.get(\"ci_90\")\n+                il_ci = il_data.get(\"ci_90\")\n+                if none_ci and il_ci:\n+                    none_rate = none_data.get(\"success_rate\", 0)\n+                    il_rate = il_data.get(\"success_rate\", 0)\n+                    diff = il_rate - none_rate\n+                    overlaps = ci_overlap(\n+                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n+                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n+                    )\n+                    sig_label = \"overlap\" if overlaps else \"sig.\"\n+                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n+\n             for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                 cond_data = r.get(cond_key)\n                 if cond_data is None:\n                     continue\n \n-                # Success: multi-run shows rate, single-run shows PASS/FAIL\n+                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n                 if \"success_rate\" in cond_data:\n                     rate = cond_data[\"success_rate\"]\n-                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n+                    ci = cond_data.get(\"ci_90\")\n+                    if ci:\n+                        success = (\n+                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n+                            f\"{rate:.0%} {self._format_ci(ci)}\"\n+                        )\n+                    else:\n+                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                 else:\n                     success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n \n@@ -344,17 +478,27 @@ class Reporter:\n                     d_time = delta.get(\"time_percent\", \"N/A\")\n                     d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n \n-                lines.append(\n+                row = (\n                     f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                     f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n-                    f\"{d_time} | {d_tokens} |\"\n+                    f\"{d_time} | {d_tokens}\"\n                 )\n \n+                if has_cis:\n+                    # Show IL vs none comparison on the intent_layer row\n+                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n+                    row += f\" | {comparison} |\"\n+                else:\n+                    row += \" |\"\n+\n+                lines.append(row)\n+\n             # Blank row between tasks\n-            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n+            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n+            lines.append(blank)\n \n         # Remove trailing blank row\n-        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n+        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n             lines.pop()\n \n         with open(path, \"w\") as f:","is_error":false}]},"uuid":"10e00479-6390-49d7-a458-f8724770f888","timestamp":"2026-02-17T20:07:51.899Z","toolUseResult":{"stdout":"diff --git a/eval-harness/lib/reporter.py b/eval-harness/lib/reporter.py\nindex cbc1901..85ee82e 100644\n--- a/eval-harness/lib/reporter.py\n+++ b/eval-harness/lib/reporter.py\n@@ -8,6 +8,7 @@ from pathlib import Path\n from typing import Any\n \n from lib.task_runner import TaskResult, Condition\n+from lib.stats import wilson_score_interval, ci_overlap\n \n \n @dataclass\n@@ -144,6 +145,12 @@ class Reporter:\n         result[\"successes\"] = successes\n         result[\"total_valid_runs\"] = len(valid_runs)\n \n+        # Wilson Score confidence interval (90%)\n+        ci_lower, ci_upper, ci_center = wilson_score_interval(\n+            successes, len(valid_runs), confidence=0.90\n+        )\n+        result[\"ci_90\"] = {\"lower\": round(ci_lower, 3), \"upper\": round(ci_upper, 3)}\n+\n         # Efficiency medians (secondary metrics — from valid runs only)\n         if valid_runs:\n             result[\"median\"] = {\n@@ -268,7 +275,7 @@ class Reporter:\n \n         infra_errors = sum(1 for r in results if self._is_infra_error(r))\n \n-        return {\n+        summary: dict[str, Any] = {\n             \"total_tasks\": len(set(r.task_id for r in results)),\n             \"infrastructure_errors\": infra_errors,\n             \"none_success_rate\": success_rate(none_results),\n@@ -276,6 +283,40 @@ class Reporter:\n             \"intent_layer_success_rate\": success_rate(il_results),\n         }\n \n+        # Add CIs when we have multi-run data\n+        has_multi_run = any(\n+            sum(1 for r2 in results if r2.task_id == r.task_id and r2.condition == r.condition) > 1\n+            for r in results\n+        )\n+        if has_multi_run:\n+            for label, cond_results in [\n+                (\"none\", none_results),\n+                (\"flat_llm\", flat_results),\n+                (\"intent_layer\", il_results),\n+            ]:\n+                valid = [r for r in cond_results if not self._is_infra_error(r)]\n+                if valid:\n+                    successes = sum(1 for r in valid if r.success)\n+                    ci_lower, ci_upper, _ = wilson_score_interval(successes, len(valid), 0.90)\n+                    summary[f\"{label}_ci_90\"] = {\n+                        \"lower\": round(ci_lower, 3),\n+                        \"upper\": round(ci_upper, 3),\n+                    }\n+\n+            # Significance: check CI overlap between none and each treatment\n+            none_ci = summary.get(\"none_ci_90\")\n+            if none_ci:\n+                for treatment in (\"flat_llm\", \"intent_layer\"):\n+                    t_ci = summary.get(f\"{treatment}_ci_90\")\n+                    if t_ci:\n+                        overlaps = ci_overlap(\n+                            (none_ci[\"lower\"], none_ci[\"upper\"]),\n+                            (t_ci[\"lower\"], t_ci[\"upper\"]),\n+                        )\n+                        summary[f\"{treatment}_vs_none_significant\"] = not overlaps\n+\n+        return summary\n+\n     def write_json(self, results: EvalResults) -> str:\n         \"\"\"Write results to JSON file.\"\"\"\n         path = self.output_dir / f\"{results.eval_id}.json\"\n@@ -283,13 +324,21 @@ class Reporter:\n             json.dump(asdict(results), f, indent=2)\n         return str(path)\n \n+    @staticmethod\n+    def _format_ci(ci: dict) -> str:\n+        \"\"\"Format a CI dict as '[lower,upper]' with percentage values.\"\"\"\n+        return f\"[{ci['lower']:.0%},{ci['upper']:.0%}]\"\n+\n     def write_markdown(self, results: EvalResults) -> str:\n         \"\"\"Write results to Markdown file.\n \n         Handles both single-run (backward-compatible) and multi-run formats.\n         Uses _get_fix_metrics() to extract efficiency numbers from any format.\n+        When multi-run data includes CIs, they're shown inline with pass rates.\n         \"\"\"\n         path = self.output_dir / f\"{results.eval_id}.md\"\n+        summary = results.summary\n+        has_cis = \"none_ci_90\" in summary or \"intent_layer_ci_90\" in summary\n \n         lines = [\n             f\"# Eval Results: {results.eval_id}\",\n@@ -298,31 +347,116 @@ class Reporter:\n             \"\",\n             \"## Summary\",\n             \"\",\n-            f\"- **Total tasks:** {results.summary['total_tasks']}\",\n-            f\"- **Infrastructure errors:** {results.summary['infrastructure_errors']}\",\n-            f\"- **None success rate:** {results.summary['none_success_rate']:.0%}\",\n-            f\"- **Flat LLM success rate:** {results.summary['flat_llm_success_rate']:.0%}\",\n-            f\"- **Intent Layer success rate:** {results.summary['intent_layer_success_rate']:.0%}\",\n+            f\"- **Total tasks:** {summary['total_tasks']}\",\n+            f\"- **Infrastructure errors:** {summary['infrastructure_errors']}\",\n+        ]\n+\n+        # Per-condition summary with optional CIs\n+        for label, display_name in [\n+            (\"none\", \"None\"),\n+            (\"flat_llm\", \"Flat LLM\"),\n+            (\"intent_layer\", \"Intent Layer\"),\n+        ]:\n+            rate = summary[f\"{label}_success_rate\"]\n+            ci = summary.get(f\"{label}_ci_90\")\n+            if ci:\n+                lines.append(\n+                    f\"- **{display_name} success rate:** {rate:.0%} \"\n+                    f\"90% CI {self._format_ci(ci)}\"\n+                )\n+            else:\n+                lines.append(f\"- **{display_name} success rate:** {rate:.0%}\")\n+\n+        # Significance flags\n+        if has_cis:\n+            lines.append(\"\")\n+            for treatment, display_name in [\n+                (\"flat_llm\", \"Flat LLM\"),\n+                (\"intent_layer\", \"Intent Layer\"),\n+            ]:\n+                sig_key = f\"{treatment}_vs_none_significant\"\n+                if sig_key in summary:\n+                    if summary[sig_key]:\n+                        lines.append(f\"- **{display_name} vs None:** significant (non-overlapping CIs)\")\n+                    else:\n+                        lines.append(f\"- **{display_name} vs None:** not significant (overlapping CIs)\")\n+\n+            # CI width as variance proxy\n+            widths = []\n+            for label in (\"none\", \"flat_llm\", \"intent_layer\"):\n+                ci = summary.get(f\"{label}_ci_90\")\n+                if ci:\n+                    widths.append((label, ci[\"upper\"] - ci[\"lower\"]))\n+            if widths:\n+                lines.append(\"\")\n+                lines.append(\"**CI widths (variance proxy):** \" + \", \".join(\n+                    f\"{label}: {w:.0%}\" for label, w in widths\n+                ))\n+\n+        lines += [\n             \"\",\n             \"## Results\",\n             \"\",\n-            \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines | \\u0394 Time | \\u0394 Tokens |\",\n-            \"|------|-----------|---------|----------|--------|------------|-------|--------|----------|\",\n         ]\n \n+        # Table header — add IL vs none column when multi-run CIs exist\n+        if has_cis:\n+            lines.append(\n+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n+                \"| \\u0394 Time | \\u0394 Tokens | IL vs none |\"\n+            )\n+            lines.append(\n+                \"|------|-----------|---------|----------|--------|------------|-------\"\n+                \"|--------|----------|------------|\"\n+            )\n+        else:\n+            lines.append(\n+                \"| Task | Condition | Success | Time (s) | Tokens | Tool Calls | Lines \"\n+                \"| \\u0394 Time | \\u0394 Tokens |\"\n+            )\n+            lines.append(\n+                \"|------|-----------|---------|----------|--------|------------|-------\"\n+                \"|--------|----------|\"\n+            )\n+\n         for r in results.results:\n             task_id = r[\"task_id\"]\n             deltas = r.get(\"deltas\", {})\n \n+            # Pre-compute per-task CI comparison for IL vs none\n+            none_data = r.get(\"none\")\n+            il_data = r.get(\"intent_layer\")\n+            il_vs_none = \"\"\n+            if has_cis and none_data and il_data:\n+                none_ci = none_data.get(\"ci_90\")\n+                il_ci = il_data.get(\"ci_90\")\n+                if none_ci and il_ci:\n+                    none_rate = none_data.get(\"success_rate\", 0)\n+                    il_rate = il_data.get(\"success_rate\", 0)\n+                    diff = il_rate - none_rate\n+                    overlaps = ci_overlap(\n+                        (none_ci[\"lower\"], none_ci[\"upper\"]),\n+                        (il_ci[\"lower\"], il_ci[\"upper\"]),\n+                    )\n+                    sig_label = \"overlap\" if overlaps else \"sig.\"\n+                    il_vs_none = f\"{diff:+.0%} ({sig_label})\"\n+\n             for cond_key in (\"none\", \"flat_llm\", \"intent_layer\"):\n                 cond_data = r.get(cond_key)\n                 if cond_data is None:\n                     continue\n \n-                # Success: multi-run shows rate, single-run shows PASS/FAIL\n+                # Success: multi-run shows rate + CI, single-run shows PASS/FAIL\n                 if \"success_rate\" in cond_data:\n                     rate = cond_data[\"success_rate\"]\n-                    success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n+                    ci = cond_data.get(\"ci_90\")\n+                    if ci:\n+                        success = (\n+                            f\"{cond_data['successes']}/{cond_data['total_valid_runs']} \"\n+                            f\"{rate:.0%} {self._format_ci(ci)}\"\n+                        )\n+                    else:\n+                        success = f\"{rate:.0%} ({cond_data['successes']}/{cond_data['total_valid_runs']})\"\n                 else:\n                     success = \"PASS\" if cond_data.get(\"success\") else \"FAIL\"\n \n@@ -344,17 +478,27 @@ class Reporter:\n                     d_time = delta.get(\"time_percent\", \"N/A\")\n                     d_tokens = delta.get(\"tokens_percent\", \"N/A\")\n \n-                lines.append(\n+                row = (\n                     f\"| {task_id} | {cond_key} | {success} | {time_s:.1f} | \"\n                     f\"{tokens_fmt} | {tool_calls} | {lines_changed} | \"\n-                    f\"{d_time} | {d_tokens} |\"\n+                    f\"{d_time} | {d_tokens}\"\n                 )\n \n+                if has_cis:\n+                    # Show IL vs none comparison on the intent_layer row\n+                    comparison = il_vs_none if cond_key == \"intent_layer\" else \"\"\n+                    row += f\" | {comparison} |\"\n+                else:\n+                    row += \" |\"\n+\n+                lines.append(row)\n+\n             # Blank row between tasks\n-            lines.append(\"|  |  |  |  |  |  |  |  |  |\")\n+            blank = \"|  |  |  |  |  |  |  |  |  |\" + (\"  |\" if has_cis else \"\")\n+            lines.append(blank)\n \n         # Remove trailing blank row\n-        if lines and lines[-1] == \"|  |  |  |  |  |  |  |  |  |\":\n+        if lines and lines[-1].strip().replace(\"|\", \"\").replace(\" \", \"\") == \"\":\n             lines.pop()\n \n         with open(path, \"w\") as f:","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"d52c695d-1521-4915-ba96-be731c15df34"}
{"parentUuid":"10e00479-6390-49d7-a458-f8724770f888","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_0138ShfE1nSU5UiaFZgXXhQZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01U3bB8TCEfWV67r62H5okRZ","name":"Bash","input":{"command":"git log --oneline -5","description":"Show recent commit messages"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":98043,"cache_read_input_tokens":20500,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":98043},"output_tokens":27,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFAyv3udkQw9wMffSmm","type":"assistant","uuid":"d9fd5708-3254-4ee8-bd7d-3fdf4c893bad","timestamp":"2026-02-17T20:07:52.527Z"}
{"parentUuid":"d9fd5708-3254-4ee8-bd7d-3fdf4c893bad","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","parentToolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","uuid":"bcb01048-c018-4e5a-94db-2e9ca670e394","timestamp":"2026-02-17T20:07:52.529Z"}
{"parentUuid":"bcb01048-c018-4e5a-94db-2e9ca670e394","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","parentToolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","uuid":"b61b81f5-89cf-4cfe-995f-22a55d6b193f","timestamp":"2026-02-17T20:07:52.529Z"}
{"parentUuid":"b61b81f5-89cf-4cfe-995f-22a55d6b193f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","toolUseID":"toolu_01U3bB8TCEfWV67r62H5okRZ","timestamp":"2026-02-17T20:07:52.782Z","uuid":"1d7e3aa1-2c86-4f82-ba87-8ec67d2ee4f3"}
{"parentUuid":"d9fd5708-3254-4ee8-bd7d-3fdf4c893bad","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01U3bB8TCEfWV67r62H5okRZ","type":"tool_result","content":"9f6b65f fix: targeted tests, cached pre-validation, thread-safe parallel runs\n05b797e fix: correct task 7 from INFRA to empty-run based on full log recovery Entire-Checkpoint: 8ba59773b5f7\n1a1a4d3 chore: hoist imports to module level, gitignore logs/ Entire-Checkpoint: 8ba59773b5f7\n6aec889 results: first full 3-condition eval on fastmcp (8 tasks)\nf4e5691 fix: test injection, pre-validation rewrite, cache concurrency","is_error":false}]},"uuid":"92181d15-c8be-4dde-9d0b-b723087cf45b","timestamp":"2026-02-17T20:07:52.781Z","toolUseResult":{"stdout":"9f6b65f fix: targeted tests, cached pre-validation, thread-safe parallel runs\n05b797e fix: correct task 7 from INFRA to empty-run based on full log recovery Entire-Checkpoint: 8ba59773b5f7\n1a1a4d3 chore: hoist imports to module level, gitignore logs/ Entire-Checkpoint: 8ba59773b5f7\n6aec889 results: first full 3-condition eval on fastmcp (8 tasks)\nf4e5691 fix: test injection, pre-validation rewrite, cache concurrency","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"d9fd5708-3254-4ee8-bd7d-3fdf4c893bad"}
{"parentUuid":"92181d15-c8be-4dde-9d0b-b723087cf45b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01GxYzSnj7V1nWprjb9vDrNR","type":"message","role":"assistant","content":[{"type":"text","text":"I'll commit only the stats-related files (the new module, reporter changes, and tests), not the unrelated untracked files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4605,"cache_read_input_tokens":118543,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4605},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFBUvp6qWptoTFe9mdq","type":"assistant","uuid":"6576bcd5-4531-45c0-9fc7-edc2974a0cc8","timestamp":"2026-02-17T20:07:57.506Z"}
{"parentUuid":"6576bcd5-4531-45c0-9fc7-edc2974a0cc8","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01GxYzSnj7V1nWprjb9vDrNR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01L4VfbM59unub297sxa9VWV","name":"Bash","input":{"command":"git add eval-harness/lib/stats.py eval-harness/lib/reporter.py eval-harness/tests/test_stats.py eval-harness/tests/test_reporter.py && git commit -m \"$(cat <<'EOF'\nfeat: add Wilson Score confidence intervals to eval reporter\n\nPort inverse normal CDF and Wilson Score CI from cerberus/src/stats.ts.\nCIs appear automatically in both JSON and markdown when repetitions > 1.\nAdds per-task IL vs none significance column and CI width variance proxy.\nSingle-run output unchanged (backward compatible). 41 tests pass.\nEOF\n)\"","description":"Stage and commit stats integration files"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4605,"cache_read_input_tokens":118543,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4605},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFBUvp6qWptoTFe9mdq","type":"assistant","uuid":"45863daf-916b-412e-80c6-99c1058d558f","timestamp":"2026-02-17T20:08:01.466Z"}
{"parentUuid":"45863daf-916b-412e-80c6-99c1058d558f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01L4VfbM59unub297sxa9VWV","parentToolUseID":"toolu_01L4VfbM59unub297sxa9VWV","uuid":"71286bc0-2dd1-4e3b-9fee-d34ba5e5d812","timestamp":"2026-02-17T20:08:01.469Z"}
{"parentUuid":"71286bc0-2dd1-4e3b-9fee-d34ba5e5d812","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01L4VfbM59unub297sxa9VWV","parentToolUseID":"toolu_01L4VfbM59unub297sxa9VWV","uuid":"1eb06639-5201-40a4-877c-7b33be6b877b","timestamp":"2026-02-17T20:08:01.469Z"}
{"parentUuid":"1eb06639-5201-40a4-877c-7b33be6b877b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01L4VfbM59unub297sxa9VWV","toolUseID":"toolu_01L4VfbM59unub297sxa9VWV","timestamp":"2026-02-17T20:08:08.273Z","uuid":"6f940012-2e16-43b7-8ddc-a80039932e14"}
{"parentUuid":"45863daf-916b-412e-80c6-99c1058d558f","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01L4VfbM59unub297sxa9VWV","type":"tool_result","content":"[main efa4342] feat: add Wilson Score confidence intervals to eval reporter\n 4 files changed, 577 insertions(+), 14 deletions(-)\n create mode 100644 eval-harness/lib/stats.py\n create mode 100644 eval-harness/tests/test_stats.py","is_error":false}]},"uuid":"d2a13e8d-5a9a-4f93-8bf1-3cac2d9fea2b","timestamp":"2026-02-17T20:08:08.272Z","toolUseResult":{"stdout":"[main efa4342] feat: add Wilson Score confidence intervals to eval reporter\n 4 files changed, 577 insertions(+), 14 deletions(-)\n create mode 100644 eval-harness/lib/stats.py\n create mode 100644 eval-harness/tests/test_stats.py","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"45863daf-916b-412e-80c6-99c1058d558f"}
{"parentUuid":"d2a13e8d-5a9a-4f93-8bf1-3cac2d9fea2b","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_017h8nfkkYFhpbm7Dq5SdDtp","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GwuAtNgdSSXCao3nYhc8X3","name":"Bash","input":{"command":"git status","description":"Verify commit succeeded"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":335,"cache_read_input_tokens":123148,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":335},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFCd4Ee8Skdai1d5eEM","type":"assistant","uuid":"c8b65544-c080-4e68-b36a-d943b9dbce04","timestamp":"2026-02-17T20:08:11.436Z"}
{"parentUuid":"c8b65544-c080-4e68-b36a-d943b9dbce04","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"/Users/ryo/.local/bin/dcg"},"toolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","parentToolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","uuid":"23c3c2ea-a6fe-4493-bfca-df8414125414","timestamp":"2026-02-17T20:08:11.438Z"}
{"parentUuid":"23c3c2ea-a6fe-4493-bfca-df8414125414","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PreToolUse","hookName":"PreToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/pretooluse.py"},"toolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","parentToolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","uuid":"5d54bc10-3e97-412b-88c0-4648d2508bcb","timestamp":"2026-02-17T20:08:11.438Z"}
{"parentUuid":"5d54bc10-3e97-412b-88c0-4648d2508bcb","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Bash","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/posttooluse.py"},"parentToolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","toolUseID":"toolu_01GwuAtNgdSSXCao3nYhc8X3","timestamp":"2026-02-17T20:08:11.709Z","uuid":"2adc72b6-fd87-46b3-b757-14d32c687211"}
{"parentUuid":"c8b65544-c080-4e68-b36a-d943b9dbce04","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GwuAtNgdSSXCao3nYhc8X3","type":"tool_result","content":"On branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   eval-harness/AGENTS.md\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t.claude/\n\t.entire/\n\t.intent-layer/\n\t.superset/\n\teval-harness/docs/brainstorms/\n\teval-harness/docs/plans/2026-02-16-eval-data-quality-optimizations.md\n\teval-harness/docs/reference/\n\teval-harness/results/2026-02-16-133336.json\n\teval-harness/results/2026-02-16-133336.md\n\teval-harness/results/2026-02-16-134621.json\n\teval-harness/results/2026-02-16-134621.md\n\teval-harness/results/2026-02-16-150803.json\n\teval-harness/results/2026-02-16-150803.md\n\teval-harness/results/2026-02-16-155213.json\n\teval-harness/results/2026-02-16-155213.md\n\teval-harness/results/2026-02-16-160655.json\n\teval-harness/results/2026-02-16-160655.md\n\teval-harness/results/2026-02-16-172007.json\n\teval-harness/results/2026-02-16-172007.md\n\teval-harness/results/2026-02-16-173014.json\n\teval-harness/results/2026-02-16-173014.md\n\teval-harness/results/2026-02-16-173750.json\n\teval-harness/results/2026-02-16-173750.md\n\teval-harness/results/2026-02-16-173936.json\n\teval-harness/results/2026-02-16-173936.md\n\teval-harness/results/2026-02-16-174503.json\n\teval-harness/results/2026-02-16-174503.md\n\teval-harness/results/2026-02-16-201138.json\n\teval-harness/results/2026-02-16-201138.md\n\teval-harness/results/2026-02-16-205339.json\n\teval-harness/results/2026-02-16-205339.md\n\teval-harness/results/2026-02-17-095051.json\n\teval-harness/results/2026-02-17-095051.md\n\teval-harness/results/2026-02-17-105514.json\n\teval-harness/results/2026-02-17-105514.md\n\teval-harness/uv.lock\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")","is_error":false}]},"uuid":"93feac05-0549-400c-839c-9be61c9dee3a","timestamp":"2026-02-17T20:08:11.707Z","toolUseResult":{"stdout":"On branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   eval-harness/AGENTS.md\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t.claude/\n\t.entire/\n\t.intent-layer/\n\t.superset/\n\teval-harness/docs/brainstorms/\n\teval-harness/docs/plans/2026-02-16-eval-data-quality-optimizations.md\n\teval-harness/docs/reference/\n\teval-harness/results/2026-02-16-133336.json\n\teval-harness/results/2026-02-16-133336.md\n\teval-harness/results/2026-02-16-134621.json\n\teval-harness/results/2026-02-16-134621.md\n\teval-harness/results/2026-02-16-150803.json\n\teval-harness/results/2026-02-16-150803.md\n\teval-harness/results/2026-02-16-155213.json\n\teval-harness/results/2026-02-16-155213.md\n\teval-harness/results/2026-02-16-160655.json\n\teval-harness/results/2026-02-16-160655.md\n\teval-harness/results/2026-02-16-172007.json\n\teval-harness/results/2026-02-16-172007.md\n\teval-harness/results/2026-02-16-173014.json\n\teval-harness/results/2026-02-16-173014.md\n\teval-harness/results/2026-02-16-173750.json\n\teval-harness/results/2026-02-16-173750.md\n\teval-harness/results/2026-02-16-173936.json\n\teval-harness/results/2026-02-16-173936.md\n\teval-harness/results/2026-02-16-174503.json\n\teval-harness/results/2026-02-16-174503.md\n\teval-harness/results/2026-02-16-201138.json\n\teval-harness/results/2026-02-16-201138.md\n\teval-harness/results/2026-02-16-205339.json\n\teval-harness/results/2026-02-16-205339.md\n\teval-harness/results/2026-02-17-095051.json\n\teval-harness/results/2026-02-17-095051.md\n\teval-harness/results/2026-02-17-105514.json\n\teval-harness/results/2026-02-17-105514.md\n\teval-harness/uv.lock\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"c8b65544-c080-4e68-b36a-d943b9dbce04"}
{"parentUuid":"93feac05-0549-400c-839c-9be61c9dee3a","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","message":{"model":"claude-opus-4-6","id":"msg_01R2vs2ZWEQpqr9fbqD289Bh","type":"message","role":"assistant","content":[{"type":"text","text":"Committed as `efa4342`. The 4 files (stats module, reporter changes, and both test files) are in the commit. The remaining untracked files are pre-existing results/docs/config that weren't part of this change."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":871,"cache_read_input_tokens":123483,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":871},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYEFCspzHmi8Ao2MuZcQX","type":"assistant","uuid":"c198787b-0657-4d19-a8b0-b4bbfd9ea859","timestamp":"2026-02-17T20:08:16.052Z"}
{"parentUuid":"c198787b-0657-4d19-a8b0-b4bbfd9ea859","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","toolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","timestamp":"2026-02-17T20:08:16.134Z","uuid":"dd83b1c7-fde7-4ec2-b81f-10a97ed9d2b4"}
{"parentUuid":"dd83b1c7-fde7-4ec2-b81f-10a97ed9d2b4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"osascript -e 'display notification \"Task complete\" with title \"Claude Code\"'"},"parentToolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","toolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","timestamp":"2026-02-17T20:08:16.135Z","uuid":"eac3c4b6-1bb4-4f26-8314-54e9f3ad69bc"}
{"parentUuid":"eac3c4b6-1bb4-4f26-8314-54e9f3ad69bc","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"${CLAUDE_PLUGIN_ROOT}/hooks/stop-hook.sh"},"parentToolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","toolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","timestamp":"2026-02-17T20:08:16.139Z","uuid":"0d508106-7dcc-4d0f-ab46-5c1aa78e7cb4"}
{"parentUuid":"0d508106-7dcc-4d0f-ab46-5c1aa78e7cb4","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"${CLAUDE_PLUGIN_ROOT}/scripts/stop-learning-check.sh"},"parentToolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","toolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","timestamp":"2026-02-17T20:08:16.140Z","uuid":"25e61f9f-dfd1-4f70-843a-66c052fd8b14"}
{"parentUuid":"25e61f9f-dfd1-4f70-843a-66c052fd8b14","isSidechain":false,"userType":"external","cwd":"/Users/ryo/dev/intent-layer","sessionId":"028cb10b-d816-4d66-b100-17f044d67c91","version":"2.1.45","gitBranch":"main","slug":"sparkling-questing-crown","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/stop.py"},"parentToolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","toolUseID":"2f2cfb14-9f74-4e2b-acf5-f9d83c8902de","timestamp":"2026-02-17T20:08:16.141Z","uuid":"e5a694be-9f9c-46d2-812a-7c820eb071fe"}
